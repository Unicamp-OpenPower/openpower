[{"authors":["lcnzg"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1564939800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1564939800,"objectID":"dae3611350c62284c07bb849357b31a4","permalink":"/authors/lcnzg/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lcnzg/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Luciano Zago","type":"authors"},{"authors":["gsalibi"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":1564939200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1564939200,"objectID":"2b92993d022e7ab9d1f1d52de8459952","permalink":"/authors/gsalibi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gsalibi/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Gustavo Storti Salibi","type":"authors"},{"authors":["klauskiwi"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"adc249be20263d550b251c8871e464c3","permalink":"/authors/klauskiwi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/klauskiwi/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Klaus Kiwi","type":"authors"},{"authors":["lfwanner"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f7445419234e14be167a051b0ebca3ca","permalink":"/authors/lfwanner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lfwanner/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Lucas Wanner","type":"authors"},{"authors":["rpsene"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7e04078d6bc07930eb7c68638f86d891","permalink":"/authors/rpsene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rpsene/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Rafael Sene","type":"authors"},{"authors":["srigo"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2bec4bbbe69315b36a06fdee37429129","permalink":"/authors/srigo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/srigo/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Sandro Rigo","type":"authors"},{"authors":["Luciano Zago"],"categories":null,"content":"","date":1564939800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564939800,"objectID":"b28b6051ce2c8f03e8cfdec19b03155d","permalink":"/talk/minicloud-linuxdevbr2019/minicloud-linuxdevbr2019/","publishdate":"2019-08-04T17:30:00Z","relpermalink":"/talk/minicloud-linuxdevbr2019/minicloud-linuxdevbr2019/","section":"talk","summary":"In this lightning talk you will know the history behind Minicloud, how students built and maintain a public cloud powered by high-performance machines and how this infrastructure is boosting the research and the open source ecosystem around the POWER processor.","tags":[],"title":"Minicloud","type":"talk"},{"authors":["Gustavo Storti Salibi"],"categories":null,"content":" Build on POWER Request   ","date":1564939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564939200,"objectID":"c573c59219f6576438836689b137c322","permalink":"/talk/power-builds-linuxdevbr2019/power-builds-linuxdevbr2019/","publishdate":"2019-08-04T17:20:00Z","relpermalink":"/talk/power-builds-linuxdevbr2019/power-builds-linuxdevbr2019/","section":"talk","summary":"In this lighting talk Gustavo will show his efforts on porting, building and making available open source projects that wasn't available on Power before. He is going to break-down the components of the infrastructure he is using and how the communities are receiving the support for a new architecture.","tags":[],"title":"Power Builds","type":"talk"},{"authors":["Gustavo Storti Salibi"],"categories":null,"content":" \n\nspaCy is an open-source software library for advanced Natural Language Processing, written in Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\nIts installation is very straightforward using the pip package manager. However, you will not succeed if you try to make it into a POWER processor. This is due to a problem with the headers of the Numpy library when using the pip. Thus, the easiest way to install spaCy is by using another package manager, Conda.\nConda is an open source, cross-platform, language-agnostic package manager and environment management system. It is released under the Berkeley Software Distribution License by Continuum Analytics.\nInstalling Python 3.7 To install spaCy, you will need to have python 3.7. To verify that you have it installed, simply use the command:\npython3.7 --version  If you have not installed it, use the package manager of your system to install.\n Start by updating the packages and installing the prerequisites:\nsudo apt update sudo apt install software-properties-common  Add the deadsnakes PPA to your sources list:\nsudo add-apt-repository ppa:deadsnakes/ppa  Once the repository is enabled, install Python 3.7 with:\nsudo apt install python3.7   Installing Conda 1. Download the Anaconda installer for POWER8 and POWER9.\n2. Enter the following on the download directory:\nbash Anaconda3-2019.03-Linux-ppc64le.sh  3. The installer prompts “In order to continue the installation process, please review the license agreement.” Click Enter to view license terms.\n4. Using Enter, scroll to the bottom of the license terms and enter “Yes” to agree to them.\n5. Click Enter to accept the default install location.\n6. Enter \u0026ldquo;yes\u0026rdquo; to initialize Anaconda3 by running conda init.\n7. Close and open your terminal window for the installation to take effect.\nsource ~/.bashrc.  Installing spaCy You only need to use the following command:\nconda install -c conda-forge spacy  ","date":1558224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558224000,"objectID":"8c658d325fa3c54e047ab91b31f84328","permalink":"/post/installing-spacy-power/","publishdate":"2019-05-19T00:00:00Z","relpermalink":"/post/installing-spacy-power/","section":"post","summary":"spaCy is an open-source software library for advanced Natural Language Processing, written in Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\nIts installation is very straightforward using the pip package manager. However, you will not succeed if you try to make it into a POWER processor.","tags":null,"title":"Installing spaCy on POWER8 or POWER9.","type":"post"},{"authors":null,"categories":null,"content":" Introduction and Prerequisites In this tutorial I will show how to use Jupyter in your browser to control scikit-learn running inside a VM. First of all you need build and connect to VM, which is showed in this tutorial.\nSSH connection Now you need connect to VM via ssh using -L 8888:localhost:8888 flag, which will bind your computer port 8888 to port 8888 from VM:\nssh ubuntu@minicloud.parqtec.unicamp.br -i ~/.ssh/your-key.pem -p \u0026lt;vm-port\u0026gt; -L 8888:localhost:8888\nInstalation and executing Now let\u0026rsquo;s update O.S., then install jupyter and sklearn:\nsudo apt update sudo apt upgrade -y sudo apt sudo apt install jupyter python3-sklearn python3-pandas -y  Initiate jupyter notebook with \u0026amp; flag, which will allow jupyter run in backgroud: Results Open link showed by jupyter on your favorite browser: And now you are ready to use scikit-learn using jupyter remotely.\n","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"b47061e46286a8d739b03b1518264b3b","permalink":"/post/how-use-scikit-learn-and-jupyter-remotely/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/post/how-use-scikit-learn-and-jupyter-remotely/","section":"post","summary":"Introduction and Prerequisites In this tutorial I will show how to use Jupyter in your browser to control scikit-learn running inside a VM. First of all you need build and connect to VM, which is showed in this tutorial.\nSSH connection Now you need connect to VM via ssh using -L 8888:localhost:8888 flag, which will bind your computer port 8888 to port 8888 from VM:\nssh ubuntu@minicloud.parqtec.unicamp.br -i ~/.ssh/your-key.pem -p \u0026lt;vm-port\u0026gt; -L 8888:localhost:8888","tags":null,"title":"How use scikit-learn and Jupyter remotely","type":"post"},{"authors":["Gustavo Storti Salibi"],"categories":null,"content":" \n\nBazel is a free software tool that allows for the automation of building and testing of software. Similar to build tools like Make, Maven, and Gradle, Bazel builds software applications from source code using a set of rules.\nIt uses a human-readable, high-level build language. Bazel supports projects in multiple languages and builds outputs for multiple platforms and supports large codebases across multiple repositories, and large numbers of users.\nIn designing Bazel, emphasis has been placed on build speed, correctness, and reproducibility. The tool uses parallelization to speed up parts of the build process. It includes a Bazel Query language that can be used to analyze build dependencies in complex build graphs\nBazel must have Power support in the future, making its installation possible through community-supported methods. However, currently, if you want to install on Power or other architectures or systems that do not have support, you need compiling Bazel from source.\nSo let's see how to install Bazel on architectures and systems not officially supported. I will use Ubuntu 14.04 as the basis of this tutorial, but it can be easily adapted to other Linux systems. \nBuilding Bazel from scratch (bootstrapping) Here we will see how to do self-compilation. If you are using Ubuntu 14.04 or Ubuntu 16.04 in ppc64le, you can skip right to: Using ready binaries.\n First, install the prerequisites:\nPkg-config\nZip, Unzip\nG++\nZlib1g-dev\nJDK 8 (you must install version 8 of the JDK. Versions other than 8 are not supported)\nPython (versions 2 and 3 are supported, installing one of them is enough)\n# add-apt-repository ppa:openjdk-r/ppa # apt-get update # apt-get install pkg-config zip unzip g++ zlib1g-dev openjdk-8-jdk python   \n Next, download the Bazel binary installer named bazel--dist.zip from the Bazel releases page on GitHub:\nwget https://github.com/bazelbuild/bazel/releases/download/\u0026lt;version\u0026gt;/bazel-\u0026lt;version\u0026gt;-dist.zip   There is a single architecture-independent distribution archive. There are no architecture-specific or OS-specific distribution archives.\nYou have to use the distribution archive to bootstrap Bazel. You cannot use a source tree cloned from GitHub (the distribution archive contains generated source files that are required for bootstrapping and are not part of the normal Git source tree).\n\n Unpack the zip file somewhere on disk:\nunzip bazel-\u0026lt;version\u0026gt;-dist.zip   \n Run the compilation script:\nbash ./compile.sh   \nThis may take several minutes\u0026hellip;\n\nAnd this should be the output:  \nUsing ready binaries If you are using Ubuntu 14.04 or Ubuntu 16.04 in ppc64le, you can use our already compiled versions of the binaries.\nMake sure you have the JDK 8 installed:\n java -version  \n If you do not have it, you need to install it:\n# add-apt-repository ppa:openjdk-r/ppa # apt-get update # apt-get install openjdk-8-jdk  We have released the last 10 versions of Bazel already compiled in this link: https://oplab9.parqtec.unicamp.br/pub/ppc64el/bazel/\n  \n Download the desired version:\nwget https://oplab9.parqtec.unicamp.br/pub/ppc64el/bazel/ubuntu_\u0026lt;version\u0026gt;/bazel_bin_\u0026lt;version\u0026gt; # mv bazel_bin_\u0026lt;version\u0026gt; bazel # chmod +x bazel   \nInstalling Bazel Finally, the compiled output is placed into output/bazel (or it is in the current directory if you have downloaded the binary). This is a self-contained Bazel binary, without an embedded JDK. You can copy it anywhere or use it in-place. For convenience we recommend copying this binary to a directory that\u0026rsquo;s on your PATH (such as /usr/local/bin on Linux).\n# mv output/bazel /usr/local/bin  or\n# mv bazel /usr/local/bin  \nWhen using Bazel for the first time, it will extract the installation and prepare everything. To do this, simply use the command:\n \nFrom now on, Bazel is installed and to use it simply use the command:\n bazel \u0026lt;command\u0026gt; \u0026lt;options\u0026gt;  \nUsing Bazel to compile Bazel Once installed, you can use Bazel itself to compile a new version. To do this, simply download the desired version (as seen in Building Bazel from scratch) or even the developing version on GitHub and use the following command in the directory of the downloaded files:\n Bazel build //src:bazel  \nReferences  https://docs.bazel.build/ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;  ","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"7ce4b765968acf6ee9db85db5570ffcc","permalink":"/post/installing-bazel-power-other-architectures-systems/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/installing-bazel-power-other-architectures-systems/","section":"post","summary":"Bazel is a free software tool that allows for the automation of building and testing of software. Similar to build tools like Make, Maven, and Gradle, Bazel builds software applications from source code using a set of rules.\nIt uses a human-readable, high-level build language. Bazel supports projects in multiple languages and builds outputs for multiple platforms and supports large codebases across multiple repositories, and large numbers of users.","tags":null,"title":"Installing Bazel on Power and Other Unsupported Architectures/Systems","type":"post"},{"authors":null,"categories":null,"content":" This tutorial I will show how create a openstack image (.qcow2) of opensuse from a ISO image using qemu. In this tutorial will be used opensuse Tumbleweed ppc64 le (because it\u0026rsquo;s the most challenging), but similiar process can be done for leap (15 and 42.3) and Tumbleweed ppc64be.\nPreparing environment First we need download opensuse image from repository (Tumbleweed, leap 15 and leap 42.3) and sha256 of respective image.\nExecute sha256:\nsha256sum openSUSE-Tumbleweed-DVD-ppc64le-Current.iso  Compare sha256sum output with sha256 downloaded:\n715d9f89d90eb795b6a64ffe856aa5b7f3a64c7195a9ede8abea14a9d4f69e67  Install qemu using:\nsudo apt update sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system -y  Now we need create a disk .qcow2 to install our O.S. with this command:\nqemu-img create -f qcow2 openSUSE-Tumbleweed-ppc64le.qcow2 5G  Execute qemu to run the instaler:\nsudo qemu-system-ppc64le -enable-kvm -m 1024 -cdrom openSUSE-Tumbleweed-DVD-ppc64le-Current.iso -drive file=openSUSE-Tumbleweed-ppc64le.qcow2,media=disk,if=virtio -nographic -smp cores=1,threads=1 -monitor pty -serial stdio -nodefaults -netdev user,id=enp0s1 -device virtio-net-pci,netdev=enp0s1 -boot order=d  Installing openSUSE Select your language (using tab and arrows): Figure 1: Language selection screen Select te most suitable bundle for your goal: Figure 2: Bundle selector screen Select expert partitioner: Figure 3-4: Partioner selection screen Select the hard drive that you want install opensuse: Figure 5: Drive selector screen Add new partition selecting add button: Figure 6: Partition screen Set partition size to 8 MiB: Figure 7: Partition size screen (Boot)\nSelect raw partition: Figure 8: Partition role screen (Boot)\nSelect file system as Ext4 (or other filesystem of your preference): Figure 9: File System type (Boot)\nSelect partition as PReP Boot Partition and next: Figure 10: Partition type (Boot)\nThe boot partition was create and now we will create O.S. partition, select add and inside Patition size screen select Maximum Size: Figure 11: Partition size screen (O.S) Select Operating System option: Figure 12: Partition role screen (O.S) Select file system as Ext4 again (or other filesystem of your preference): Figure 13: File System type (O.S) Left selected Linux Native: Figure 14: Partition type (O.S) Left Mount device as / and select next: Figure 15: Mount point Partition configuration will look like this: Figure 16: Final partion configuration We will receive warning message but we can ignore it and select yes: Figure 17: Warning message Next again: Figure 18: Sumary partition screen Select your clock and time zone: Figure 19: Clock and time zone screen Put you username and password: Figure 20: Local user screen Accept instalation and install: Figure 21: Summary screen Figure 22: Instalation screen Preparing image Update all packages and install necessary ones (you can also uninstall unnecessary packages):\nsudo zypper update sudo zypper install cloud-init growpart yast2-network yast2-services-manager acpid  Remove hard-coded MAC address:\nsudo cat /dev/null \u0026gt; /etc/udev/rules.d/70-persistent-net.rules  Enable ssh and cloud-init:\nsudo systemctl enable cloud-init sudo systemctl enable sshd  Disable firewall:\nsudo systemctl stop firewalld sudo systemctl disable firewalld  Inside /etc/default/grub file, set grub timeout to 0:\nGRUB_TIMEOUT=0  Figure 23: Grub configuration Update grub:\nsudo exec grub2-mkconfig -o /boot/grub2/grub.cfg \u0026quot;$@\u0026quot;  Only for openSUSE Tumbleweed Le/Be Opensuse Tumbleweed ppc64 Le/Be lacks some parameters on cloud-init.service, this causes instability on boot, which, sometimes, causes network connection errors. This problem was reported and hopefully will be solved when you read this tutorial.\nEdit cloud-init.service file:\nsudo vim /etc/systemd/system/cloud-init.target.wants/cloud-init.service  Add lines bellow after After=systemd-networkd-wait-online.service line:\nRequires=wicked.service After=wicked.service After=dbus.service Conflicts=shutdown.target  Figure 24: Configuration of cloud-init.service Reload cloud-init service:\nsudo systemctl restart cloud-init sudo systemctl daemon-reload  Because Leap 42.3 ppc64Le\u0026rsquo;s configuration fits better for a cloud role, so we will replace cloud.cfg of Tumbleweed by Leap42.3\u0026rsquo;s cloud.cfg:\nsudo vim /etc/cloud/cloud.cfg   Cleaning image Now delete all remaining data:\ncat /dev/null \u0026gt; ~/.bash_history \u0026amp;\u0026amp; history -c \u0026amp;\u0026amp; sudo su cat /dev/null \u0026gt; /var/log/wtmp cat /dev/null \u0026gt; /var/log/btmp cat /dev/null \u0026gt; /var/log/lastlog cat /dev/null \u0026gt; /var/run/utmp cat /dev/null \u0026gt; /var/log/auth.log cat /dev/null \u0026gt; /var/log/kern.log cat /dev/null \u0026gt; ~/.bash_history \u0026amp;\u0026amp; history -c \u0026amp;\u0026amp; sudo poweroff  Adding to openstack And finaly add image to openstack:\nglance image-create --file openSUSE-Tumbleweed-ppc64le.qcow2 --container-format bare --disk-format qcow2 --property hw_video_model=vga --name \u0026quot;openSUSE Tumbleweed ppc64le\u0026quot;  If all the steps worked, you should see these messages at the next boot. ","date":1541462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541462400,"objectID":"1c83c29c4f8d43cb9362826bb25f4d58","permalink":"/post/opensuse-tutorial/","publishdate":"2018-11-06T00:00:00Z","relpermalink":"/post/opensuse-tutorial/","section":"post","summary":"This tutorial I will show how create a openstack image (.qcow2) of opensuse from a ISO image using qemu. In this tutorial will be used opensuse Tumbleweed ppc64 le (because it\u0026rsquo;s the most challenging), but similiar process can be done for leap (15 and 42.3) and Tumbleweed ppc64be.\nPreparing environment First we need download opensuse image from repository (Tumbleweed, leap 15 and leap 42.3) and sha256 of respective image.","tags":null,"title":"Building a opensuse openstack image","type":"post"},{"authors":["Luciano Zago"],"categories":null,"content":" In this guide, we will show how to setup a public FTP server with directory access control and disk quota per-user. We used Ubuntu Server 16.04, running on ppc64le architecture, but it should work on other architectures as well, because no exclusive software was used, only open source software.\nDisk space You will need an ext4 partition with enough space, that can be mounted on / or on /var/www. If you need help, look at this tutorial.\nAfter that, create the directories that will be used in the web and ftp servers:\nsudo mkdir /var/www/html sudo mkdir /var/www/html/pub  Set the permissions to these directories:\nsudo chown nobody:nogroup /var/www/html sudo chmod a-w /var/www/html  HTTP Server (apache) We intend that our files can be accessed through a web browser. In that case, we will need a HTTP Server, like Apache.\nInstallation Install the package apache2, with the following commands:\nsudo apt-get update sudo apt-get install apache2  Restart the service to make sure that the web server works:\nsudo systemctl restart apache2  Content You can create a welcome page in HTML with links to /pub folder, to show the files though the browser. Your page index.html need to be in the directory /var/www/html.\nFor reference, you can look at our web page in this link.\nSSL Certificate (certbot) Certbot is a client that deploy free SSL certificates from Let\u0026rsquo;s Encrypt to any web server. If you already have a SSL certificate, you can skip this part.\nInstallation Run these commands to install the package certbot:\nsudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install python-certbot-apache  Configuration We need to configure the web server to work with the certificate. Run this command to use the Certbot certificate with the Apache web server:\nsudo certbot --apache  The certificate expires in 90 days, so you need to renew this certificate periodically. To schedule the execution of certobot renew command, we will use cronjob, a time-base job scheduler. To use the scheduler, run this command:\nsudo crontab -e  And add the following line in the end of the file:\n0 0 * * * sudo certbot renew  Save the file. After that, the renew command is scheduled to run everyday.\nFirewall (ufw) The UFW is an easy frontend interface for iptables. We need to configure the firewall to work with the other installed software.\nInstallation Install the package ufw to manage the firewall, with the following commands:\nsudo apt-get update sudo apt-get install ufw  Configuration Forwarding the ports:\nsudo ufw allow 20/tcp sudo ufw allow 21/tcp sudo ufw allow 990/tcp sudo ufw allow 60000:60500/tcp sudo ufw allow ssh sudo ufw allow 'Apache Full' sudo ufw status  Restart to conclude the steps:\nsudo ufw disable sudo ufw enable  FTP Server (vsftpd) We will use the vsftpd software to run the FTP server, the default in the Ubuntu, CentOS, Fedora, NimbleX, Slackware and RHEL Linux distributions.\nInstallation Install the package vsftpd with the following command:\nsudo apt-get install vsftpd  Configuration Backup your original file:\nsudo cp /etc/vsftpd.conf /etc/vsftpd.orig  Edit the configuration file with the following command:\nsudo nano /etc/vsftpd.conf  Example config file: \nIn the previous config, we allowed read permission for anonymous.\nTo create the userlist that have permission to access the FTP server, and allow the anonymous user, use the following commands:\nsudo touch /etc/vsftpd.userlist sudo echo \u0026quot;anonymous\u0026quot; \u0026gt;\u0026gt; /etc/vsftpd.userlist  Disabling shell for ftp users With these commands, we will create a new shell with no functionalities, to restrict the access of the FTP users:\nsudo touch /bin/ftponly sudo echo -e '#!/bin/sh\\necho \u0026quot;This account is limited to FTP access only.\u0026quot;' \u0026gt;\u0026gt; /bin/ftponly sudo chmod a+x /bin/ftponly sudo echo \u0026quot;/bin/ftponly\u0026quot; \u0026gt;\u0026gt; /etc/shells  Restart the FTP server service:\nsudo systemctl restart vsftpd  Disk Quota We will use a disk quota to limit the disk space used by the FTP users.\nInstallation Install the package quota with the following command:\nsudo apt install quota  Configuration Edit the fstab file and add usrquota option in the partition you chose earlier:\nsudo nano /etc/fstab  Remount partition and enable the quota:\nsudo mount -o remount /var/www sudo quotacheck -cum /var/www sudo quotaon /var/www  Defining a default quota Create a new user to copy the quota settings for the new users:\nsudo adduser ftpuser  Insert a password.\nAfter that, you will need to edit the quota of ftpuser with this command:\nsudo edquota ftpuser  Put the values of soft and hard quota in these columns.\nExample: 10GB: 10000000 and 10485760 in block quota session.\nLet 0 if you don\u0026rsquo;t want to have a limit.\nSet the default quota user as ftpuser to copy a quota for the new users:\nsudo sed -i -e 's/.*QUOTAUSER=\u0026quot;\u0026quot;.*/QUOTAUSER=\u0026quot;ftpuser\u0026quot;/' /etc/adduser.conf  Commands There are a few commands useful for controlling the quota: - quota user shows the user quota. - repquota -a shows the general quota report. - edquota user to edit user quota.\nAccess List (acl) We will use Access List Control, or ACL, to have a better control of file permissions. With ACL we can set different file permissions, in different directories, to each FTP user.\nInstallation Install the package acl with the following command:\nsudo apt install acl  Configuration Edit the fstab file and add acl option in the /var/www partition:\nsudo nano /etc/fstab  Remount the partition to apply the changes:\nsudo mount -o remount /var/www  Commands The commands used to enable write permission to $USER in $DIRECTORY were:\nsetfacl -d -R -m u:$USER:rwX $DIRECTORY setfacl -R -m u:$USER:rwX $DIRECTORY  Adding new users We created the following script to manage the creation of new users: \nchmod +x create_user.sh  Add new users by running the script this way:\nsudo ./create_user.sh 'user' 'pass' 'directory'  Directory instructions:\n for the root of FTP directory, use . . for other directories, don\u0026rsquo;t write the initial and final slashes (ex: ppc64el/debian for /www/html/pub/ppc64el/debian/).  Should any problem with file permissions ocurr, use the fix_acl.sh script, that will remake the permissions based on acl.list file.\n Add execute permission to the script:\nchmod +x fix_acl.sh  Run the script with sudo, this way:\nsudo ./fix_acl.sh  References  https://www.digitalocean.com/community/tutorials/how-to-install-the-apache-web-server-on-ubuntu-16-04 https://www.digitalocean.com/community/tutorials/how-to-setup-a-firewall-with-ufw-on-an-ubuntu-and-debian-cloud-server https://www.digitalocean.com/community/tutorials/how-to-set-up-vsftpd-for-a-user-s-directory-on-ubuntu-16-04  ","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541376000,"objectID":"39eba745a5cfdf797c026425b3a9dfdd","permalink":"/post/ftp-server-setup-with-acl-and-quota/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/post/ftp-server-setup-with-acl-and-quota/","section":"post","summary":"In this guide, we will show how to setup a public FTP server with directory access control and disk quota per-user. We used Ubuntu Server 16.04, running on ppc64le architecture, but it should work on other architectures as well, because no exclusive software was used, only open source software.\nDisk space You will need an ext4 partition with enough space, that can be mounted on / or on /var/www. If you need help, look at this tutorial.","tags":null,"title":"Setting up a FTP Server with Access List and Disk Quota","type":"post"},{"authors":null,"categories":null,"content":"  [data-text] { } [data-text]::after { content: attr(data-text); }  TensorFlow is a widespread software library for numerical computation using data flow graphs. It is very common on machine learning and deep neural networks projects. Therefore, today we are going to see how to install it on POWER with CPU only configuration.\n\nBefore installing TensorFlow, there are a couple of details we have to pay attention to: 1. Due to Bazel, one of TF dependencies, the operating system must be Ubuntu 14.04 or Ubuntu 16.04. 2. We are going to use Python 2.7, since TF doesn\u0026rsquo;t seem to be supported by Python 3.5 on POWER.\nTensorflow Dependencies You can use the commands below to solve most of the dependencies:\napt-get update apt-get install python-numpy python-dev python-pip python-wheel  Bazel installation Bazel is one of the TF dependencies, but its installation is less intuitive than the others due to its community not officially supporting POWER architecture. That said, we must compile it from the Source. First of all, we need to install its own dependencies by the following commands:\napt-get update apt-get install unzip build-essential python openjdk-8-jdk protobuf-compiler zip g++ zlib1g-dev  It is also important to add enviroment variables on .bashrc for JDK.\nvi .bashrc export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-ppc64el export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH  For compiling Bazel, we are going to download and unpack its distribution archive (the zip file from the release page https://github.com/bazelbuild/bazel/releases. The .sh is not compatible with ppc64le) and compile it.\nmkdir bazel cd bazel wget -c https://github.com/bazelbuild/bazel/releases/download/0.11.1/bazel-0.11.1-dist.zip #if you want to download other version of bazel, this link must be switched by the one you are intenting to use. unzip bazel-0.11.1-dist.zip ./compile.sh  As we can see, this tutorial was tested with bazel 0.11.1, but feel free to try other version and see if it works properly.\nAlso, if you are having any trouble about lack of resources, you can take a look on \u0026lsquo;Build issues and Support Websites\u0026rsquo; to see if there\u0026rsquo;s any link that could help you. Anticipating: if you don\u0026rsquo;t have memory enough and your Bazel can\u0026rsquo;t complete the compile step, you might have a problem with the garbage collector of JAVA (and there\u0026rsquo;s a link which explains how to deal with it).\nInstalling Tensorflow Since we are going to use the current version of TF, we need to clone it from the official GitHub and execute the configuration script.\ngit clone https://github.com/tensorflow/tensorflow cd ~/tensorflow ./configure  On this step, we have to specify the pathname of all relevant TF dependencies and other build configuration options. On most of them we can use the answers suggested on each question. Here, I will show how it was done for this tutorial. (Yours might be a little different, depending on the pathnames)\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7 Found possible Python library paths: /usr/local/lib/python2.7/dist-packages /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/lib/python2.7/dist-packages]: /usr/lib/python2.7/dist-packages  Using python library path: /usr/local/lib/python2.7/dist-packages #Y/N Answers given: All of them as suggested in each question. Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -mcpu=native Configuration finished  To build and install TF, we use:\nbazel build --copt=\"-mcpu=native\" --jobs 1 --local_resources 2048,0.5,1.0 //tensorflow/tools/pip_package:build_pip_package bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg #creates the pip package pip install /tmp/tensorflow_pkg/tensorflow-1.5.0rc0-cp27-cp27mu-linux_ppc64le.whl #installs the pip package. This name depends on your operating system, Python version and CPU only vs. GPU support. Therefore, check it out its name before this step.  By this moment, your TF must be working. Remember not to import it into its own directory: you have to chance directory before executing Python.\nBuild Issues and Support Websites: While testing this tutorial, I could separate some useful issues reports and links to help some of the troubles you might have on the way.\n https://github.com/tensorflow/tensorflow/issues/14540 It solves a protobuf problem I had. It seems pretty common on PPC TF installation. https://github.com/tensorflow/tensorflow/issues/349 This one is about local resources. If you are running out of memory (your build fails on C++ compilation rules), you have to specify your resources on the command line when you build TF. On the tutorial, it is already done. https://www.tensorflow.org/install/install_sources An official tutorial about how to install TF from Sources https://docs.bazel.build/versions/master/install-compile-source.html An official tutorial about how to install Bazel from Sources. https://www.ibm.com/developerworks/community/blogs/fe313521-2e95-46f2-817d-44a4f27eba32/entry/Building_TensorFlow_on_OpenPOWER_Linux_Systems?lang=en IBM source about Tensorflow installation. Provides interesting information about bazel installation on PPC and how to install TF with GPU support. It also points to an IBM Bazel modified to PPC (which we are not using in this tutorial, but you can take a look on it). https://github.com/tensorflow/tensorflow/issues/7979#issuecomment-283559640 An issue about enviroment variables: on the configuration step, if it does not recognize some of the TF variables, this might help you to solve the problem. https://github.com/bazelbuild/bazel/issues/1308 An issue about Bazel: \u0026ldquo;The system is out of resources\u0026rdquo;. You might need to add a command line on compile file to change the garbage collector size. On the issue on git, it\u0026rsquo;s suggested to change it to 384, but, at least on one of the computers I tried to compile, I needed to change it to 512 (in other words, change -J-Xmx384m on the solution line to -J-Xmx512m). It\u0026rsquo;s important to see that ideally we should not have to change the source code, but it solves the problem. Another option is to increase the memory of your system if it\u0026rsquo;s possible (recommended).  ","date":1516060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516060800,"objectID":"ce4dfda09ff2e011d52e3e372ac026b9","permalink":"/post/building-tensorflow-on-power/","publishdate":"2018-01-16T00:00:00Z","relpermalink":"/post/building-tensorflow-on-power/","section":"post","summary":"[data-text] { } [data-text]::after { content: attr(data-text); }  TensorFlow is a widespread software library for numerical computation using data flow graphs. It is very common on machine learning and deep neural networks projects. Therefore, today we are going to see how to install it on POWER with CPU only configuration.\n\nBefore installing TensorFlow, there are a couple of details we have to pay attention to: 1. Due to Bazel, one of TF dependencies, the operating system must be Ubuntu 14.","tags":null,"title":"Building Tensorflow on POWER - CPU only","type":"post"},{"authors":null,"categories":null,"content":"  [data-text] { } [data-text]::after { content: attr(data-text); }  Deploying HTTPS is essential for security, and OpenStack Ansible does it by default. However, if no certificates are provided, it will generate self-signed ones, which although are more secure than no SSL at all, it will trigger a warning when accessing the dashboard in the browser. Luckily, the Let’s Encrypt project provides signed SSL certificates for free.\n\nLet’s Encrypt requires your server to be validated before issuing the certificate. This means it will create a temporary file on your server and then try to access it from their servers, to verify that you control the domain you\u0026rsquo;re trying to get a certificate to.\n\nIt can launch a temporary web server to do so, however, this will require to stop your usual web server (e.g. Apache) and lead to a few seconds of downtime. Alternatively, you can provide your web root path. Let’s Encrypt will create the files there, and they will be served directly by your usual web server. This approach does not lead to downtime, but presents some additional challenges when using it with OpenStack Ansible:\n OpenStack Ansible does not have a web root path out of the box to be used by Let’s Encrypt. SSL certificates must be provided to HAProxy, which runs on metal, while the Apache server to be used by Let’s Encrypt runs inside a container.  \nInstalling OpenStack Ansible Install OpenStack as usual, without providing any certificates. Self-signed ones will be therefore generated, and we will replace them later.\nEnable web root We will not actually create a web root. Since Let’s Encrypt only requires writing on your-domain.com/.well-known directory, we will create an alias to the .well-known path.\nAttach to the horizon container. Replace the container name accordingly with your setup. If you don’t know the name, run lxc-ls | grep horizon to get the container name.\nlxc-attach -n infra1_horizon_container-XXXXXXXX  Add the following line to /etc/apache2/sites-enabled/openstack-dashboard.conf, inside the \u0026lt;VirtualHost *:80\u0026gt; tag\nAlias /.well-known /var/www/html/.well-known  Restart the apache2 service:\nservice apache2 restart  Now, we can use /var/www/html as our web root, at least from the Let’s Encrypt Certbot point of view.\nGetting the certificates Now install the Let’s Encrypt Certbot. The intention is to only get the certificates files, not configure them in Apache. Use the following commands to do so:\napt-get update apt-get install software-properties-common add-apt-repository ppa:certbot/certbot apt-get update apt-get install certbot certbot certonly  When asked to choose an authentication method, choose 2\nHow would you like to authenticate with the ACME CA? ------------------------------------------------------------------------------- 1: Spin up a temporary webserver (standalone) 2: Place files in webroot directory (webroot) ------------------------------------------------------------------------------- Select the appropriate number [1-2] then [enter] (press \u0026#39;c\u0026#39; to cancel): 2  When asked for the webroot, input /var/www/html\nSelect the webroot for your-domain.com: ------------------------------------------------------------------------------- 1: Enter a new webroot ------------------------------------------------------------------------------- Press 1 [enter] to confirm the selection (press \u0026#39;c\u0026#39; to cancel): 1 Input the webroot for unicamp.br: (Enter \u0026#39;c\u0026#39; to cancel): /var/www/html  After this, the certificate files will be placed on /etc/letsencrypt/live/your-domain.com\nAllow the container to copy files to the host Generate an SSH key inside the container:\nssh-keygen -t rsa  Print the public key and copy it to the clipboard:\ncat /root/.ssh/id_rsa.pub  Now append the container\u0026rsquo;s public key to the authorized_keys file in the host:\necho [PASTE THE COPIED KEY HERE]  /root/.ssh/authorized_keys  This will allow the container to copy the certificates to the host using scp.\nApplying the certificates in HAProxy To use them in HAProxy, we must concatenate some files. Replace your-domain.com accordingly. cat /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.key cat /etc/letsencrypt/live/your-domain.com/cert.pem /etc/letsencrypt/live/your-domain.com/chain.pem /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.pem \nSet the permissions properly:\nchmod 640 /etc/letsencrypt/live/your-domain.com/haproxy.key chmod 644 /etc/letsencrypt/live/your-domain.com/haproxy.pem  Still inside the horizon container, copy the files we just generated to the host. Replace your-domain.com and HOST_IP_ADDRESS accordingly.\nscp /etc/letsencrypt/live/your-domain.com/haproxy.* HOST_IP_ADDRESS:/etc/ssl/private  Now exit the container and apply the new certificate:\nservice haproxy reload  \nRenewing the certificates automatically As Let’s Encrypt certificates are only valid for 90 days, it is highly advisable to schedule automatic renewing. We can do this using crontab inside the horizon container.\nAttach to the horizon container. Replace the container name accordingly with your setup. If you don’t know the name, run lxc-ls | grep horizon to get the container name.\nlxc-attach -n infra1_horizon_container-XXXXXXXX  Open the crontab editor:\ncrontab -e  Place this line at the end of the file, replacing your-domain.com and HOST_IP_ADDRESS accordingly.\n26 3 * * 5 certbot renew \u0026amp;\u0026amp; cat /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.key \u0026amp;\u0026amp; cat /etc/letsencrypt/live/your-domain.com/cert.pem /etc/letsencrypt/live/your-domain.com/chain.pem /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.pem \u0026amp;\u0026amp; scp /etc/letsencrypt/live/your-domain.com/haproxy.* HOST_IP_ADDRESS:/etc/ssl/private \u0026amp;\u0026amp; ssh HOST_IP_ADDRESS service haproxy reload  This will run every week, but it will only actually renew the certificate at most every 60 days, as only certificates that expire in less than 30 days are renewed. Running it more often than every 60 days makes it safer, as even if it fails to run once after the 60 days window, it will still run again before the certificate expire.\n","date":1513468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513468800,"objectID":"c7bb686b2296c01f591a5dcd4aa917ca","permalink":"/post/integrating-openstack-ansible-with-lets-encrypt/","publishdate":"2017-12-17T00:00:00Z","relpermalink":"/post/integrating-openstack-ansible-with-lets-encrypt/","section":"post","summary":"[data-text] { } [data-text]::after { content: attr(data-text); }  Deploying HTTPS is essential for security, and OpenStack Ansible does it by default. However, if no certificates are provided, it will generate self-signed ones, which although are more secure than no SSL at all, it will trigger a warning when accessing the dashboard in the browser. Luckily, the Let’s Encrypt project provides signed SSL certificates for free.\n\nLet’s Encrypt requires your server to be validated before issuing the certificate.","tags":null,"title":"Integrating OpenStack Ansible with Let’s Encrypt","type":"post"},{"authors":null,"categories":null,"content":" Buildbot its tool to automate compilation and tests. This tutorial we will install it on three important distro and run it.\n1 - Installation dependencies:  Installation of necessary packages to correct installation of Buildbot bundle.\nFedora 25: sudo dnf install python-devel python-pip redhat-rpm-config make gcc   Fedora 26: sudo dnf install python-pip redhat-rpm-config make gcc  Ubuntu and Debian: sudo apt-get install Python-dev build-essential python-pip  2 - Installation without virtualenv: sudo pip install --upgrade pip sudo pip install 'buildbot[bundle]' sudo pip install buildbot-grid-view sudo pip install buildbot-worker sudo pip install setuptools-trial  2.5 - Installation with virtualenv (optional):  First we need install virtual environment.  Fedora: sudo dnf install python-virtualenv  Ubuntu and Debian: sudo apt install python-virtualenv   Now we need activate the environment\nvirtualenv --no-site-packages YourSandbox source YourSandbox/bin/activate   Instalation: pip install --upgrade pip pip install 'buildbot[bundle]' pip install buildbot-grid-view pip install buildbot-worker pip install setuptools-trial  3- Initial Master setup:  Creation of folder where Buildbot archives will stay:\nmkdir -p BuildBot cd BuildBot  Creation of Master with name[master]:\nbuildbot create-master master   The configuration of all functions of Buildbot its done in configuration file inside Master folder, to simplify we will use the sample configuration file provided in default template of Master [master.cfg.sample], but it’s needed be renamed to [master.cfg] to be recognized by Buildbot:\nmv master/master.cfg.sample master/master.cfg  Here we will start Master daemon:\nbuildbot start master    4- Initial Worker setup:  Here we will create a worker (previously slave) with name [worker]:\nbuildbot-worker create-worker worker localhost example-worker pass    The command syntax: * buildbot-worker = Buildbot Worker program. * create-worker = Command to creation of Worker * worker = Name of worker folder * localhost = Master location on the network (This example Master are in the same VM) * example-worker = Name of worker * pass = Authentication password   Start of worker daemon:\nbuildbot-worker start worker  Access address http://localhost:8010/ on your browser.\n  ","date":1506038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506038400,"objectID":"b6f6a56cfa4531b7f3f4bc280efe11eb","permalink":"/post/buildbot-tutorial/","publishdate":"2017-09-22T00:00:00Z","relpermalink":"/post/buildbot-tutorial/","section":"post","summary":"Buildbot its tool to automate compilation and tests. This tutorial we will install it on three important distro and run it.\n1 - Installation dependencies:  Installation of necessary packages to correct installation of Buildbot bundle.\nFedora 25: sudo dnf install python-devel python-pip redhat-rpm-config make gcc   Fedora 26: sudo dnf install python-pip redhat-rpm-config make gcc  Ubuntu and Debian: sudo apt-get install Python-dev build-essential python-pip  2 - Installation without virtualenv: sudo pip install --upgrade pip sudo pip install 'buildbot[bundle]' sudo pip install buildbot-grid-view sudo pip install buildbot-worker sudo pip install setuptools-trial  2.","tags":null,"title":"Install and initial configuration Buildbot on Fedora, Ubuntu and Debian","type":"post"},{"authors":null,"categories":null,"content":" Introduction to OpenCL2CUDA We all know that software is replacing many people functions. Said that, many very complicated data processing are now made by computers, that are getting better and better ways to do those tasks. There are many libraries and frameworks that helps programmers and engineers writing code to process some big amount of data. Two of these well known libraries are OpenCL, developed for heterogeneous computing (gpu, processors, fpga), and CUDA, a NVIDIA library created so people can write code to run on their GPUs. These libraries have some similar routines, cause there are many steps you have to do on both of them. Thinking about it, I started writing a OpenCL to CUDA converter.\nImplementation The implementation of this code still really simple, since all I am doing is searching for some OpenCL functions and replacing it for its equivalent on CUDA. Besides, if its not a direct translation, the converter suggests some possible fixes for you code. To find the suggestions on your code search for the #tranlation# word. We are using Python 3. To run this code all you have to do is:\nchmod +x createCUDAkernel.py (just the first time) ./createCUDAkernel.py --opencl_name=\u0026quot;name of the opencl file\u0026quot; --main_name=\u0026quot;name of the C/C++ file\u0026quot;  How to contribute Now, I am searching for CUDA and OpenCL codes that do the same thing, so I can go on this project. Besides, you can fork the project on Github.\nThanks a lot.\n","date":1496793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496793600,"objectID":"0be4d300e6a82bc2771254f3a2c8f189","permalink":"/post/opencl2cuda/","publishdate":"2017-06-07T00:00:00Z","relpermalink":"/post/opencl2cuda/","section":"post","summary":"Introduction to OpenCL2CUDA We all know that software is replacing many people functions. Said that, many very complicated data processing are now made by computers, that are getting better and better ways to do those tasks. There are many libraries and frameworks that helps programmers and engineers writing code to process some big amount of data. Two of these well known libraries are OpenCL, developed for heterogeneous computing (gpu, processors, fpga), and CUDA, a NVIDIA library created so people can write code to run on their GPUs.","tags":null,"title":"Introduction to OpenCL2CUDA","type":"post"},{"authors":null,"categories":null,"content":" Introduction to PowerGraph As computers evolve, people are trying new methods do analyse how good some machine is when compared to another one. The amount of energy that some machine is consuming, seems to be a nice measure, once we are willing to produce faster and cheaper. IPMI(Intelligent Platform Management Interface), on the other side, is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system. One of the measures that IPMI allow us to do is:\ndcmi power reading  This command shows the instant power consumption. With this tools, my team decided to create a software that gets informations about the consumption of a machine and exports it on a readable way. Thats how we did it:\nInfrastructure The infrastructure was not so complicated to configure. We have to download some packages and set some configurations (process we are automating with Ansible). All the system is deployed on Minicloud. The packages (all via apt) we are using are:\n ipmitool apache2 python2.7 python-pip git htop tmux  Besides, we are using crontab to be sure our service is still runing, and if it is not, restart it. We are doing this verification every ten minutes with the\nTo solve all ```Python``` dependencies you can run:  pip install -r requirements.txt\n### Apache configurations Here you will install two modules in your apache server and change the virtual host configuration file. Doing this you will be able to control your browser's cache. In your server terminal run:  sudo a2enmod headers sudo a2enmod expires sudo service apache2 restart\nAfter that, find your virtual host configuration file (*/etc/apache2/sites-available/default/000-default.conf* if you are using ubuntu OS) and insert the following lines, adjusting the parameters according to your needs:  ExpiresActive On ExpiresDefault \u0026ldquo;access plus 10 minutes\u0026rdquo; ExpiresByType text/html \u0026ldquo;access plus 1 day\u0026rdquo; ExpiresByType text/javascript \u0026ldquo;access plus 1 day\u0026rdquo;\n# if it is your interest, you can set a specific expiration time for your csv file # ExpiresByType text/csv \u0026ldquo;access plus 30 seconds\u0026rdquo;\nHeader set Cache-control \u0026ldquo;no-cache\u0026rdquo;  \n## Back-end code PowerGraph was totally developed using ```Python```. There are three main codes: - powergraph.py - graph_csv.py - csvcreator.py Below, I will explain each code and its function. ### powergraph.py This is the code that keeps getting power info about a machine and save it to  tinyDB``` or prompt the result to user. You can run it using the command:\npython2.7 powergraph.py --host=\u0026quot;server address\u0026quot; --port=\u0026quot;server port\u0026quot; --user=\u0026quot;allowed user\u0026quot; --passwd=\u0026quot;password for this user\u0026quot;  You can use the optional parameter --store in order to save the infos as json on tinydb. Without this parameter, the script will print on the terminal. Besides, you can use --feedback with store in order to see the measures status. If you want to set the time interval that a new csv file is generated, you can use the flag --csv_interval. The --tail_length is used to set the number of lines the csv file will have.\ncsvcreator.py This is the code that converts the JSON stored on tinyDB for a csv file. This code is really important, cause our front end is expecting a csv with two columns: timestamp and consumption. In order to run this code, you have to store the data of powergraph.py on the database, as we explained before. To run this use:\npython2.7 csvcreator.py --jsonfile=\u0026quot;generated_json_name\u0026quot;  There are two optional arguments: --date, to create the csv only with the data from a specific day and --name, with the name you want your\n### graph_csv.py This is the code that orchestrates the other ones. It is a multithread code that creates one thread always running with the ```powergraph.py``` code and another one generating a new thread with ```csvcreator``` running from time to time updating the measures. To run this code use:  python2.7 graph_csv.py \u0026ndash;host=\u0026ldquo;server address\u0026rdquo; \u0026ndash;port=\u0026ldquo;server port\u0026rdquo; \u0026ndash;user=\u0026ldquo;allowed user\u0026rdquo; \u0026ndash;passwd=\u0026ldquo;password for this user\u0026rdquo; \u0026ndash;jsonfile=\u0026ldquo;path to bd jsonfile\u0026rdquo; ```\nBesides, you can use the following optional arguments:\n interval: interval between each ipmi measure (default=10) nread: number of ipmi measures to be done (default=infinity) csv_interval: interval that a new csv file is made (deafult=300s) tail_length: size of the csv files (default=300)  Front-end code In order to create an interactive website that plot a graph from the csv file, you first need to deal with the following dependencies:\n apache configurations javascript libraries  After that, we have developed the code in javascript/graph.js where you can read and present in real time the data provided by the back-end.\nJavascript libraries Here we have three libraries included in our html file (index.html).\nThe first one is the D3.js library, a worldwide known tool to create dynamic and interactive data visualizations, in other words, this is the engine of the website. Insert in your html body the \u0026lt;script src=\u0026quot;http://d3js.org/d3.v4.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; to get the most recent code from D3.js.\nThe next library, Moment.js, is used to manipulate date objects and in our case, for example, it allows us to show the time adjusted to the user\u0026rsquo;s location. You can download the code from the following address https://momentjs.com/downloads/moment.min.js.\nFinally, the D3-tip library just inserts tooltips in the graph for a better experience of use. This library was donwloaded from https://github.com/Caged/d3-tip/blob/master/index.js. It is also interesting that you take a look our style implementation for the tooltips from the file style/style.css.\nConclusion If you want to see the project working, acces this link. If you want to contribute, acces the github link. Hope you all enjoy it. Thanks a lot!\n","date":1496102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496102400,"objectID":"835cb41e8d3555711d67d426de1c3537","permalink":"/post/powergraph/","publishdate":"2017-05-30T00:00:00Z","relpermalink":"/post/powergraph/","section":"post","summary":"Introduction to PowerGraph As computers evolve, people are trying new methods do analyse how good some machine is when compared to another one. The amount of energy that some machine is consuming, seems to be a nice measure, once we are willing to produce faster and cheaper. IPMI(Intelligent Platform Management Interface), on the other side, is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system.","tags":null,"title":"Introduction to PowerGraph","type":"post"},{"authors":[],"categories":null,"content":"","date":1494525600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494525600,"objectID":"428526ea820c46e3ce272cc4d7ac8e15","permalink":"/talk/power9/","publishdate":"2017-05-11T18:00:00Z","relpermalink":"/talk/power9/","section":"talk","summary":"Leonardo Garcia é Engenheiro de Computação e mestre em Ciência da Computação pela Universidade Estadual de Campinas (Unicamp) e trabalha no Linux Technology Center da IBM há 11 anos. Ele tem liderado o time de desenvolvimento de KVM para Power há 4 anos.","tags":[],"title":"POWER9: a nova geração de processadores POWER (ou processador para a Era Cognitiva)","type":"talk"},{"authors":null,"categories":null,"content":" #Acessing a Docker Container outside Minicloud Docker containers are widely used nowadays for making software development and delivery easier, since it isolates the container from the rest of the system. This is very useful, cause the developer can install any software, depencies to run the project perfectly, delivering the \u0026ldquo;whole package\u0026rdquo; to anyone who wants to run it. Some applications are expected to access or be accessed from outside, like a webserver, Jenkins, and so on. To do it, you have to map a container port with a server port.\nMaping a Container port with a server port Is very simple and useful to do it. All you have to do is to include a parameter on command line when launching a container with docker run, like this example running a Jenkins container:\ndocker run -i -t -p \u0026quot;physical machine port\u0026quot;:\u0026quot;container port\u0026quot; guilhermeslucas/jenkins:2.0 /bin/bash  In this example Jenkins container is running through port \u0026ldquo;container port\u0026rdquo; and you can access it by reaching the \u0026ldquo;physical machine port\u0026rdquo; of the server.\nAcessing Docker Container from a local browser Some applications are configured or managed using the browser. In this case, you can run the application on a server, but configure it using a ssh tunnel on your local machine. This is very simple too, just add a parameter on the ssh command line, mapping it correctly, like the example.\nssh user@host -L local-port:host:remote-port  it can be used like:\nssh guilherme@123.456.78.910 -L 8080:localhost:8080  In this example, the 8080 remote port will be forward to localhost:8080 and you can access it via browser.\nSo, you\u0026rsquo;ll have to map a container port with a server port and forward this server port on your localhost, on any port not in use.\nThis should do the work.\nWritten by Guilherme Lucas. You can see some of my work at my Github Page.\n","date":1489449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489449600,"objectID":"870fb59ab447475fd2b48d0d90ddaa51","permalink":"/post/external_docker/","publishdate":"2017-03-14T00:00:00Z","relpermalink":"/post/external_docker/","section":"post","summary":"#Acessing a Docker Container outside Minicloud Docker containers are widely used nowadays for making software development and delivery easier, since it isolates the container from the rest of the system. This is very useful, cause the developer can install any software, depencies to run the project perfectly, delivering the \u0026ldquo;whole package\u0026rdquo; to anyone who wants to run it. Some applications are expected to access or be accessed from outside, like a webserver, Jenkins, and so on.","tags":null,"title":"Acessing a Docker Container outside Minicloud","type":"post"},{"authors":[],"categories":null,"content":"","date":1480615200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480615200,"objectID":"72f180f70935ca4f0c35132a949512f1","permalink":"/talk/hhvm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/hhvm/","section":"talk","summary":"Atualmente, paginas da internet são combinações de conteúdos gerados dinamicamente no servidor e programas que fornecem interatividade do lado cliente, tornando-se cada vez mais complexas. Nesse contexto, linguagens dinâmicas para a web como o PHP, são amplamente utilizadas. Entretanto, a medida que a complexidade cresce, ao mesmo tempo, cresce a necessidade de obter-se um desempenho cada vez maior e algumas dessas linguagens não são capazes de tirar o melhor proveito na execução de código nativo pois foram desenvolvidas em uma época onde a maior parte do contéudo para web era estático. Para obter um melhor desempenho tecnicas mais avançadas como JIT (Just In Time compilation) têm sido utilizadas. Nessa palestra iremos explorar os detalhes internos da HipHop Virtual Machine (HHVM) uma maquina virtual de processo baseada em JIT que serve para executar código escrito em PHP e Hack e que permite melhorar o desempenho na execução de programas escritos nessas linguagens. **About the speaker:** Rogerio Alves é mestre em Ciência da Computação com mais de 5 anos de experiência com desenvolvimento de aplicações de baixo nível, além de experiência com linguagens para web. Atualmente, trabalha no Linux Technology Center da IBM com compiladores JIT portando o HHVM para a arquitetura POWER.","tags":[],"title":"HHVM - Uma breve introdução a compiladores JIT para linguagens dinâmicas","type":"talk"},{"authors":null,"categories":null,"content":" How to use the SDAccel Service SDAccel is a service that allow the user to load C/C++ aplications and optmize it using FPGA acceleration. To use this service, first go to the link below:\nhttps://ny1.ptopenlab.com/sdaccel/auth/login/?next=/sdaccel/project/#/projects/b767365a-8402-41a5-97b4-d148c359b114/detail?_k=p64eew  In this page, you can enter your username and password to log in the SDAccel service. If you do not have on account, just create one and get back to that link.\nYou will be redirect to a page with the following tabs: - Overview : this page contains some explanation about how the SDAccel is built and the advantages of using a service like that. - Document : tutorials about SDAccel and some C/C++ code to run. - Project : manage your projects and upload some code.\nTo create a new project, go to Project -\u0026gt; New Project.\nJust put a any name and description and press Submit.\nClick on the project name (that should be blue) and you wil be redirect to a files page.\nGo to the compile tab and wait a little till the loading is finished and, the console password on the password place and hit Enter. A desktop environment will appear on the screen.\nNow, on the virtualized desktop, go to Applications -\u0026gt; System Tools -\u0026gt; Terminal. Now type the following commands:\ncd mkdir test cd test git clone https://github.com/Guilhermeslucas/SDAccel_Examples.git  Now, close the terminal and click on the SDAccel Icon on the desktop, and hit ok on the first window and close the welcome tab.\nOn the Project Explorer -\u0026gt; New -\u0026gt; Xilinx SDAccel Project\nNow, enter any string to be the Project Name and change the locarion for the folder you placed the vadd project(just the src folder, from de SDACell_Examples. I will name the project as tutorial_code. The next step, is to click with the right button of the button on the project folder and hit build project. It will ask you to create a solution. Go ahed and create one. In order to do that click on add(change the name if you want) -\u0026gt; ok -\u0026gt; \u0026gt;\u0026gt; -\u0026gt; ok Now, try to build the project again as we said above (it should take some seconds).\nNow, open a terminal and go to the src folder for the project we are using. In that folder, should appear o .tcl file. Type:\nsdaccel \u0026quot;some_name\u0026quot;.tcl  It will run and the results will appear on the folder.\nNote: if you prefer a video tutorial on YouTube, Bruno made a really good one.\nHope it was helpful.\nPost written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1475712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475712000,"objectID":"621cb3a7a095e4f7fa737d364b71d232","permalink":"/post/sdaccel/","publishdate":"2016-10-06T00:00:00Z","relpermalink":"/post/sdaccel/","section":"post","summary":"How to use the SDAccel Service SDAccel is a service that allow the user to load C/C++ aplications and optmize it using FPGA acceleration. To use this service, first go to the link below:\nhttps://ny1.ptopenlab.com/sdaccel/auth/login/?next=/sdaccel/project/#/projects/b767365a-8402-41a5-97b4-d148c359b114/detail?_k=p64eew  In this page, you can enter your username and password to log in the SDAccel service. If you do not have on account, just create one and get back to that link.\nYou will be redirect to a page with the following tabs: - Overview : this page contains some explanation about how the SDAccel is built and the advantages of using a service like that.","tags":null,"title":"A Brief tutorial about how to use the SDAccel service","type":"post"},{"authors":[],"categories":null,"content":"","date":1475172000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475172000,"objectID":"b5976a390a62a81876c234555154da73","permalink":"/talk/newcomers-oss-community/","publishdate":"2016-09-29T18:00:00Z","relpermalink":"/talk/newcomers-oss-community/","section":"talk","summary":"Open Source Software is an important economic driving force. Companies are aware of the benefits and are adopting OSS as a strategy, opening their source code. However, fostering an OSS developer community is challenging. Newcomers to OSS projects face many technical and social barriers and commonly drop out before making their first contribution. In this keynote, I will talk about how companies are opening their code, the barriers newcomers face to join OSS projects, and FLOSSCoach, a tool we developed to support newcomers first steps. **About the speaker:** Marco Aurélio Gerosa is an Associate Professor in the Computer Science Department at the University of São Paulo (USP), Brazil. His research lies in the intersection between Software Engineering and Social Computing, focusing on the fields of empirical software engineering, mining software repositories, software evolution, and social dimensions of software development. He has published more than 150 peer-reviewed papers. He served as Program Chair at IEEE ICGSE 2016 and PC member in several conferences, such as ACM CSCW, SANER, MSR, etc. In addition to his research, he also coordinates award-winning open source projects.","tags":[],"title":"Leveraging the Crowd: Supporting Newcomers to Build an OSS Community","type":"talk"},{"authors":[],"categories":null,"content":"","date":1474567200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474567200,"objectID":"d34b53bed728098b758e3380377b4ae7","permalink":"/talk/git/","publishdate":"2016-09-22T18:00:00Z","relpermalink":"/talk/git/","section":"talk","summary":"Se você já perdeu horas e horas de trabalho por não ter salvo de forma apropriado o que havia feito, é melhor assistir a palestra desta quinta feita (22 Setembro no IC3, sala 353). Nela Juvenal A. Silva Jr., engenheiro de software com mais de 25 anos de experiência e que atualmente trabalha com GNU Toolchain no Linux Technology Center da IBM, vai mostrar os beneficios de se usar um software de controle de versão para atividades que vão além de versionar código fonte. Depois desta palestra, dizer que seu animal de estimição comeu seu trabalho não será mais aceito :-) **About the speaker:** Juvenal A. Silva Jr. é engenheiro de software com mais de 25 anos de experiência e atualmente trabalha com GNU Toolchain no Linux Technology Center da IBM.","tags":[],"title":"GIT","type":"talk"},{"authors":null,"categories":null,"content":" Setting Up SuperVessel and SSh connection to the Machines on Debian Based SuperVessel is a cloud based plataform created by IBM Research - China It allows you to set up containers on the cloud to run experiments, test aplications, do some data analysis and so on. One great feature of SuperVessel is to allow FPGA and GPU acelleration for C/C++ aplications.\nCreating an acount on SuperVessel First, go to the page below:\nhttps://ptopenlab.com/cloudlabconsole/?cm_mc_uid=35942743919214714482383\u0026amp;cm_mc_sid_50200000=1471628149#/  When you reach this adress, you will see that the first item of the Service Zone is for SuperVessel Cloud. Click on Apply VM -\u0026gt; Direct Acces. After doing that, you can login, or create an account. You can login as Community user . Do one of these steps and you have an SuperVessel account. This wil allow you to use all the other services on the page, like Acceleration and Big Data Services.\nSetting up a Machine After creating an account, you\u0026rsquo;re good to launch a machine. On the left side of the page, click on Instances. On the up menu of the page, there is a Current Region box. Make sure it is assigned to Beijing so we can access the machines via SSH easily. Now, click on Launch Instance. This should redirect you to a page with the specs of the machine on it. On the first drop-down menu, select Launch Docker Image and choose the specs that best fit your needs and then click on the button on the bottom of the forms to launch the instance and wait. It will appear a box with the instance\u0026rsquo;s information. If you want to access the machine from the browser, go to More Actions -\u0026gt; Console. The informations to log in appear on the top of the terminal.\nConnecting VPN First, install the VPNC:\nsudo apt-get install vpnc  Then, create a SuperVessel conf file with the following command:\nsudo vim /etc/vpnc/supervessel.conf  Note: you can use any editor, just don\u0026rsquo;t forget to run with sudo or as root user. The content of this file has to be:\nIPSec gateway 36.110.51.131 IPSec ID Gemini IPSec secret G3m!ni1bmVpn Xauth username PoXXXX (change PoXXXX to your own VPN account) Xauth password secret_password (change secret_passsword to your own VPN password)  The Xauth username and password have to be changed to your personal informattion, that you can find at the person symbol ate the upper left of the SuperVessel page, that you used to create an instance. Click on VPN Conf and put the Beijing fields on the supervessel.conf file. Now, run the vpnc:\nsudo vpnc-connect supervessel.conf  A message like that should appear:\nVPNC started in background (pid: 12434)...  Running SSH Now, on the Instances page, find the External IP(VPN) field. Now run:\nssh opuser@ExternalIP  Note: change ExternalIP with the number you just saw. You can use ssh -X to xforward and ssh -C to compress connection.\nAfter you logout, don\u0026rsquo;t forget to run :\nsudo vpnc-disconnect  That should do the work!\nPost written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1471564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471564800,"objectID":"8f43d7a44051c779f46acfcf0e5e8055","permalink":"/post/supervessel/","publishdate":"2016-08-19T00:00:00Z","relpermalink":"/post/supervessel/","section":"post","summary":"Setting Up SuperVessel and SSh connection to the Machines on Debian Based SuperVessel is a cloud based plataform created by IBM Research - China It allows you to set up containers on the cloud to run experiments, test aplications, do some data analysis and so on. One great feature of SuperVessel is to allow FPGA and GPU acelleration for C/C++ aplications.\nCreating an acount on SuperVessel First, go to the page below:","tags":null,"title":"Setting Up SuperVessel and SSH connection to Machines on Debian Based","type":"post"},{"authors":null,"categories":null,"content":" Continuous integration allows code to be tested automatically every time it’s changed, detecting errors as early as possible. In this tutorial a CI using a GitHub repository will be approached.\nStep 1: Installing and setting up Jenkins and Git To install Jenkins, execute the following commands:\nwget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list' sudo apt-get update sudo apt-get install jenkins  To install git, simply execute:\nsudo apt-get install git  Access Jenkins through http://localhost:8080 and follow the instructions for the initial setup. Choose Install suggested plugins when asked.\nStep 2: Creating a job In Jenkins dashboard, click on New Item, give your project a name and select Freestyle project.\nYou may choose Discard old builds in order to avoid using too much storage in the long term.\nCheck GitHub project and enter the GitHub URL of the project. Use the format https://github.com/YOUR-USERNAME/YOUR-REPOSITORY\nIn source code management section, choose Git and enter the repository URL the same way as above.\nStep 2.1: Choosing the build trigger Under Build Triggers it is possible to choose to build periodically or when a change is pushed into GitHub. Although building only when GitHub changes is more efficient, it is required to your Jenkins server to be accessible through the internet, and the you must own the repository. Building periodically may waste resources, but it is simpler to configure.\nStep 2.1.1: Build Periodically Check Build Periodically and define the period using the proper syntax found when clicking the ?.\nComplete the job creating by adding a build step (e.g. a shell script to compile and run a test) and jump to step 4\nThe test input and expected output should be in the repository.\nStep 2.1.2: Build when a change is pushed into GitHub Check build when a change is pushed into GitHub\nComplete the job creating by adding a build step (e.g. a shell script to compile and run a test) and follow to step 3\nThe test input and expected output should be in the repository.\nStep 3: Configuring GitHub plugin - Skip if building periodically Go to Manage Jenkins → Configure System → GitHub section → Advanced → Manage additional GitHub Actions → Convert login and password to token\nA new sub-section will appear right above.\nSelect From login and password, fill your login and password from GitHub and press Create token credentials\nAbove this sub-section, click Add GitHub server. Keep the API URL unchanged.\nUnder Credentials dropdown menu, select the token just created and test your connection.\nStep 4: Testing it If using periodical build, click the Build now icon to test. If the test fails, check the console output to find the issue (e.g. missing compiler).\nIf using GitHub trigger, change a file in the repository. The build should start automatically in a few seconds.\n#Step 5: Adding slave machines - Optional\nAs your projects grow, you may run out of resources in your machine. A possible solution is to add one or more slave machines, which will be responsible for building your projects, while the current machine will become the master and manage everything (the master will still be able to run jobs if desired).\nThe slave machine doesn\u0026rsquo;t need Jenkins installed on it. There are many ways to connect the slave with the master, here, SSH will be used.\nInstall Java and Git in the slave using:\nsudo apt-get install default-jre sudo apt-get install git  Create a directory to be used by Jenkins, in this case will be the same path used by default in the master machine: /var/lib/jenkins\nsudo mkdir /var/lib/jenkins  Change the ownership of the directory to the same user used to login using SSH\nchown ubuntu:ubuntu /var/lib/jenkins  Back to the master machine:\nGo to Manage Jenkins → Manage Nodes → New Node\nName your node and select Permanent Agent\nThe recommended # of executors is the number of cores in the slave machine\nThe Remote root directory is the path to the directory created.\nIf necessary to divide the slave machines into different groups, label them (e.g. the OS running in the machine, the CPU architecture)\nThe Launch method used here will be SSH, but other methods are also fine.\nSimply enter your host and create a credential using your username and password, or username and private key.\nPress Save\n##Step 5.1: Restricting machines where projects can be run\nIf your slaves have different environments, your should restrict the machines where each project will run.\nUnder the project settings, check Restrict where this project can be run and type the machine name, use a label, or even use a more complex rule using logical operators (click the ? for more information)\nTo prevent the master machine to run projects, go to Manage Jenkins → Manage Nodes → master → Configure → # of executors and set to 0.\n","date":1465948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465948800,"objectID":"6f29ac7afa870784ac93f27093588259","permalink":"/post/building-a-continuous-integration-platform-using-jenkins-and-github/","publishdate":"2016-06-15T00:00:00Z","relpermalink":"/post/building-a-continuous-integration-platform-using-jenkins-and-github/","section":"post","summary":"Continuous integration allows code to be tested automatically every time it’s changed, detecting errors as early as possible. In this tutorial a CI using a GitHub repository will be approached.\nStep 1: Installing and setting up Jenkins and Git To install Jenkins, execute the following commands:\nwget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list' sudo apt-get update sudo apt-get install jenkins  To install git, simply execute:","tags":null,"title":"Building a continuous integration platform using Jenkins and GitHub","type":"post"},{"authors":[],"categories":null,"content":"","date":1465408800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465408800,"objectID":"6d1b1bee3048596e0fd9ec7e37cda8ec","permalink":"/talk/porting-to-power/","publishdate":"2016-06-08T18:00:00Z","relpermalink":"/talk/porting-to-power/","section":"talk","summary":"Manter sistemas legados em operação em novas arquiteturas representa um desafio tecnológico. Algumas empresas chegam a investir milhares de dólares na tentativa de migrar seus sistemas para novas arquiteturas. Contudo, em virtude da complexidade de algumas aplicações, o resultado final da migração nem sempre sai conforme o esperado. Variações sensíveis entre as arquiteturas, como diferenças no conjunto de instruções, da ABI ou na forma como os bytes são organizados em memória, podem ocasionar bugs difíceis de serem detectados durante a fase de teste. Em alguns casos, a migração pode ter um resultado muito aquém do esperado, reduzindo drasticamente o desempenho da aplicação. Nessa palestra, abordaremos alguns problemas clássicos de migração e como devemos proceder para resolvê-los, de forma a reduzir o impacto gerado no desempenho e na precisão dos resultados após a migração. Além disso, apresentaremos de forma didática alguns cuidados que devemos ter quando buscamos desenvolver programas portáveis. **About the speaker:** Alisson Linhares é mestre em Ciência da Computação pela Unicamp e tem mais de 7 anos de experiência com desenvolvimento de aplicações de baixo nível. Atualmente, trabalha no Linux Technology Center da IBM desenvolvendo o IBM SDK (kit de desenvolvimento oficial para Linux nos processadores Power).","tags":[],"title":"Migrando aplicações para a arquitetura Power","type":"talk"},{"authors":null,"categories":null,"content":" Downloading Packages You can use a script to download the necessary packages in the link above:\nftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/at_downloader/?cm_mc_uid=92476109699314629396752\u0026amp;cm_mc_sid_50200000=1464625581  Download the script and change its permission with the command above:\nchmod +x \u0026lt;script name\u0026gt;  And then run the script with:\n./\u0026lt;script name\u0026gt;  Please download the packages for Ubuntu 14.10. Then, the folder will be full of .deb files. The next step is to install some of these packages using dpkg.\nInstaling Packages and Toolchain Now, you need to install some of these. This is simple, but you have to do it following the order above. The command for each of those is\ndpkg -i \u0026lt;package name\u0026gt;  The correct order is (the X is the version of the software):\nadvance-toolchain-atX.X-runtime-X.X-X advance-toolchain-atX.X-devel-X.X-X advance-toolchain-atX.X-perf-X.X-X advance-toolchain-atX.X-mcore-libs-X.X-X  You can ignore the errors and move on.\nInstalling IBM SDK After all the dependencies issues are solved, download fdpr_wrap, fdpr-pro, pthread-mon, ibm-sdk-lop and ibm-sdk-lop-remote-dependencies on this link:\nhttps://www-304.ibm.com/webapp/set2/sas/f/lopdiags/sdkdownload.html#4  Install each of these packages with dpkg again, running:\nsudo dpkg -i \u0026lt;package name\u0026gt;  respecting the sequence above.\nNote: to run the ibm-sdk-loop on the Power Machines using ssh, you will have to connect to the server using\nssh -XC -c blowfish-cbc,arcfour \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt; -p \u0026lt;port_number\u0026gt;  The blowfish-cbc,arcfour will make your connection even better. This command is necessary to xforward the image of the sdk running. Note2: if you get the \u0026ldquo;no matching cipher found\u0026rdquo; error, here you go the solution:\nrun the command above:\nssh -Q cipher localhost | paste -d , -s  Now its time to change the sshd_config file on the server(super privileges needed). Add on the /etc/ssh/sshd_config a line with:\nCiphers \u0026lt;output of the last command\u0026gt;  Reboot the machine to make this alterations running:\nsudo shutdown -r now  Post written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1464566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464566400,"objectID":"0a01874d72b3ee4a153ed48923bdf551","permalink":"/post/sdk/","publishdate":"2016-05-30T00:00:00Z","relpermalink":"/post/sdk/","section":"post","summary":"Downloading Packages You can use a script to download the necessary packages in the link above:\nftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/at_downloader/?cm_mc_uid=92476109699314629396752\u0026amp;cm_mc_sid_50200000=1464625581  Download the script and change its permission with the command above:\nchmod +x \u0026lt;script name\u0026gt;  And then run the script with:\n./\u0026lt;script name\u0026gt;  Please download the packages for Ubuntu 14.10. Then, the folder will be full of .deb files. The next step is to install some of these packages using dpkg.","tags":null,"title":"How to Install IBM SDK on Ubuntu 16.04/14.04 Power Machines","type":"post"},{"authors":null,"categories":null,"content":" Optimizing C/C++ Applications with IBM SDK Build Advisor This is a brief text about how to use the IBM SDK Build Advisor to optimize C/C++ aplications on Power Servers. You can get the project and learn how to run on the link below:\nhttps://github.com/Guilhermeslucas/cmp  Running seismic applications without optimization flags Here are some results on Power Machine before the optimization process.\nreal 0m46.837s\nreal 0m48.391s\nreal 0m52.570s\nreal 0m49.249s\nreal 0m48.863s\nImporting a C\\C++ Project to IBM SDK Before you begin, make sure there is a Makefile inside your project. Now, inside the SDK:\n1. Go to File \u0026gt; Import 2. In the import window, expand C\\C++ and click in Existing Code as Makefile Project 3. Now go to Browse next to the Existing code Location. 4. Type a name for your project 5. Locate the code and then click OK. 6. Back to the Import Existing Code window, click the Advanced Toolchain Version corresponding to the one you have installed on the Power Machine. 7. Click Finish\nUsing the Build Advisor  Right Click on the project, go to Properties Select the build advisor Enable Enable extra advice and then Finish. Right click on the project and build. The suggestions will appear.  Final results After using the flags that the SDK suggested\n-std=c99 -Ofast -fpeel-loops -flto -fopenmp -mcmodel=medium -ftree-vectorize -mcpu=power8 -mtune=power8 -funroll-loops  I got the following results:\nreal 0m3.177s\nreal 0m2.573s\nreal 0m3.066s\nreal 0m2.954s\nreal 0m2.930s\nAs you can see, Build Advisor is very effective.\nPost written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1464566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464566400,"objectID":"546f36a957ae777bce116bf53d7337a7","permalink":"/post/sdk_opt/","publishdate":"2016-05-30T00:00:00Z","relpermalink":"/post/sdk_opt/","section":"post","summary":"Optimizing C/C++ Applications with IBM SDK Build Advisor This is a brief text about how to use the IBM SDK Build Advisor to optimize C/C++ aplications on Power Servers. You can get the project and learn how to run on the link below:\nhttps://github.com/Guilhermeslucas/cmp  Running seismic applications without optimization flags Here are some results on Power Machine before the optimization process.\nreal 0m46.837s\nreal 0m48.391s\nreal 0m52.570s\nreal 0m49.","tags":null,"title":"Optimizing C/C++ applications","type":"post"},{"authors":null,"categories":null,"content":" Instalação do SU e do Cmp_toy na máquina pessoal Instalação do SU Seismic Unix é um conjunto de ferramentas extremamente úteis para o processamento de dados sísmicos. Para instalá-lo no Ubuntu 14.04, precisei baixar algumas bibliotecas. Segue quais são e como instalá-las:\n\nsudo apt-get install libx11-dev  \n\u0026lt;X11/Intrinsic.h\u0026gt; - sudo apt-get install libxt-dev  Também precisamos do CMake para completar a instalação do Seismic Unix. Rode:\nsudo apt-get install cmake  Link do repositório do cmp_toy:\ngithub.com/gga-cepetro/cmp  Agora para realmente terminar a instalção, basta rodar os seguintes comandos:\ninstall_dir=~/src/cwp mkdir -p $install_dir \u0026amp;\u0026amp; cd $install_dir export CWPROOT=$PWD echo \u0026quot;export CWPROOT=$PWD\u0026quot; \u0026gt;\u0026gt; ~/.bashrc echo 'export PATH=$PATH:$CWPROOT/bin' \u0026gt;\u0026gt; ~/.bashrc wget ftp://ftp.cwp.mines.edu/pub/cwpcodes/cwp_su_all_44R1.tgz tar zxf cwp_su_all_44R1.tgz cd src sed -i 's/^XDRFLAG/#XDRFLAG/' Makefile.config make install ; make xtinstall  Agora para testar se o software, basta executar\nsuplane | suximage  Uma imagem com três planos deve ser apresentada. Ela parece ser um pouco esquista, a princípio, mas se uma janela com uma imagem foi aberta, então a instalação foi realizada com sucesso.\nInstalação do cmp_toy Agora para instalar o cmp_toy e obter as curvas desejadas, precisamos executar os seguintes comandos, após obter o cmp_toy.tar.gz :\ntar -xzf cmp_toy.tar.gz mkdir build cd build cmake .. make  Agora que o software está instalado, vamos testá-lo. É necessário mudar para o diretório pai desse que estamos e rodar o script de teste, que usa uma imagem criada extamente para esse teste:\ncd .. ./test-cmp.sh  A saída desse comando (ao completar 100%) deve seguir o seguinte padrão, com números possivelmente diferentes:\n[100%] Processing CDP 300 real\t0m56.573s user\t0m56.589s sys\t0m0.008s  Agora, para testar a imagem gerada, basta rodar o seguinte comando:\nsuximage \u0026lt; cmp.stack.su  O output desse comando deve ser uma imagem com o plot de algumas curvas.\nPost escrito por Guilherme Lucas. Mais um pouco do meu trabalho no meu Github https://github.com/Guilhermeslucas .\n","date":1463702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463702400,"objectID":"d477cdca83f937f59fce331ee2527fb6","permalink":"/post/ic/","publishdate":"2016-05-20T00:00:00Z","relpermalink":"/post/ic/","section":"post","summary":"Instalação do SU e do Cmp_toy na máquina pessoal Instalação do SU Seismic Unix é um conjunto de ferramentas extremamente úteis para o processamento de dados sísmicos. Para instalá-lo no Ubuntu 14.04, precisei baixar algumas bibliotecas. Segue quais são e como instalá-las:\n\nsudo apt-get install libx11-dev  \n\u0026lt;X11/Intrinsic.h\u0026gt; - sudo apt-get install libxt-dev  Também precisamos do CMake para completar a instalação do Seismic Unix. Rode:\nsudo apt-get install cmake  Link do repositório do cmp_toy:","tags":null,"title":"Instalação de Seismic Unix e Cmp_toy em arquitetura Intel","type":"post"},{"authors":null,"categories":null,"content":" Tutorial de instalação do cmp_toy em Máquinas Power Esse é um breve tutorial de como foi para instalar e rodar o código sísmico cmp_toy nas máquinas Power, acessado pelo sistema de Minicloud. Todo o processo foi feito usando uma máquina com Ubuntu 16.04 instalado, mas também deve funcionar em máquinas com sistemas Debian Based.\nResolução de problemas nos pacotes Ao logar na máquina, tive alguns problemas com pacotes quebrados. Para arrumar esse problema usei os comandos comandos do dpkg e do apt-get, respectivamente:\nsudo dpkg --configure -a sudo apt-get -f install  Primeiros Softwares e Pacotes necessários O primeiro software que precisei, foi o editor de texto Vim, muito poderoso e eficiente, ainda mais quando estamos acessando uma máquina remotamente. Para instalá-lo basta executar:\nsudo apt-get install vim  Após copiar o meu .vimrc para a máquina remota, baixei os dois compiladores necessários para rodar o código. Pra esse, preciso do compilador de C e C++, e por preferências pessoais instalei o gcc e g++. Segue os comandos para instalação de cada um deles:\nsudo apt-get install gcc sudo apt-get install g++  Link do repositório do cmp_toy:\ngithub.com/gga-cepetro/cmp  Durante a instalação é necessário rodar o CMake, que também não esta instalado. Para acertar esse problema, rode:\nsudo apt-get install cmake  Vale lembrar que todos os comandos desde o início do desse tutorial devem ser feitos logado na máquina remota.\nTrocando arquivos remotamente e instalando do software Como fazemos acesso às máquinas com o comando ssh -p , ou seja, acessamos por meio de uma porta específica, temos que executar o comando scp de maneira diferente também, para copiar um arquivo local para uma máquina remota. Nesse caso, queremos copiar o arquivo cmp_toy.tar.gz (você deve estar no diretório desse arquivo, ou seja, na sua máquina física e não logado nas máquinas remotas).\nscp -P \u0026lt;numero da porta\u0026gt; cmp_toy.tar.gz seu_usuario@host_remoto:/algum/diretorio/remoto  Agora, se conectado na máquina remota, e vá até o diretório onde mandou o arquivo. Voce deve encontrar o cmp_toy.tar.gz. Agora para instalá-lo, rode os seguintes comandos:\ntar -xzf cmp_toy.tar.gz cd cmp_toy mkdir build cd build cmake .. make  Agora que a instalação do software foi feita com sucesso, precisamos testá-lo. Para isso, basta ir para o diretório pai e rodar o script de teste, que usa uma imagem pronta para teste.\ncd .. ./test-cmp.sh  Após um tempinho, você deve receber uma saída que segue o seguinte padrão (não necessariamente com os mesmos números):\n[100%] Processing CDP 300 real\t0m59.968s user\t0m59.908s sys\t0m0.056s  Basicamente, os passos são esses. Tentei fazer esse tutorial da maneira mais detalhada possível. Se ainda restarem dúvidas, não exitem em entrar em contato.\nPost escrito por Guilherme Lucas. Mais um pouco do meu trabalho no meu Github https://github.com/Guilhermeslucas .\n","date":1463702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463702400,"objectID":"adfc65621c6840d992fe706d1cf4c6d9","permalink":"/post/power/","publishdate":"2016-05-20T00:00:00Z","relpermalink":"/post/power/","section":"post","summary":"Tutorial de instalação do cmp_toy em Máquinas Power Esse é um breve tutorial de como foi para instalar e rodar o código sísmico cmp_toy nas máquinas Power, acessado pelo sistema de Minicloud. Todo o processo foi feito usando uma máquina com Ubuntu 16.04 instalado, mas também deve funcionar em máquinas com sistemas Debian Based.\nResolução de problemas nos pacotes Ao logar na máquina, tive alguns problemas com pacotes quebrados. Para arrumar esse problema usei os comandos comandos do dpkg e do apt-get, respectivamente:","tags":null,"title":"Instalação de Seismic Unix e Cmp_toy em arquitetura Power","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":[],"categories":null,"content":"","date":1442858400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442858400,"objectID":"34de75fdb75d5510e2f33b9faec1cd86","permalink":"/talk/opensource-development/","publishdate":"2015-09-21T18:00:00Z","relpermalink":"/talk/opensource-development/","section":"talk","summary":"O palestrante irá abordar o tema Desenvolvimento OpenSource: Participando das comunidades, enviando patches, o que não fazer, como fazer... Buscando explanar como os aspirantes ao desenvolvimento opensource podem dar os primeiros passos. **About the speaker:** Leônidas S. Barbosa a.ka leosilva a.k.a Kirotawa, trabalho no Linux Technology Center na IBM no time de segurança.Atualmente é maintainer dos drivers nx-crypto e vmx-crypto, drivers cryptográficos suportados em p7+ e p8. Desenvolvedor aventureiro em tempo livre, sempre que possível coda alguma coisa e pública no .git. Sempre quis trabalhar com OpenSource e com Kernel. Aspira um dia enviar um patch para o Kexec ou Efi subsystem no Linux Kernel.","tags":[],"title":"Alice no País do Desenvolvimento OpenSource","type":"talk"},{"authors":null,"categories":null,"content":"Glance is the Openstack Image Service and enables users to discover, register, and retrieve virtual machine images. This service allows users to query virtual machine images information and retrieve the image itself using a REST API and must be installed in the controller node.\n#Prerequisites\nLoad the OpenRC file:\nsource adm-credentiansl.sh  Create a database for Glance:\nmysql -u root -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance'; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';  Create the keystone entities, service and endpoint:\nopenstack user create --password-prompt glance openstack role add --project service --user glance admin openstack service create --name glance --description \u0026quot;OpenStack Image service\u0026quot; image openstack endpoint create --publicurl http://controller:9292 \\ --internalurl http://controller:9292 --adminurl http://controller:9292 \\ --region RegionOne image  #Install and Configure\nInstall the packages:\napt-get install glance python-glanceclient  Edit the file /etc/glance/glance-api.cnf as following:\n[DEFAULT] ... notification_driver = noop verbose = True [database] connection = mysql://glance:GLANCE_DBPASS@controller/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] flavor = keystone [glance_store] default_store = file filesystem_store_datadir = /var/lib/glance/images/  Also edit the file /etc/glance/glance-registry.cnf:\n[DEFAULT] ... notification_driver = noop verbose = True [database] connection = mysql://glance:GLANCE_DBPASS@controller/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] flavor = keystone  Populate the image service database:\nsu -s /bin/sh -c \u0026quot;glance-manage db_sync\u0026quot; glance  Finalize the installation process:\nservice glance-registry restart service glance-api restart rm -f /var/lib/glance/glance.sqlite  #Greenlet Fix\nThe Greenlet version that is in the Ubuntu repository has problems with PPC64le architecture, therefore we need to manually download and install a newer version of the library.\nDownload and unpack the package code:\nwget https://github.com/python-greenlet/greenlet/archive/master.zip -D greenlet.zip unzip greenlet.zip -D cd greenlet  Install the package:\nexport CFLAGS=-O1; ./setup.py install  Restart the glance services:\nservice glance-registry restart service glance-api restart  #Upload an image to the Image Server\nConfigure the server to use the API version 2.0 and reload the credentials:\necho \u0026quot;export OS_IMAGE_API_VERSION=2\u0026quot; | tee -a adm-credentials.sh source adm-credentials.sh  As an example upload Ubuntu 14.04.02 PPC64le to the server:\nglance image-create --name=\u0026quot;ubuntu1404-ppc64le\u0026quot; --disk-format=qcow2 --container-format=bare \\ --is-public=true \\ -copy-from https://cloud-images.ubuntu.com/releases/14.04/14.04.2/ubuntu-14.04-server-cloudimg-ppc64el-disk1.img  Check the list of images:\nglance image-list  ","date":1442361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442361600,"objectID":"8fdadf041d1ad60f6fd073089a2356a7","permalink":"/post/openstack_setup_4/","publishdate":"2015-09-16T00:00:00Z","relpermalink":"/post/openstack_setup_4/","section":"post","summary":"Glance is the Openstack Image Service and enables users to discover, register, and retrieve virtual machine images. This service allows users to query virtual machine images information and retrieve the image itself using a REST API and must be installed in the controller node.\n#Prerequisites\nLoad the OpenRC file:\nsource adm-credentiansl.sh  Create a database for Glance:\nmysql -u root -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance'; GRANT ALL PRIVILEGES ON glance.","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 04 – Glance","type":"post"},{"authors":null,"categories":null,"content":"This service provides a central directory of users mapped to the OpenStack services. It\u0026rsquo;s used to provide an authentication and authorization service for other OpenStack services. In addition to the identity service, we will install two more packages, the Apache HTTP server and the Memcahed, responsible, respectively, for receiving requests and store the authentication tokens.\n#Prerequisites\nAll the keystone data will be stored in a database, acess the mysql and execute the following commands:\n\u0026gt;mysql -u root -p CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone'; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';  #Install and Configure\nThe version Kilo uses a WSGI server to listen the requests to keystone, we choose to use the Apache Server running a WSGI mod.\nExecute the following commands to disable keystone automatic start and install the packages:\n\u0026gt;echo \u0026quot;manual\u0026quot; \u0026gt; /etc/init/keystone.override apt-get install keystone python-openstackclient apache2 libapache2-mod-wsgi \\ memcached python-memcache  Edit the following sections in the file /etc/keystone/keystone.conf:\n\u0026gt;[DEFAULT] ... admin_token = ADMIN_TOKEN verbose = True [database] connection = mysql://keystone:KEYSTONE_DBPASS@controller/keystone [memcache] servers = localhost:11211 [token] provider = keystone.token.providers.uuid.Provider driver = keystone.token.persistence.backends.memcache.Token [revoke] driver = keystone.contrib.revoke.backends.sql.Revoke  Create the file /etc/apache2/sites-available/wsgi-keystone.conf with the following content:\n\u0026gt;Listen 5000 Listen 35357 \u0026lt; VirtualHost *:5000 \u0026gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /var/www/cgi-bin/keystone/main WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On \u0026lt; IfVersion \u0026gt;= 2.4 \u0026gt; ErrorLogFormat \u0026quot;%{cu}t %M\u0026quot; \u0026lt; /IfVersion \u0026gt; LogLevel info ErrorLog /var/log/apache2/keystone-error.log CustomLog /var/log/apache2/keystone-access.log combined \u0026lt; /VirtualHost \u0026gt; \u0026lt; VirtualHost *:35357 \u0026gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /var/www/cgi-bin/keystone/admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On \u0026lt; IfVersion \u0026gt;= 2.4 \u0026gt; ErrorLogFormat \u0026quot;%{cu}t %M\u0026quot; \u0026lt; /IfVersion \u0026gt; LogLevel info ErrorLog /var/log/apache2/keystone-error.log CustomLog /var/log/apache2/keystone-access.log combined \u0026lt; /VirtualHost \u0026gt;  Install and configure the WSGI mod for Apache:\n\u0026gt;ln -s /etc/apache2/sites-available/wsgi-keystone.conf /etc/apache2/sites-enabled mkdir -p /var/www/cgi-bin/keystone curl http://git.openstack.org/cgit/openstack/keystone/plain/httpd/keystone.py?h=stable/kilo \\ | tee /var/www/cgi-bin/keystone/main /var/www/cgi-bin/keystone/admin chown -R keystone:keystone /var/www/cgi-bin/keystone chmod 755 /var/www/cgi-bin/keystone/*  Restart the Apache service:\n\u0026gt;rm -f /var/lib/keystone/keystone.db service apache2 restart  #Create the service endpoint\nTo really have the Keystone service working is needed to create a service endpoint to listen requests:\nExport the following lines to obtain administrator privileges on Keystone:\n\u0026gt;export OS_TOKEN=ADMIN export OS_URL=http://controller:35357/v2.0  Registry the service:\n\u0026gt;openstack service create --name keystone --description \u0026quot;OpenStack Identity\u0026quot; identity openstack endpoint create --publicurl http://controller:5000/v2.0 \\ --internalurl http://controller:5000/v2.0 --adminurl http://controller:35357/v2.0 \\ --region RegionOne identity  #Create the environment entities\nEach of the services that compose OpenStack uses the keystone as the authentication point, this process involves a combination of various entities (users, projects, etc.). You can better understand the functioning of Keystone at the documentation page:\nCreate the Admininstration and Demo projetct:\n\u0026gt;openstack project create --description \u0026quot;Admin Project\u0026quot; admin openstack project create --description \u0026quot;Demo Project\u0026quot; demo  Create users and roles and after make sure each user is registred in one role:\n\u0026gt;openstack role create admin openstack role create user openstack user create --password-prompt admin openstack user create --password-prompt demo openstack role add --project admin --user admin admin openstack role add --project demo --user demo user  Each service that will be installed is represent as an user in Keystone, they will be part of an project that will contain all the services:\n\u0026gt;openstack project create --description \u0026quot;Service Project\u0026quot; service  #OpenRC files\nTo simplify the keystone authentication process create a file that contains the credenditals and export to the system.\nCreate the file adm-credentials.sh and add the following:\n\u0026gt;export OS_PROJECT_DOMAIN_ID=default export OS_USER_DOMAIN_ID=default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=SENHA_ADMIN export OS_AUTH_URL=http://controller:35357/v3  Load the file content:\n\u0026gt;source adm-credentials.sh  ","date":1441756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441756800,"objectID":"c3d7e0b566723996e24d685f1ffe031a","permalink":"/post/openstack_setup_3/","publishdate":"2015-09-09T00:00:00Z","relpermalink":"/post/openstack_setup_3/","section":"post","summary":"This service provides a central directory of users mapped to the OpenStack services. It\u0026rsquo;s used to provide an authentication and authorization service for other OpenStack services. In addition to the identity service, we will install two more packages, the Apache HTTP server and the Memcahed, responsible, respectively, for receiving requests and store the authentication tokens.\n#Prerequisites\nAll the keystone data will be stored in a database, acess the mysql and execute the following commands:","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 03 – Keystone","type":"post"},{"authors":null,"categories":null,"content":"The environment will consist of two Power8 machines with Ubuntu 15.04 installed, one of the machines will contain the core cloud services (controller node), and the other one the virtualization services (compute node). In this guide we are going to install the newest OpenStack release, the Kilo version. However, before installing the cloud services it\u0026rsquo;s necessary to properly set up the environment, this post will cover all the datails.\n#Passwords\nAs a convention, passwords for each service will be the service name in lowercase, do not forget that in a real environment passwords must be chosen carefully.\n#Network\nIt\u0026rsquo;s possible to install OpenStack with two different network architectures, legacy networking (nova-network) and Neutron. Initially we will use nova-network, our network environment is represented as shown below:\nNote that we have two networks, the 10.0.0.0/24 is the management network (where the nodes will establish comunicate) and the 10.0.2.0/24 is the external network (each created VM will have an IP external acessible).\nEdit the file /etc/hosts on all machines and add the following:\n#controller 10.0.0.10 controller #compute 10.0.0.11 compute01  #Network Time Protocol (NTP)\nIn order to synchronize services installed on different nodes we will install NTP.\nFirst of all install NTP on all machines:\napt-get install ntp  ##On controller\nEdit the file /etc/ntp.conf and add or edit the follwing lines:\nserver 0.ubuntu.pool.ntp.org iburst restrict -4 default kod notrap nomodify restrict -6 default kod notrap nomodify  Restart the NTP service:\nservice ntp restart  ##On compute\nEdit the file /etc/ntp.conf and change the server to:\nserver controller iburst  Restart the NTP service:\nservice ntp restart  #Openstack and system packages\nWe have to configure the package respository to point to the Openstack Kilo release and verify if the system is up-to-date. Perform the followig step on all nodes.\nInstall Ubuntu keyring and set the Kilo repository:\napt-get install ubuntu-cloud-keyring echo \u0026quot;deb http://ubuntu-cloud.archive.canonical.com/ubuntu\u0026quot; \\ \u0026quot;trusty-updates/kilo main\u0026quot; \u0026gt; /etc/apt/sources.list.d/cloudarchive-kilo.list  Upgrade the packages on your system:\napt-get update apt-get dist-upgrade  #SQL database\nThe SQL database will be installed only on controlle node, the openstack services mostly use a database to store all the information they need. We choose to install MySQL server.\nInstall packages:\napt-get install mysql-server python-mysqldb  Edit the file /etc/mysql/mysql.conf.d/mysqld.conf in the [mysqld] section:\n[mysqld] ... bind-address = 10.0.0.10 default-storage-engine = innodb innodb_file_per_table collation-server = utf8_general_ci init-connect = 'SET NAMES utf8' character-set-server = utf8  Restart the Mysql service:\nservice mysql restart  Execute the following mysql script to secure the database service:\nmysql_secure_installation  ##Message Queue\nOpenstack uses the strategy of a queue message to coordinate the actions related to the services, in other words, a message queue running on the controller node coordinates the comunication between the services in order to have all the services properly. In this guide we will use a message queue server called RabbitMQ.\nInstall the packages:\napt-get install rabbitmq-server  Create the openstack user:\nrabbitmqctl add_user openstack RABBIT_PASS  As mentioned in password section, replace RABBIT_PASS by rabbit.\nConfigure permissions to openstack user:\nrabbitmqctl set_permissions openstack \u0026quot;.*\u0026quot; \u0026quot;.*\u0026quot; \u0026quot;.*\u0026quot;  ##Next steps\nWith the environment properly configured we can setup the OpenStack services, in the next post we will configure the identity service, known as Keystone.\n","date":1440115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440115200,"objectID":"5626bb191c7615178eca24b5cd833a8a","permalink":"/post/openstack_setup_2/","publishdate":"2015-08-21T00:00:00Z","relpermalink":"/post/openstack_setup_2/","section":"post","summary":"The environment will consist of two Power8 machines with Ubuntu 15.04 installed, one of the machines will contain the core cloud services (controller node), and the other one the virtualization services (compute node). In this guide we are going to install the newest OpenStack release, the Kilo version. However, before installing the cloud services it\u0026rsquo;s necessary to properly set up the environment, this post will cover all the datails.\n#Passwords","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 02 – Environment Setup","type":"post"},{"authors":null,"categories":null,"content":"#Introduction\nIn this series we are going to detail all the necessary steps to setup an OpenStack-based cloud with IBM POWER8 machines from the scratch, but first of all, let\u0026rsquo;s take a look at some basic questions like what is OpenStack and why one would want to install and use it?\n#What is OpenStack?\nOpenStack is a free and open-source set of connected components aiming to serve as an cloud computing operating system capable of managing large pools of compute, storage and networking resources, all managed through a administrator dashboard. It\u0026rsquo;s robustness and reliability as one of the most active open-source project today makes it an really good choice for offering cloud computing services (IaaS) on standarized hardware, and due to its simplicity and massive scalability it can be used as an solution for a large amout of users, from a small home environments with few machines to large datacenters with hundreds of machines.\nThe OpenStack project began first in 2010 as an joint project of Rackspace Hosting and NASA, today, the project itself is managed by the OpenStack Foundation and have more than 500 supporters among companies and research centers.\n#Components\nThe main project is implemented in a modular architecture with many components, each one performing its own responsibility in the system. The diagram below can be found at the OpenStack official documentation:\n##Horizon (Dashboard)\nThis component responsibility consists in providing a web-based interface to easily access the OpenStack services.\n##Compute (Nova)\nThe main part of any IaaS system, the Nova project performs the controller role, managing and automating pools of computer resources. It supports many virtualization technologies and is it responsibility to manage the virtual machines on the system.\n##Networking (Neutron)\nManages the networks and IP adresses, providing users total control over network configurations. Standard network models works with separate VLANs for each user to distribute the network access, the Neutron component treats the question differently, managing the IP adressess, allowing a more flexible and maintainable network usage.\n##Object Storage (Swift)\nThe Swift component stores and retrieves unstructured data object through the HTTP based APIs.\n##Block Storage (Cinder)\nProvides persistent storage to running services, its implemented in such a way that makes creating and managing block storage very easy.\n##Identity Service (Keystone)\nThis provides a central directory of users mapped to the OpenStack services. It is used to provide an authentication and authorization service for other OpenStack services.\n##Image Service (Glance)\nThis provides the discovery, registration and delivery services for the disk and server images. It stores and retrieves the virtual machine disk image.\n##Telemetry (Ceilometer)\nIt monitors the usage of the Cloud services and decides the billing accordingly. This component is also used to decide the scalability and obtain the statistics regarding the usage.\n##Orchestration (Heat)\nThis component manages multiple Cloud applications through an OpenStack-native REST API and a CloudFormation-compatible Query API.\n#Why would I use OpenStack?\nIn sight of all the features listed above and all the benefits that OpenStack can offer to its users and administrators we choose it to serve as infrastructure to our POWER8-based cloud, the steps to setup and manage the system will be discussed throughout the next series posts, keep in touch!\n#Sources:\nWikipedia OpenStack article\nOfficial OpenStack documentation\n","date":1440028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440028800,"objectID":"022bf125ca7c73d7d84888222ae5613d","permalink":"/post/openstack_setup_1/","publishdate":"2015-08-20T00:00:00Z","relpermalink":"/post/openstack_setup_1/","section":"post","summary":"#Introduction\nIn this series we are going to detail all the necessary steps to setup an OpenStack-based cloud with IBM POWER8 machines from the scratch, but first of all, let\u0026rsquo;s take a look at some basic questions like what is OpenStack and why one would want to install and use it?\n#What is OpenStack?\nOpenStack is a free and open-source set of connected components aiming to serve as an cloud computing operating system capable of managing large pools of compute, storage and networking resources, all managed through a administrator dashboard.","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 01 – Introduction","type":"post"},{"authors":null,"categories":null,"content":" There are many reasons why one would want to build its custom router instead of buying one. Control and flexibility are two reasons, and we need both when dealing with large traffic. The purpose of this guide is to give a step-by-step solution starting on how to build a virtual machine. For this, we will assume that PowerKVM is already up and running along with its network configurations.\nSo, for this guide we will need:\n A PowerKVM machine Two network cards  In this case, eth0 will be our internal network interface and eth1 our external network interface. Both of them will be bridged to the virtual machine and this configuration can be made through Kimchi\u0026rsquo;s web interface.\nCreating a Debian Virtual Machine Downloading the right ISO First we\u0026rsquo;ll download Debian\u0026rsquo;s 8.1 DVD Image for PPC64el architecture. It can be found on this link and should be stored in /var/lib/kimchi/isos/ folder.\ncd /var/lib/kimchi/isos wget http://cdimage.debian.org/debian-cd/8.1.0/ppc64el/iso-dvd/debian-8.1.0-ppc64el-DVD-1.iso  Then run md5sum to see if the file is corrupted:\nwget http://cdimage.debian.org/debian-cd/8.1.0/ppc64el/iso-dvd/MD5SUMS md5sum -c MD5SUMS  The result should be:\ndebian-8.1.0-ppc64el-DVD-1.iso: OK  Otherwise, try downloading again.\nBringing to Life Now that we have our ISO, we\u0026rsquo;ll create an qcow2 image using qemu to act as a hard drive. Those images should be stored in /var/lib/libvirt/images/.\nqemu-img create -f qcow2 -o preallocation=metadata storage.qcow2 10G  Then, we can start the installation using virt-install:\nvirt-install -r 12228 --os-variant=debianwheezy --network bridge=virbr0,model=virtio --accelerate -n debian --vcpus=maxvcpus=16,sockets=2,cores=2,threads=4 -f ./storage.qcow2 --graphics vnc,listen=0.0.0.0 -c /var/lib/kimchi/isos/debian-8.1.0-ppc64el-DVD-1.iso  If you\u0026rsquo;re using a different OS, you can list all available options with:\nvirt-install --os-variant list  Instalation will start. In this case, it was done throught Kimchi\u0026rsquo;s web monitor, but can be done using libvirt. Proceed normally. After it\u0026rsquo;s finished, you can start your VM and login with:\nvirsh start debian virsh console debian  #Network Configuration\nAs said before, eth0 and eth1 will be bridged to the VM through Kimchi\u0026rsquo;s web interface, where eth0 is our internal network interface and eth1, external network.\n##Setting IPs We\u0026rsquo;ll edit /etc/network/interfaces file and assign static IP\u0026rsquo;s both internal and external. Your external address and gateway should be provided by your ISP.\nnano /etc/network/interfaces  auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 iface eth0 inet static address 10.0.0.1 netmask 255.255.255.0 allow-hotplug eth1 iface eth1 inet static address 0.0.0.0 netmask 255.255.255.0 gateway 0.0.0.0  Edit your /etc/resolv.conf if needed by your ISP:\nnano /etc/resolv.conf  nameserver ISP_server; search ISP_address;  After restarting your network service, you should have something like this:\nsystemctl restart networking \u0026amp;\u0026amp; ifconfig eth0 Link encap:Ethernet HWaddr 52:54:00:37:bc:11 inet addr:10.0.0.1 Bcast:10.0.0.255 Mask:255.255.255.0 eth1 Link encap:Ethernet HWaddr 52:54:00:7b:74:6f inet addr: 0.0.0.0 Bcast:0.0.0.0 Mask:255.255.255.0 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0  See if it\u0026rsquo;s working by pinging internal and external addresses:\nping www.cnn.com ping 10.0.0.5  ##Routing Start by flushing all previous configurations, if they exist.\niptables -F iptables -t nat -F iptables -t mangle -F iptables -X  We\u0026rsquo;ll now allow established connections, outgoing connections and setup masquerade as follows:\niptables -A INPUT -i lo -j ACCEPT iptables -A FORWARD -i eth1 -o eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE  And now, we\u0026rsquo;ll allow IP Forwarding:\necho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward  And your Iptables should look like this:\niptables -L  Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT all -- anywhere anywhere Chain OUTPUT (policy ACCEPT) target prot opt source destination  Now a client should successfully connect to the internet.\n##Making it Permanent Now we want to apply these iptables configurations everytime we start this machine. This can be done by saving them in a file and restoring on the next boot.\niptables-save \u0026gt;\u0026gt; /etc/iptables.rules  On /etc/network/interfaces, add this line underneath \u0026ldquo;iface lo inet loopback\u0026rdquo;:\nnano /etc/network/interfaces  pre-up iptables-restore \u0026lt; /etc/iptables.rules  #That\u0026rsquo;s it\nBy now you should have a basic Linux gateway for your network. Much more advanced configuration can be done that can add enormous flexibility. It\u0026rsquo;s up to you to start exploring and unleash the true power of having a dedicated machine as your router.\n","date":1438992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438992000,"objectID":"4aeb49cef50e37592310b61a0cb920ce","permalink":"/post/debian_gateway/","publishdate":"2015-08-08T00:00:00Z","relpermalink":"/post/debian_gateway/","section":"post","summary":"There are many reasons why one would want to build its custom router instead of buying one. Control and flexibility are two reasons, and we need both when dealing with large traffic. The purpose of this guide is to give a step-by-step solution starting on how to build a virtual machine. For this, we will assume that PowerKVM is already up and running along with its network configurations.\nSo, for this guide we will need:","tags":null,"title":"Setting up a Debian Gateway Virtual Machine on PowerKVM","type":"post"},{"authors":[],"categories":null,"content":"","date":1434132000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434132000,"objectID":"c73862ef29f2e7937614a8add46eac49","permalink":"/talk/digital-signature/","publishdate":"2015-06-12T18:00:00Z","relpermalink":"/talk/digital-signature/","section":"talk","summary":"Assinatura digital é uma analogia a conhecida assinatura escrita. No meio digital podemos utilizar uma assinatura digital para (1) prover garantias que foi realmente o signatário quem assinou uma determinada informação, e também (2) detectar se a informação foi modificada ou não após ter sido assinada (integridade da informação). A tecnologia assinatura digital é especificada pelo NIST através da publicação FIPS 184-6. Nesta palestra, o objetivo é mostrar como se faz para gerar e verificar uma assinatura digital em Linux com base na publicação FIPS 184-6. Para mostrar a sua abrangência em Linux, o uso de assinatura digital será discutido em três níveis de aplicações: email, kernel e firmware. **About the speaker:** Claudio Carvalho é engenheiro de software na IBM e atualmente trabalha com desenvolvimento de segurança em Linux no LTC (Linux Technology Center). Claudio é bacharel em Engenharia da Computação pela PUC-GO e Mestre em Ciência da Computação pela Unicamp, e trabalha com Linux há mais de 10 anos.","tags":[],"title":"Assinatura digital: de user-space a firmware","type":"talk"},{"authors":[],"categories":null,"content":"","date":1431108000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431108000,"objectID":"1566725f6485b7fd74316bb77bd28ab7","permalink":"/talk/openpower-research/","publishdate":"2015-05-08T18:00:00Z","relpermalink":"/talk/openpower-research/","section":"talk","summary":"**About the speaker:** Ricardo Matinata, arquiteto do Linux Technology – IBM","tags":[],"title":"Oportunidades de Pesquisa em OpenPOWER","type":"talk"},{"authors":[],"categories":null,"content":"","date":1428602400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428602400,"objectID":"57f0403264ed78a085afa82c8f84fb6d","permalink":"/talk/power-architecture/","publishdate":"2015-04-09T18:00:00Z","relpermalink":"/talk/power-architecture/","section":"talk","summary":"A arquitetura de processadores POWER da IBM, apesar de já muito bem consolidada no mercado corporativo de servidores de médio e grande porte ainda é relativamente pouco explorada na área acadêmica, principalmente no Brasil. Se você nunca pensou em trabalhar com outras arquiteturas de processadores que não a x86, você se surpreendera com algumas características de escalabilidade de processamento paralelo e acesso a grandes quantidades de memória que o processador POWER oferece sem que seja necessário mudar sua aplicação. Venha conhecer um pouco sobre a arquitetura Power e o processador POWER8, a última geração desta família. Se você já conhece a arquitetura e está interessado nela, venha ver como você pode acessar uma máquina remotamente sem grande esforço. **About the speaker:** Leonardo é engenheiro de software do LTC, e hoje trabalha no projeto PowerKVM.","tags":[],"title":"Arquitetura dos processadores POWER da IBM","type":"talk"},{"authors":[],"categories":null,"content":"","date":1428602400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428602400,"objectID":"505db68d361bc775d342373dc31ec90e","permalink":"/talk/openstack/","publishdate":"2015-04-09T18:00:00Z","relpermalink":"/talk/openstack/","section":"talk","summary":"Criado originalmente por uma parceira entre a Nasa e a RackSpace atualmente o OpenStack é gerenciado por um consorcio de mais de 500 membros, incluindo grandes empresas como IBM, Intel, Cisco, Dell e etc. O OpenStack pode ser definido como um Sistema Operacional projetados para nuvem capaz de controlar uma grande quantidade de recursos, utilizando um um conjunto de projetos de software open source para configurar e operar uma infraestrutura de computação e armazenamento. **About the speaker:** Marcelo Claudio Sousa Araújo é graduado pela Universidade Federal do Tocantins (UFT), atualmente mestrando pela Universidade Estadual de Campinas (UNICAMP) e pesquisando em parceira a IBM através do LTC Unicamp. Atualmente trabalha com virtualização, cloud e Power 8 e também tem interesses na área de computação de alto desempenho, paralela e distribuída.","tags":[],"title":"OpenStack","type":"talk"}]
[{"authors":["guilhermedobins"],"categories":null,"content":"I\u0026rsquo;m a Computer Engineering undergrad student at UNICAMP who is passionate about programming and learning more about technology related subjects.\n","date":1655596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1655596800,"objectID":"7062a4a56d3d21c5fdec0878a6022f30","permalink":"https://openpower.ic.unicamp.br/authors/guilhermedobins/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guilhermedobins/","section":"authors","summary":"I\u0026rsquo;m a Computer Engineering undergrad student at UNICAMP who is passionate about programming and learning more about technology related subjects.","tags":null,"title":"Guilherme Dobins","type":"authors"},{"authors":["iagocaran"],"categories":null,"content":"I’m a Computer Engineering undergrad student at UNICAMP interested in Computer Architecture and Design.\n","date":1648857600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1648857600,"objectID":"855c64de48094ae328332d1728cdf9c1","permalink":"https://openpower.ic.unicamp.br/authors/iagocaran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/iagocaran/","section":"authors","summary":"I’m a Computer Engineering undergrad student at UNICAMP interested in Computer Architecture and Design.","tags":null,"title":"Iago Caran Aquino","type":"authors"},{"authors":["jr-santos"],"categories":null,"content":"I\u0026rsquo;m an undergrad student in Computer Science at UNICAMP that loves GNU/Linux and programming.\nI work with Open Souce software portability for the Power architecture, As well as making these packages available to the OpenPower community. I also worked with OpenStack, configuring and installing a public cloud service called Minicloud.\n","date":1638576000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1638576000,"objectID":"530f7e6eee399c33cc16d082505514e9","permalink":"https://openpower.ic.unicamp.br/authors/jr-santos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jr-santos/","section":"authors","summary":"I\u0026rsquo;m an undergrad student in Computer Science at UNICAMP that loves GNU/Linux and programming.\nI work with Open Souce software portability for the Power architecture, As well as making these packages available to the OpenPower community. I also worked with OpenStack, configuring and installing a public cloud service called Minicloud.","tags":null,"title":"Júnior Santos","type":"authors"},{"authors":["matheuscod"],"categories":null,"content":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.\n","date":1618963200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1618963200,"objectID":"aa0f5a9b6a11927b3a2e7fcf6f51fbf3","permalink":"https://openpower.ic.unicamp.br/authors/matheuscod/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/matheuscod/","section":"authors","summary":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.","tags":null,"title":"Matheus Fernandes","type":"authors"},{"authors":["juliokiyoshi"],"categories":null,"content":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.\n","date":1617926400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617926400,"objectID":"4fcad08fe4f49ba44d48cbc314932b56","permalink":"https://openpower.ic.unicamp.br/authors/juliokiyoshi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/juliokiyoshi/","section":"authors","summary":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.","tags":null,"title":"Julio Kiyoshi","type":"authors"},{"authors":["vcouto"],"categories":null,"content":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.\n","date":1586822400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1586822400,"objectID":"94a5a0ada1c651cd5ab10d8e86937789","permalink":"https://openpower.ic.unicamp.br/authors/vcouto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vcouto/","section":"authors","summary":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.","tags":null,"title":"Vinicius Couto Espindola","type":"authors"},{"authors":["lcnzg"],"categories":null,"content":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP that loves programming and solving problems.\nI\u0026rsquo;ve designed a solution to distribute POWER builds worldwide, along with some CI recipes to build Open Source software. I also worked with OpenStack, configuring and installing a public cloud service called Minicloud.\n","date":1583280000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1583280000,"objectID":"dae3611350c62284c07bb849357b31a4","permalink":"https://openpower.ic.unicamp.br/authors/lcnzg/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lcnzg/","section":"authors","summary":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP that loves programming and solving problems.\nI\u0026rsquo;ve designed a solution to distribute POWER builds worldwide, along with some CI recipes to build Open Source software. I also worked with OpenStack, configuring and installing a public cloud service called Minicloud.","tags":null,"title":"Luciano Zago","type":"authors"},{"authors":["marcelo-martins"],"categories":null,"content":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.\n","date":1583280000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1583280000,"objectID":"bcbb33e2e65206df04cd694288d3e3d4","permalink":"https://openpower.ic.unicamp.br/authors/marcelo-martins/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marcelo-martins/","section":"authors","summary":"I\u0026rsquo;m an undergrad student in Computer Engineering at UNICAMP.","tags":null,"title":"Marcelo Martins","type":"authors"},{"authors":["rpsene"],"categories":null,"content":"","date":1566219900,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1566219900,"objectID":"7e04078d6bc07930eb7c68638f86d891","permalink":"https://openpower.ic.unicamp.br/authors/rpsene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rpsene/","section":"authors","summary":"","tags":null,"title":"Rafael Sene","type":"authors"},{"authors":["gsalibi"],"categories":null,"content":"","date":1564939200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1564939200,"objectID":"2b92993d022e7ab9d1f1d52de8459952","permalink":"https://openpower.ic.unicamp.br/authors/gsalibi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gsalibi/","section":"authors","summary":"","tags":null,"title":"Gustavo Storti Salibi","type":"authors"},{"authors":["everton"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f45a0a280a7fe903a9a8bb0a7dbc3c05","permalink":"https://openpower.ic.unicamp.br/authors/everton/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/everton/","section":"authors","summary":"","tags":null,"title":"Everton Constantino","type":"authors"},{"authors":["bertolini"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c15b493d038477e9491420451f5d633f","permalink":"https://openpower.ic.unicamp.br/authors/bertolini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bertolini/","section":"authors","summary":"","tags":null,"title":"Jose Bertolini","type":"authors"},{"authors":["klauskiwi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"adc249be20263d550b251c8871e464c3","permalink":"https://openpower.ic.unicamp.br/authors/klauskiwi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/klauskiwi/","section":"authors","summary":"","tags":null,"title":"Klaus Kiwi","type":"authors"},{"authors":["lfwanner"],"categories":null,"content":"I am an Assistant Professor in the Institute of Computing at the University of Campinas (Unicamp). My interests are in the intersection of hardware and software systems, particularly in runtime systems for embedded applications, energy optimizations, and handling of heterogeneity and variability in hardware.\nI graduated with a Ph.D. in Computer Science from the University of California, Los Angeles (UCLA) in 2014. During my Ph.D. I worked with Mani Srivastava in the Networked and Embedded Systems Lab (NESL) and was funded in part with a Fulbright Fellowship. In the latter part of my time at UCLA I served as Executive Director of the NSF Expedition in Variability-Aware Software. Before joining UCLA I graduated with a BS and MS in Computer Science from the Federal University of Santa Catarina (UFSC), where I worked in the Software/Hardware Integration Lab (LISHA).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f7445419234e14be167a051b0ebca3ca","permalink":"https://openpower.ic.unicamp.br/authors/lfwanner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lfwanner/","section":"authors","summary":"I am an Assistant Professor in the Institute of Computing at the University of Campinas (Unicamp). My interests are in the intersection of hardware and software systems, particularly in runtime systems for embedded applications, energy optimizations, and handling of heterogeneity and variability in hardware.\nI graduated with a Ph.D. in Computer Science from the University of California, Los Angeles (UCLA) in 2014. During my Ph.D. I worked with Mani Srivastava in the Networked and Embedded Systems Lab (NESL) and was funded in part with a Fulbright Fellowship.","tags":null,"title":"Lucas Wanner","type":"authors"},{"authors":["pcaldeira"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d789d99a96428c5dbb168455742eebcc","permalink":"https://openpower.ic.unicamp.br/authors/pcaldeira/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pcaldeira/","section":"authors","summary":"","tags":null,"title":"Pedro Caldeira","type":"authors"},{"authors":["srigo"],"categories":null,"content":"Interests: Compilers and Programming Languages Computer Architecture Computer Systems Design  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2bec4bbbe69315b36a06fdee37429129","permalink":"https://openpower.ic.unicamp.br/authors/srigo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/srigo/","section":"authors","summary":"Interests: Compilers and Programming Languages Computer Architecture Computer Systems Design  ","tags":null,"title":"Sandro Rigo","type":"authors"},{"authors":["Guilherme Dobins"],"categories":null,"content":"Despite GitHub offering some options for CI-CD platforms, such as Travis and Actions, you may want to try out GitLab’s alternatives, but feel turned off by the fact you might need to take your codes to a different platform. To avoid this problem, using these two platforms together sounds like a good idea. The goal of this tutorial is to show you how to configure a GitLab runner, a pipeline, and use the GitLab-CI to run your GitHub pipelines, without the need for premium services.\nFor this tutorial, you will need to be registered to GitHub and GitLab, and also a computer where the GitLab-runner will be installed.\nConfiguring the GitLab repo The first thing you need to do is to create a repository on GitLab. In this repository, the only thing you have to do is to create a file called \u0026ldquo;.gitlab-ci.yml\u0026rdquo;. This file works as a \u0026ldquo;travis.yml\u0026rdquo; file and is responsible for the configuration of the pipeline, such as the steps to be executed, installing the prerequisites\u0026hellip; You can find more information about it in the gitlab documentation. For this tutorial, the goal is to use this file to retrieve the contents of the GitHub repository. The easiest way of doing this is to clone the repository before running the script.\nTo make it clear, suppose your GitHub repository contains a \u0026ldquo;build.sh\u0026rdquo; file, and your pipeline consists of running this script. Then, your .gitlab-ci.yml would look something like this:\ndefault: tags: - *your_runner_tag* #This tag is used to select a runner stages: - build before_script: - git clone *your_github_repo_url* build: stage: build script: - bash github_repo/build.sh Setting up a GitLab-runner The next part of the configuration is to set up a runner. To do so, you will need a computer where the runner will be installed. One suggestion is to use a minicloud VM for this purpose. After connecting to the VM, the next step is to install the runner. To do that, you can follow the steps described below. For more details, you can check the gitlab documentation. First, download the runner\nsudo curl -L --output /usr/local/bin/gitlab-runner \u0026#34;https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-ppc64le\u0026#34;  sudo chmod +x /usr/local/bin/gitlab-runner Then, you must create a user for the runner.\nsudo useradd --comment \u0026#39;GitLab Runner\u0026#39; --create-home gitlab-runner --shell /bin/bash Note: Make sure to give this user the right permissions to execute your pipeline. For example, make sure this user is in the sudoers list if you plan to execute any sudo commands. Also, if you want the pipelines to be run as an already existing user, you can skip the previous step.\nFinally, install and execute the runner\nsudo gitlab-runner install --user=gitlab-runner –working-directory=/home/gitlab-runner  sudo gitlab-runner start Notice that, sometimes, due to an error, the working directory might not be set to the option you wanted. To fix this, you must edit the /etc/systemd/system/gitlab-runner.service file, and change the working directory to the right one. You might need to reboot your computer to apply this change.\nNow that the GitLab runner is installed, you must register it, so it will be connected to your GitLab account.\nsudo gitlab-runner register After that, type in the GitLab instance URL you are using (For example, https://gitlab.com/) Then, your registration token will be required. To find it, go to the GitLab website, enter the repo you created for this CI project, and then go to “Settings” \u0026gt; “CI/CD”. After that, click “Expand” on the right side of the “Runners” section. In the “Specific Runners” tab, you will find your registration token, copy it, and paste it to your terminal to continue the runner registration.\nThe next step is to name your runner, so you can identify it later. Finally, you must add tags to the runner. These tags will make it easier for you to choose a runner for your pipelines. For example, you can install python on this runner, then add the \u0026ldquo;python\u0026rdquo; tag to it, so you can use it on pipelines where python is required.\nThen, you will be asked to enter a maintenance note, but you can just hit \u0026ldquo;enter\u0026rdquo; and leave it blank.\nFor the final step on the runner setup, you need to choose an executor. The main ones are docker and shell. If you choose docker, you must provide a docker image on the pipeline configuration, and your pipeline will be run on a container build with said image. On the other hand, if you choose \u0026ldquo;shell\u0026rdquo;, the pipeline will be executed on the shell of the computer where the runner was installed. Notice that, in order to use the docker executor, docker must be previously installed. After this choice, the runner is completely installed.\nBefore you continue with the configuration, make sure to remove the \u0026ldquo;.bash_logout\u0026rdquo; file from the gitlab-runner user\u0026rsquo;s home folder, because if you don\u0026rsquo;t, an error will occur during the pipeline execution. Also, back on the GitLab website, in the Runners section you just visited, make sure to disable the \u0026ldquo;Allow shared runners for this project\u0026rdquo; option, under \u0026ldquo;Shared Runners” so your own runners will always be used.\nLinking the GitHub and GitLab repositories The last thing you have to do is to link both repositories so that whenever a change is made to GitHub, it will trigger the GitLab pipeline. First, go to the GitLab repository, then \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;CI/CD\u0026rdquo;. Click the \u0026ldquo;Expand\u0026rdquo; button next to \u0026ldquo;Pipeline triggers\u0026rdquo; and create a new trigger by giving it a description and pressing \u0026ldquo;Add trigger\u0026rdquo;. After that, find the \u0026ldquo;Use webhook\u0026rdquo; option under the tokens and copy this URL, but make sure to change the REF_NAME to be the branch you are using for the project (possibly the main branch), and also change the TOKEN variable to the value of the token you just created.\nWith this URL, head to your GitHub repository, and click \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Webhooks\u0026rdquo; (under the \u0026ldquo;Code and automation\u0026rdquo; section). Click \u0026ldquo;Add webhook\u0026rdquo;, and under \u0026ldquo;Payload URL\u0026rdquo;, add the URL you got from GitLab (with the correct values for REF_NAME and TOKEN). You can leave the other options as they are and then add the webhook.\nAfter all those steps, every time you make a change to your GitHub repository, it should trigger GitLab, and use the runner you just configured to execute a pipeline.\n","date":1655596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655596800,"objectID":"ebe815f5a76fc4418067c228684247eb","permalink":"https://openpower.ic.unicamp.br/post/gitlab-ci-with-github-repo/","publishdate":"2022-06-19T00:00:00Z","relpermalink":"/post/gitlab-ci-with-github-repo/","section":"post","summary":"Despite GitHub offering some options for CI-CD platforms, such as Travis and Actions, you may want to try out GitLab’s alternatives, but feel turned off by the fact you might need to take your codes to a different platform. To avoid this problem, using these two platforms together sounds like a good idea. The goal of this tutorial is to show you how to configure a GitLab runner, a pipeline, and use the GitLab-CI to run your GitHub pipelines, without the need for premium services.","tags":null,"title":"Using GitLab CI with GitHub repositories","type":"post"},{"authors":["Iago Caran Aquino"],"categories":null,"content":"GHDL is an open-source analyzer, compiler and synthesizer for VHDL generating machine code from a Hardware Description Language. As it is not an interpreter, it allows for high speed simulation which is crucial for applications like the Microwatt, a tiny Open POWER ISA softcore.\nIn this tutorial you will be guided step-by-step in the setup of your environment for simulating Microwatt with GHDL.\nDisclaimer: For the purpose of this tutorial I will be using an Ubuntu 20.04 system and I\u0026rsquo;ll assume you\u0026rsquo;re familiar with it.\nPrerequisites First of all, make sure you have this packages installed:\nsudo apt update sudo apt install -y build-essential gnat zlib1g-dev llvm-dev clang git This includes the project\u0026rsquo;s dependencies, as well as git and the basic packages to build a project.\nInstalling GHDL We\u0026rsquo;ll now clone and build GHDL.\ngit clone https://github.com/ghdl/ghdl.git cd ghdl ./configure --prefix=/usr/local --enable-libghdl --with-llvm-config make make install Install Microwatt We are now ready to clone and build Microwatt.\ngit clone https://github.com/antonblanchard/microwatt.git cd microwatt make Testing Now that everything is installed we can run some code. Fortunately the project already includes a hello_world ready to be executed, just change the file to be executed and then run the simulation:\nln -s hello_world/hello_world.bin main_ram.bin ./core_tb \u0026gt; /dev/null You should see a lightbulb being drawn on the terminal. But if you want something more substantial, you will find bundled a python runtime:\nrm main_ram.bin ln -s micropython/firmware.bin main_ram.bin ./core_tb \u0026gt; /dev/null (Optional) Build MicroPython To compile code for the Power platform you will need a cross compiler, luckily it is already in the Ubuntu repository. With that, just compile the project normally.\nsudo apt install -y gcc-powerpc64le-linux-gnu git clone https://github.com/micropython/micropython.git cd micropython/ports/powerpc make -j$(nproc) UART=lpc_serial cd ../../../ rm main_ram.bin ln -s ../micropython/ports/powerpc/build/firmware.bin main_ram.bin ./core_tb \u0026gt; /dev/null Note that it is necessary to set the UART interface to lpc_serial when simulating using GHDL.\nCompiling code To compile code for the Power architecture you need the cross compiler shown in the optional section on MicroPython. With it installed, just duplicate the hello_world folder and use it as a base for your code. It is important to remember that the communication should be done using the headers provided by MicroWatt, as the terminal is redirected to the UART port.\nProblems using WSL and Windows You can do this entire process within WSL for Windows 10 and 11, but some people will experience random crashes when using the Home version of these systems. This is due to some incompatibility of the Virtual Machine Platform component with your hardware. A possible solution is to upgrade to Windows Pro, but I opted to use Ubuntu instead.\nReference  Building GHDL from sources A tiny Open POWER ISA softcore written in VHDL 2008  ","date":1648857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648857600,"objectID":"c722fd84e70fdd4cebcae9413f993553","permalink":"https://openpower.ic.unicamp.br/post/simulating-microwatt-with-ghdl/","publishdate":"2022-04-02T00:00:00Z","relpermalink":"/post/simulating-microwatt-with-ghdl/","section":"post","summary":"GHDL is an open-source analyzer, compiler and synthesizer for VHDL generating machine code from a Hardware Description Language. As it is not an interpreter, it allows for high speed simulation which is crucial for applications like the Microwatt, a tiny Open POWER ISA softcore.\nIn this tutorial you will be guided step-by-step in the setup of your environment for simulating Microwatt with GHDL.\nDisclaimer: For the purpose of this tutorial I will be using an Ubuntu 20.","tags":null,"title":"Simulating Microwatt with GHDL","type":"post"},{"authors":["Júnior Santos"],"categories":null,"content":"Docker is a set of systems that use virtualization to deliver software in packages, which are called containers. The containers are isolated from the operating system and other containers, they have unique libraries, as well as packages and configuration files. In addition, they can run different operating systems and have a lower resource cost than traditional virtualization.\nWe provide the docker-ce, an edition of Docker that is free software, updated by the community. However, the package is officially not available for the Power architecture (ppc64 / ppc64le). Thus, the laboratory makes the latest versions available for installation from the package managers: Advanced Packaging Tool (APT) and Red Hat Package Manager (RPM).\nThis tutorial shows the step-by-step installation of Docker using the \u0026ldquo;.deb/.rpm\u0026rdquo; package. If you want to install using a package manager, read Installing Docker from the OpenPower Lab Repository.\nRequirements First of all, you need to install the Docker requirements. You can perform this operation using one of the commands below:\nTo Ubuntu/Debian: sudo apt update sudo apt install -y wget libseccomp2 libc6 libdevmapper1.02.1 libsystemd0 apparmor ca-certificates git libltdl7 pigz procps xz-utils runc To RHEL/CentOS/Fedora: sudo yum update sudo yum install -y wget libseccomp glibc-utils device-mapper-libs systemd-libs ca-certificates git libtool-ltdl pigz procps xz runc Install Docker To install Docker-ce, it is necessary to download and install the packages: containerd, docker-ce, docker-ce-cli and docker-ce-rootless-extras. All these packages are available for download through our FTP, use one of the commands below to perform this operation:\n Containerd was used in version 1.5.7 and Docker in 20.10.6. Change package versions! For the latest versions, access Oplab FTP.\n To Ubuntu/Debian wget https://oplab9.parqtec.unicamp.br/pub/repository/debian/ppc64el/containerd/containerd-1.5.7-ppc64le.deb https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker/version-20.10.6/ubuntu-focal/docker-ce-rootless-extras_20.10.6~3-0~ubuntu-focal_ppc64el.deb https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker/version-20.10.6/ubuntu-focal/docker-ce-cli_20.10.6~3-0~ubuntu-focal_ppc64el.deb https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker/version-20.10.6/ubuntu-focal/docker-ce_20.10.6~3-0~ubuntu-focal_ppc64el.deb To install on Ubuntu, use the dpkg:\nsudo dpkg -i containerd-1.5.7-ppc64le.deb docker-ce-rootless-extras_20.10.6~3-0~ubuntu-focal_ppc64el.deb docker-ce-cli_20.10.6~3-0~ubuntu-focal_ppc64el.deb docker-ce_20.10.6~3-0~ubuntu-focal_ppc64el.deb To RHEL/CentOS/Fedora: wget https://oplab9.parqtec.unicamp.br/pub/repository/rpm/ppc64le/containerd/containerd-1.5.7-1.ppc64le.rpm https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker/version-20.10.6/centos-8/docker-ce-rootless-extras-20.10.6-3.el8.ppc64le.rpm https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker/version-20.10.6/centos-8/docker-ce-cli-20.10.6-3.el8.ppc64le.rpm https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker/version-20.10.6/centos-8/docker-ce-20.10.6-3.el8.ppc64le.rpm To install on CentOS 8, use the yum localinstall:\nsudo yum localinstall containerd-1.5.7-1.ppc64le.rpm docker-ce-rootless-extras-20.10.6-3.el8.ppc64le.rpm docker-ce-cli-20.10.6-3.el8.ppc64le.rpm docker-ce-20.10.6-3.el8.ppc64le.rpm Enable and verify Docker Before using Docker, you need to enable it and start it. This process is also used to verify that the installation was successful.\nsudo systemctl enable docker sudo systemctl start docker sudo systemctl status docker Expected output:\nubuntu@docker-build:~$ sudo systemctl enable docker Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service. ubuntu@docker-build:~$ sudo systemctl start docker ubuntu@docker-build:~$ sudo systemctl status docker ● docker.service - Docker Application Container Engine  Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)  Active: active (running) since Sat 2021-12-04 00:15:12 UTC; 3min 33s ago TriggeredBy: ● docker.socket  Docs: https://docs.docker.com  Main PID: 88776 (dockerd)  Tasks: 16  Memory: 76.6M  CGroup: /system.slice/docker.service  └─88776 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock  Dec 04 00:15:10 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:10.992533127Z\u0026#34; level=warning msg=\u0026#34;Your kernel does \u0026gt; Dec 04 00:15:10 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:10.992943323Z\u0026#34; level=warning msg=\u0026#34;Your kernel does \u0026gt; Dec 04 00:15:10 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:10.993053898Z\u0026#34; level=warning msg=\u0026#34;Your kernel does \u0026gt; Dec 04 00:15:10 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:10.993340051Z\u0026#34; level=info msg=\u0026#34;Loading containers: \u0026gt; Dec 04 00:15:11 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:11.419025732Z\u0026#34; level=info msg=\u0026#34;Default bridge (dock\u0026gt; Dec 04 00:15:11 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:11.585401847Z\u0026#34; level=info msg=\u0026#34;Loading containers: \u0026gt; Dec 04 00:15:12 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:12.507244532Z\u0026#34; level=info msg=\u0026#34;Docker daemon\u0026#34; commi\u0026gt; Dec 04 00:15:12 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:12.507417125Z\u0026#34; level=info msg=\u0026#34;Daemon has completed\u0026gt; Dec 04 00:15:12 docker-build systemd[1]: Started Docker Application Container Engine. Dec 04 00:15:12 docker-build dockerd[88776]: time=\u0026#34;2021-12-04T00:15:12.675971607Z\u0026#34; level=info msg=\u0026#34;API listen on /run/d\u0026gt; If the output looks like the one above, then your Docker is installed and ready to use.\n","date":1638576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638576000,"objectID":"26a8efe61d330e4635ae4d1e83bc5664","permalink":"https://openpower.ic.unicamp.br/post/install-docker/","publishdate":"2021-12-04T00:00:00Z","relpermalink":"/post/install-docker/","section":"post","summary":"Docker is a set of systems that use virtualization to deliver software in packages, which are called containers. The containers are isolated from the operating system and other containers, they have unique libraries, as well as packages and configuration files. In addition, they can run different operating systems and have a lower resource cost than traditional virtualization.\nWe provide the docker-ce, an edition of Docker that is free software, updated by the community.","tags":null,"title":"Install Docker from package","type":"post"},{"authors":["Matheus Fernandes","Júnior Santos"],"categories":null,"content":"This blogpost aims to teach how to build and create a Docker .deb and .rpm packages starting from Docker 20.10 release, considering that since that version the Docker Engine and Docker CLI are built directly from the source repositories.\nRequirements We used Ubuntu 20.04 for this tutorial for both .deb and .rpm builds.\nFirst, make sure you have both git and the make package on your machine.\nYou can install then with:\nsudo apt install git sudo apt install make sudo apt install unzip After that, we need to install Docker-CE. To do that, just add our POWER packages repository to your machine:\nEdit the file /etc/apt/sources.list by adding the following line:\ndeb https://oplab9.parqtec.unicamp.br/pub/repository/debian/ ./\nDownload our GPG key, and use the command below to add it to the system:\nsudo apt-key add openpower-gpgkey-public.asc After that, update the package list and install docker-ce:\nsudo apt update sudo apt install docker-ce More information about our repository in: POWER Repository\nBuild and Packaging We\u0026rsquo;ll need to download docker-cli and moby (current name of the docker engine) and clone the repositories from scan-cli-plugin and docker-ce-packaging.\nClone the following docker repositories:\ngit clone https://github.com/docker/scan-cli-plugin.git git clone https://github.com/docker/docker-ce-packaging.git Download the desired version (we\u0026rsquo;ll use 20.10.6) of the cli and moby by downloading its releases (you can use git clone to build the master branch too):\n# Download the cli source code and change its zip name wget https://github.com/docker/cli/archive/refs/tags/v20.10.6.zip mv v20.10.6.zip cli.zip  # Download the moby source code and change its zip name wget https://github.com/moby/moby/archive/refs/tags/v20.10.6.zip mv v20.10.6.zip moby.zip  # Unzip the downloaded source-codes unzip cli.zip unzip moby.zip  #Change the folders name mv cli-20.10.6 cli mv moby-20.10.6 moby Because the Docker Build uses containerd.io, we need to modify two files on docker-ce-packaging in order to use the community version of the same software, which is probably already installed on your machine if you installed Docker-CE from our repository(POWER Repository).\nBesides that\nModify the files with python3 by running the following script:\nimport re  print(\u0026#34;Running Patching Script...\u0026#34;)  deb_path = \u0026#34;docker-ce-packaging/deb/common/control\u0026#34; deb_ver = \u0026#34;containerd (\u0026gt;= 1.2.1)\u0026#34;  rpm_path = \u0026#34;docker-ce-packaging/rpm/SPECS/docker-ce.spec\u0026#34; rpm_ver = \u0026#34;Requires: containerd \u0026gt;= 1.2.1\u0026#34;  # Update debian containerd dependency print(\u0026#34;Patching DEB...\u0026#34;) deb = open(deb_path, \u0026#39;r\u0026#39;) data = deb.read() new = re.sub(r\u0026#39;containerd.io \\([^)]*\\)\u0026#39;, deb_ver, data) assert data != new, \u0026#34;Nothing was changed in the file.\u0026#34; open(deb_path, \u0026#39;w\u0026#39;).write(new)  # Update rpm containerd dependency print(\u0026#34;Patching RPM...\u0026#34;) rpm = open(rpm_path, \u0026#39;r\u0026#39;) data = rpm.read() new = re.sub(r\u0026#39;Requires: containerd.io [^\\n]*\u0026#39;, rpm_ver, data) assert data != new, \u0026#34;Nothing was changed in the file.\u0026#34; open(rpm_path, \u0026#39;w\u0026#39;).write(new)  print(\u0026#34;DONE Patching\u0026#34;) After the patch is done, we need to create specific folders inside docker-ce-packaging and copy the other cloned repositories into that folders.\nFrom the outside of docker-ce-packaging, do that with:\n# Create the folders mkdir -p docker-ce-packaging/src/github.com/docker/cli mkdir -p docker-ce-packaging/src/github.com//docker/docker mkdir -p docker-ce-packaging/src/github.com/docker/scan-cli-plugin  # Copy cli, moby and scan-cli-plugin sudo cp -r cli/* docker-ce-packaging/src/github.com/docker/cli sudo cp -r moby/* docker-ce-packaging/src/github.com/docker/docker sudo cp -r scan-cli-plugin/* docker-ce-packaging/src/github.com/docker/scan-cli-plugin Making .deb packages Systems available:\nUbuntu:\nubuntu-buster, ubuntu-bionic, ubuntu-focal, ubuntu-groovy, ubuntu-hirsute, ubuntu-xenial\nDebian:\ndebian-bullseye, debian-buster\nRaspbian:\nraspbian-bullseye, raspbian-buster\nMake the packages with:\ncd docker-ce-packaging/deb sudo VERSION=20.10.6 make ubuntu-focal They will be available at: docker-ce-packaging/deb/debbuild/\nIn our example, the .deb files will be at\ndocker-ce-packaging/deb/debbuild/ubuntu-focal\nMaking .rpm packages Edit the file docker-ce-packaging/rpm/gen-rpm-ver\nby changing the characters || to \u0026amp;\u0026amp; in line 46\nSystems available:\nCentOS:\ncentos-7, centos-8\nFedora:\nfedora-32, fedora-33, fedora-34\nRHEL:\nrhel-7\nMake the packages with:\ncd docker-ce-packaging/rpm sudo VERSION=20.10.6 make centos-8 They will be available at: docker-ce-packaging/rpm/rpmbuild/\nIn our example, the .rpm files will be at\ndocker-ce-packaging/rpm/rpmbuild/centos-8/SRPMS and\ndocker-ce-packaging/rpm/rpmbuild/centos-8/RPMS/ppc64le\nReferences Docker CLI: https://github.com/docker/cli\nDocker Engine: https://github.com/moby/moby\nscan-cli-plugin: https://github.com/docker/scan-cli-plugin\nDocker-CE Packaging: https://github.com/docker/docker-ce-packaging\nOpenPOWER@UNICAMP POWER Repository: https://openpower.ic.unicamp.br/project/power-repository/\n","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"82bffe7de8362205c1311fc40caaa083","permalink":"https://openpower.ic.unicamp.br/post/building-docker-for-power/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/post/building-docker-for-power/","section":"post","summary":"This blogpost aims to teach how to build and create a Docker .deb and .rpm packages starting from Docker 20.10 release, considering that since that version the Docker Engine and Docker CLI are built directly from the source repositories.\nRequirements We used Ubuntu 20.04 for this tutorial for both .deb and .rpm builds.\nFirst, make sure you have both git and the make package on your machine.\nYou can install then with:","tags":null,"title":"Building Docker for POWER","type":"post"},{"authors":["Julio Kiyoshi","Matheus Fernandes"],"categories":null,"content":"Motivation First we need to talk about the motivation behind this plugin. The architecture Power was designed for use in artificial intelligence and deep learning. Investigating tools for deep learning and machine learning we found TensorBoard. TensorBoard is a tool to view models which were created in TensorFlow, the TensorBoard is a toolkit that allows graphic visualization of your models, making it easier to understand the used model, debug bottlenecks and, as a result, optimize it. In this blog post, we’ll show how to use the plugin we’ve created, which adds a new feature to the TensorBoard, this plugin assists on debugging bottlenecks in conjunction with the trace-viewer.\nPowerBoard PowerBoard is a plugin designed to show the power that is consumed while the neural network is trained, doing that allows the trace-viewer a better understanding of the model and helps to debug the bottlenecks.\nPrerequisites:\n Python 3.6 \u0026gt;= TensorFlow 2.2 \u0026gt;= TensorBoard 2.2 \u0026gt;= Pandas 1.2.1 \u0026gt;= ipmitool   If you\u0026rsquo;re having trouble installing tensorflow follow the link to a blog post that teaches you how to install tensorflow in the Power architecture: https://openpower.ic.unicamp.br/post/profiling-using-tensorboard-profiler/ https://openpower.ic.unicamp.br/post/installing-tensorflow-on-power/\n For the installation of PowerBoard Plugin acess the site https://pypi.org/project/powerboard/, or use the following pip command:\npip install powerboard Now you\u0026rsquo;re able to use the powerboard. The powerboard possesses an implementation library called libipmi, which is responsible for accessing the low level register to obtain the power consumption. For this, do the following import:\nfrom powerboard import libipmi libipmi:  The function start()  : This function is responsible for measuring the power consumption and time of each aquisition. The function stop()  : This function is responsible for stopping the aquisition of data. The function dbToCSV(\u0026lt;PATH\u0026gt;): The implementation of the function gets the data and saves it to a database, when the database is full the implementation saves all data in a csv file. For this, the argument PATH is the path to a directory were the data will be stored. My suggestion is to default the path to \u0026ldquo;./data\u0026rdquo; like this:  libipmi.dbToCSV(\u0026#39;./data\u0026#39;) Now you are able to test the plugin, for this I\u0026rsquo;ll show an example of the code:\nfrom datetime import datetime from packaging import version import os import tensorflow as tf from powerboard import libipmi   from tensorflow.keras import datasets, layers, models  stamp = datetime.now().strftime(\u0026#34;%Y%m%d-%H%M%S\u0026#34;) logdir = \u0026#39;logs/Nfit/%s\u0026#39; % stamp writer = tf.summary.create_file_writer(logdir) tf.summary.trace_on(profiler=True)  (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()  # Normalize pixel values to be between 0 and 1 train_images, test_images = train_images / 255.0, test_images / 255.0   class_names = [\u0026#39;airplane\u0026#39;, \u0026#39;automobile\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;deer\u0026#39;,  \u0026#39;dog\u0026#39;, \u0026#39;frog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;ship\u0026#39;, \u0026#39;truck\u0026#39;]   model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(32, 32, 3))) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;))  model.summary() model.add(layers.Flatten()) model.add(layers.Dense(64, activation=\u0026#39;relu\u0026#39;)) model.add(layers.Dense(10)) model.summary()     model.compile(optimizer=\u0026#39;adam\u0026#39;,  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  metrics=[\u0026#39;accuracy\u0026#39;])   libipmi.start() history = model.fit(train_images, train_labels, epochs=2,batch_size=64,  validation_data=(test_images, test_labels))  test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)  libipmi.stop() libipmi.dbToCSV(\u0026#39;./data\u0026#39;)   with writer.as_default():  tf.summary.trace_export(  name=\u0026#34;convolution\u0026#34;,  step=0,  profiler_outdir=logdir) before you run the script let\u0026rsquo;s make a directory in tmp files, so go to /tmp by doing  cd /tmp and make a directory inside. I\u0026rsquo;ll create the \u0026ldquo;teste\u0026rdquo; directory inside of tmp, as follows:\ncd /tmp mkdir teste Now go to the directory which the script was saved and run the script. Now copy the following directory into the /tmp/teste by doing:\ncp -r demo_logs /tmp/teste cp -r logs /tmp/teste Now run the TensorBoard:\n tensorboard --logdir /tmp/teste Now let\u0026rsquo;s have some fun by cracking our heads to understand the bottlenecks using the trace-viewer and powerboard tools. the following image shows the powerboard.\n Figure 1: PowerBoard.\n Now I\u0026rsquo;ll show the trace-viewer overview.  Figure 2: Trace-viewer overview.\n Let\u0026rsquo;s zoom in the image. You may control the graph by clicking in the button w to zoom in , s to zoom out, d to scroll right and e to scroll left.  Figure 3: Trace-viewer zoom in.\n The trace-viewer cracks, parallels and executes all operations in CPU and/or GPU to train our neural network. If you\u0026rsquo;re using GPU the documentation of TensorBoard says \u0026ldquo;As a general rule of thumb, it is a good idea to always keep the device (GPU/TPU) active. Use the tf.data API to optimize the input pipeline. In this case, let\u0026rsquo;s cache the training dataset and prefetch the data to ensure that there is always data available for the GPU to process. See here for more details on using tf.data to optimize your input pipelines\u0026rdquo;. In the trace-viewer we have the trace-context to aid in a better understanding of what is happening.\n Go to: https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras for a complete documentation.\n  Figure 4: Trace-viewer: trace context.\n Click on the first occurrence of the traceContext and look at \u0026lsquo;slice\u0026rsquo;, this part of the trace viewer shows all the arguments of the function, such as the time spent on this function.  Figure 5: Trace context: inside of slice.\n Note that we are at the epoch number \u0026ldquo;0\u0026rdquo; and step_name \u0026ldquo;0\u0026rdquo;, that is, in this part of the x-axis the graph indicates the first image to train our neural network. If we proceed to the next trace context it will follow to the next images to be trained. When the epoch value changes to 1 it means that we have finished training our neural network for the first time, and a new epoch will begin. Note that the size of the set of images is selected by the batch_size slice of our code, that is, we take the total number of images from the dataset and divide them by batch_size, in general the batch_size are powers of 2.  Figure 6: Trace context: the next interation.\n Now the next part is understanding the power consumption and relationships with the code. For this the following image shows the last interation of 1 complete epoch:\n Figure 7: Trace context: the last interation of epoch 0.\n Note that a full epoch takes about 22 seconds, as shown by the powerboard tab. Symmetry in the graph is expected, but it does not occur. Also note that between times 27.31 s and 28.78 s we have a sudden increase in power, so we will go in the trace-viewer to look at what is being carried out during this interval. In the meantime, we note that there is the following set of images from the training in epoch 1 [185-258]. Returning this set to epoch 0 it corresponds to the time interval between 9.35 s and 11.13 s. It is possible to conclude that theoretically the epochs should be symmetrical in terms of operations, that is, the energy expenditure should be the same, but it is noticed that something beyond the code is supplying this increase in extra power consumption by the same part of operations. Perhaps a new approach at a lower level would be needed to reach better conclusions.\n","date":1617926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617926400,"objectID":"26fd48b8a9fcac4214e2a3fd6a85cd4a","permalink":"https://openpower.ic.unicamp.br/post/powerboard-plugin-for-tensorboard/","publishdate":"2021-04-09T00:00:00Z","relpermalink":"/post/powerboard-plugin-for-tensorboard/","section":"post","summary":"Motivation First we need to talk about the motivation behind this plugin. The architecture Power was designed for use in artificial intelligence and deep learning. Investigating tools for deep learning and machine learning we found TensorBoard. TensorBoard is a tool to view models which were created in TensorFlow, the TensorBoard is a toolkit that allows graphic visualization of your models, making it easier to understand the used model, debug bottlenecks and, as a result, optimize it.","tags":null,"title":"PowerBoard plugin for TensorBoard","type":"post"},{"authors":["Matheus Fernandes","Julio Kiyoshi"],"categories":null,"content":"TensorFlow is a very popular open-source library for Machine Learning and in this post we will see two ways (from a Community Supported Build and from the IBM Watson Machine Learning Community Edition) of installing it on POWER.\nInstalling TensorFlow from the Community Supported Build On the TensorFlow repository README on GitHub (https://github.com/tensorflow/tensorflow) there is a list of community builds of TensorFlow, in which includes CPU and GPU builds for POWER.\nWe will be using the links available on there to install TensorFlow on POWER.\nBefore Installation Install the build-essential package with:\nsudo apt-get update sudo apt-get install build-essential Download and Install Anaconda package manager: We\u0026rsquo;ll use Anacona to install TensorFlow within a virtual environment.\nDownload: https://www.anaconda.com/products/individual\nRemember to check the script sha256sum: https://docs.anaconda.com/anaconda/install/hashes/\nTo install Anaconda Individual Edition, you just need to run the script downloaded and follow the inscructions that it provides.\nCreate Virtual Environment and activate it:\nconda create --name tf_env python=3.6 conda activate tf_env We\u0026rsquo;ll be using python3.6 for this environment installation.\nTo install Anaconda Individual Edition, you just need to run the script downloaded and follow the inscructions that it provides.\nDownload and Install a TensorFlow build: The Community Supported Builds from TensorFlow repository README provides both TF Release 1.15 and 2.x versions for both CPU-only and GPU.\nIn this tutorial we will be installing a 2.x CPU-only version, altough the same process can be use for the GPU version.\n Access https://github.com/tensorflow/tensorflow and select the needed build.\nWe\u0026rsquo;ll use the Release 2.x CPU (https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/)  Download the needed version and pay attention to the requirements (for instance, our selected build was built for GLIBC 2.17 and above).\nWe\u0026rsquo;ll download the cp36 version, since we created a virtual conda environment for python 3.6.  After the .whl file was downloaded, we\u0026rsquo;ll install it with pip within our conda environment.  Attention When installing TensorFlow, it is possible that some requirements errors occurs. If that happens, just install the missing library and try instaling TensorFlow again.\nFor intance, we needed to install scipy 1.4.1 and h5py before the installation succeeded, and that can be done with Anaconda with the following commands:\nconda install -c conda-forge scipy=1.4.1 conda install -c conda-forge h5py Installing TensorFlow from the .whl file with pip inside the conda environment:\npip install tensorflow_cpu-2.2.0-cp36-cp36m-linux_ppc64le.whl After the installation is completed, check TensorFlow within a python shell with:\nimport tensorflow as tf print(tf.__version__) [ALTERNATIVE] Installing TensorFlow with Watson Machine Learning Community Edition IBM Watson Machine Learning Community Edition also provides a TensorFlow installation through Anaconda, which will be taught in this tutorial.\nDownload and Install Anaconda package manager: Download: https://www.anaconda.com/products/individual\nRemember to check the script sha256sum: https://docs.anaconda.com/anaconda/install/hashes/\nTo install Anaconda Individual Edition, you just need to run the script downloaded and follow the inscructions that it provides.\nAdd WMLCE Channel to Anaconda After the installation, add WMLCE (IBM Watson Machine Learning Community Edition) channel:\nconda config --prepend channels \\  https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/ Create an Anaconda Python3.6 environment for WMLCE and activate it:\n Currently (09/2020) WMLCE only works with python version 3.6\n conda create --name wmlce_env python=3.6 conda activate wmlce_env Install TensorFlow For CPU-only use, install TensorFlow with:\nconda install tensorflow For GPU use, install TensorFlow with:\nconda install tensorflow TF is now installed and ready for use.\nReference: Anaconda:\nhttps://www.anaconda.com/products/individual\nTensorFlow GitHub repository:\nhttps://github.com/tensorflow/tensorflow\nIBM Watson Machine Learning Community Edition:\nhttps://www.ibm.com/support/knowledgecenter/SS5SF7_1.6.1/navigation/welcome.html\nWMLCE softwares list:\nhttps://www.ibm.com/support/knowledgecenter/SS5SF7_1.6.1/navigation/wmlce_software_pkgs.html\n","date":1615075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615075200,"objectID":"bc115801bc2ca6247a1149b61e356ae0","permalink":"https://openpower.ic.unicamp.br/post/installing-tensorflow-on-power/","publishdate":"2021-03-07T00:00:00Z","relpermalink":"/post/installing-tensorflow-on-power/","section":"post","summary":"TensorFlow is a very popular open-source library for Machine Learning and in this post we will see two ways (from a Community Supported Build and from the IBM Watson Machine Learning Community Edition) of installing it on POWER.\nInstalling TensorFlow from the Community Supported Build On the TensorFlow repository README on GitHub (https://github.com/tensorflow/tensorflow) there is a list of community builds of TensorFlow, in which includes CPU and GPU builds for POWER.","tags":null,"title":"Installing Tensorflow on POWER","type":"post"},{"authors":["Júnior Santos"],"categories":null,"content":"In a comparison between IBM Power and x86, it can be said that the best options between them will depend on their use. The x86 chips are intended for general use, have good scalability and high performance in almost all uses. On the other hand, Power chips are focused on using high-performance and high-performance servers. It has support to meet emerging demands, it has virtualization natively, with several hardware resources focused on virtualization, being the best possible choice for this type of work.\nIn addition, IBM Power is focused on the business line, having support plans for different business activities. Mainly focused on virtualization solutions, to meet massive work demands. In addition, the chips have resources to share jobs or pool resources, making multiple servers behave as one.\nIn this way, IBM Power becomes a fundamental part of the business plan of any company linked to technology or that needs a great scalability of resources to meet a massive demand for work. As well as mainly for the cloud computing area.\nPOWER8 Power8 was presented by IBM in 2014. There, IBM made several improvements over its previous version. The machines that were created with this chip, had the provision of 6 to 12 cores, in addition to a clock that varies from 2.5 GHz to 5 GHz. Power8 has 32 KB for instructions + 64 KB for data in L1 cache, 512 KB for SRAM type in L2 cache, 96 MB for eDRAM type in L3 cache and 128 MB for eDRAM type in L4 cache.\n   Core CPU L1 cache L2 cache L3 cache L4 cache     6 to 12 2.5 GHz to 5 GHz 64 KB + 32 KB 512 KB 96 MB 128 MB    When it comes to processors, memory is a fundamental resource. The cache memory is faster than the main memory (RAM memory), because of that, its size is fundamental for better processor performance. When compared to the previous version of the chip, the L3 cache memory had its size increased. This resulted in part to the higher performance of the processor compared to its predecessor.\nPower8 has many more features than its x86 competitors and its predecessor, being more powerful than them. In addition, it has support for simultaneous multithreading with eight cores per thread (SMT-8), having a very high degree of parallelism.\nPOWER9 Power9 was presented by IBM in 2017. This version has improved core and hardware, the chip is smaller resulting in an optimization in energy consumption. The number of cores doubled to 24, the clock was set at 4 GHz. Power9 has 32 KB for instructions + 32 KB for data in L1 cache, 512 KB for SRAM type in L2 cache, 128 MB for eDRAM type in L3 cache.\n   Core CPU L1 cache L2 cache L3 cache     24 4 GHz 32 KB + 32 KB 512 KB 128 MB    The increase in the number of cores, plus the reduction in the size of the chip, with the increase of the L3 cache, optimizes and increases the processing power. Power9 has 1.5x better performance and 2x more memory than Power8.\nPower9 has a greater acceleration than its previous versions, it had optimized the reading of memories of the type DDR4. Based on the acceleration, it was possible to reduce the cycle processes, the cost of hardware and increase efficiency.\nThe chip has been improved to have a higher bandwidth and low latency interface. This improvement was achieved through an interface created by NVIDIA, called NVLink. In addition, the architecture has also been optimized for emerging workloads. Improving your performance in carrying out work for high performance computing.\nAll of these improvements were made with the prospect of creating a more optimized processor to develop high-performance operations, from cloud computing, to large data centers and research.\nPOWER10 Power10 was presented by IBM in 2020. The chip has been enhanced for faster processing speed and greater capacity for intensive calculations. The number of cores can vary from 15 to 30, with a clock that varies from 4.5 GHz to 4 GHz. Power10 has 32 KB for instructions + 32 KB for data in the L1 cache, 2 MB type SRAM in the L2 cache, 128 MB type eDRAM in the L3 cache.\n   Core CPU L1 cache L2 cache L3 cache     15 to 30 3.5 GHz to 4 GHz 32 KB + 32 KB 2 MB 128 MB    Power10 is designed to achieve a high degree of performance in existing encryption standards and in future encryption standards. Power10 has implemented a mathematical matrix accelerator in its cores. This resulted in an AI 10x, 15x, 20x faster inference for FP32, BFloat16 and INT8 calculations, respectively, compared to Power9.\nSeveral changes have been made compared to its predecessor. Power10 had its reading hardware optimized, support for DDR5 memories was added, the L2 cache memory had its capacity increased, in addition to the chip which had its size reduced. These characteristics mean that the Power10 chip has increased performance and its optimization prepares the chip to support the newest technologies developed.\nIBM implemented PowerAXON on Power10. It has the ability to share the main memory of a Power10 server with other Power10 servers. This feature can be used to allow a set of small machines to become a large machine with great processing power.\nWith all the features of previous versions optimized, with the addition of innovative features and with increased processing power and capacity, make this processor the most powerful that IBM has ever created. Perfect for data analysis jobs, cloud computing, high performance programming and among other jobs that need up-to-date and powerful machines.\n OpenPower Lab does not have Power10 servers within its collection.\n ","date":1614902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614902400,"objectID":"280845e5610f79acef8dd466f49a1daf","permalink":"https://openpower.ic.unicamp.br/post/series-ibm-power/","publishdate":"2021-03-05T00:00:00Z","relpermalink":"/post/series-ibm-power/","section":"post","summary":"In a comparison between IBM Power and x86, it can be said that the best options between them will depend on their use. The x86 chips are intended for general use, have good scalability and high performance in almost all uses. On the other hand, Power chips are focused on using high-performance and high-performance servers. It has support to meet emerging demands, it has virtualization natively, with several hardware resources focused on virtualization, being the best possible choice for this type of work.","tags":null,"title":"IBM Power Systems - Power8, Power9 and Power10","type":"post"},{"authors":["Júnior Santos"],"categories":null,"content":"Docker has been used for a long time with Kubelet and Kubernetes. However, recently Docker announced that it would cut support for kubelet.\nBecause of this, this blog intends to present existing options to assume this role with Kubernetes. All Kubernetes needs is a Container Runtime Interface (CRI), for that there are several portable software for the IBM Power architecture (ppc64le), as it will be presented throughout the post.\nMain Runtimes Through the Open Source community it is possible to find some packages like: Containerd-CRI, Cri-O e Gvisor. All packages have the function of providing the support that Kubernetes needs, and some of them even have extra and more specialized features.\nFor most of these packages it was possible to port to the Power architecture, however, as will be presented later, some were not possible to perform portability.\nContainerd-CRI Description and Portability CRI is a Containerd plugin for Kubernetes, it is a Container Runtime Interface (CRI) implemented to interact with Kubelet in the creation of containers using Containerd as Runtime.\nThe software does not have official support for the IBM Power architecture, despite that, OpenPower Lab did a build job and made the package functional in Power. In addition, we offer the package in the following formats: binary, .rpm and .deb.\nBuild The construction of the package was done following the recipe for CI/CD of the package, it is possible to obtain it through the GitHub. Based on it, we make the package available in binary format which can be obtained through FTP. The rest of the packages, available for Debian and RHEL based distros, can be installed through OPenPower lab repository.\nCri-O Description and Portability Cri-O is a CRI (Container Runtime Interface) implementation, being a lighter alternative to Docker as a Runtime. Since Docker is no longer an option, it becomes one of the lightest options available.\nThe software does not have official support for the IBM Power architecture, despite that, OpenPower Lab did a build job and made the package functional in Power. In addition, the package is available in binary version and in .deb format. Although it was not possible to generate the .rpm package, it is still possible to install the package with the installation of the binary.\nBuild The construction of the package can be obtained through build. The binary can be obtained by FTP, despite this, it is necessary to install the prerequisites. The .deb format can be installed via the OPenPower lab repository.\nInstall It was not possible to convert the binary to the RPM Package Manager, so the only way to install these systems is to install the binary. Before installing the binary it is necessary to install the prerequisites. After installing the prerequisites, just use the command below:\nsudo tar -C / -xzvf crio-1.20.0.linux-ppc64le.tar.gz Confirm the installation by executing the command:\ncrio --version Gvisor Description and Portability GVisor is a software widely used by the community, it is focused on integration with Docker and Kubernetes. The code is written in GO, uses Bazel to build the source code and generate the binary. However, Bazel requires several build files, and supported architectures must be specified. However, the Gvisor only has reference to AMD (x86_64).\nAs such, Bazel is unable to generate the binary for IBM Power (ppc64le). Therefore, Gvisor does not have support for the Power architecture, and portability is considered as difficult and necessary reverse engineering work in the source code.\nLinks  The OpenPower Lab repository contains several Open-Source software for Power. See the list of available packages. See more about the lab\u0026rsquo;s work and our research by reading other posts.  ","date":1614297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614297600,"objectID":"e957d1114fa224f4af11595d9c836a07","permalink":"https://openpower.ic.unicamp.br/post/options-for-kubernetes-cri/","publishdate":"2021-02-26T00:00:00Z","relpermalink":"/post/options-for-kubernetes-cri/","section":"post","summary":"Docker has been used for a long time with Kubelet and Kubernetes. However, recently Docker announced that it would cut support for kubelet.\nBecause of this, this blog intends to present existing options to assume this role with Kubernetes. All Kubernetes needs is a Container Runtime Interface (CRI), for that there are several portable software for the IBM Power architecture (ppc64le), as it will be presented throughout the post.\nMain Runtimes Through the Open Source community it is possible to find some packages like: Containerd-CRI, Cri-O e Gvisor.","tags":null,"title":"Options for Kubernetes-CRI (Container Runtime Interface)","type":"post"},{"authors":["Júnior Santos"],"categories":null,"content":"Docker is a set of systems that use virtualization to deliver software in packages, which are called containers. The containers are isolated from the operating system and other containers, they have unique libraries, as well as packages and configuration files. In addition, they can run different operating systems and have a lower resource cost than traditional virtualization.\nWe provide the docker-ce, an edition of Docker that is free software, updated by the community. However, the package is officially not available for the Power architecture (ppc64 / ppc64le). Thus, the laboratory makes the latest versions available for installation from the package managers: Advanced Packaging Tool (APT) and Red Hat Package Manager (RPM).\nAdd the repository and install Docker To add the repository to the system, and to always obtain the new versions of software that we provide, use the command for your package manager.\nAdd APT repository To install, use the command:\nsudo echo \u0026#34;deb https://oplab9.parqtec.unicamp.br/pub/repository/debian/ ./\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list; wget https://oplab9.parqtec.unicamp.br/pub/key/openpower-gpgkey-public.asc; sudo apt-key add openpower-gpgkey-public.asc; sudo apt -y update; sudo apt -y install docker-ce Add RPM repository To install, use the command:\nwget https://oplab9.parqtec.unicamp.br/pub/repository/rpm/open-power-unicamp.repo; sudo mv open-power-unicamp.repo /etc/yum.repos.d/; sudo yum -y update; sudo yum -y install docker-ce  wget requirement.\n  To install using other means or other packages, see Project Power Repository or Project Power Builds.\n ","date":1611273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611273600,"objectID":"78c0e04bae8a65f7ad5d7b0630a945ab","permalink":"https://openpower.ic.unicamp.br/post/installing-docker-from-repository/","publishdate":"2021-01-22T00:00:00Z","relpermalink":"/post/installing-docker-from-repository/","section":"post","summary":"Docker is a set of systems that use virtualization to deliver software in packages, which are called containers. The containers are isolated from the operating system and other containers, they have unique libraries, as well as packages and configuration files. In addition, they can run different operating systems and have a lower resource cost than traditional virtualization.\nWe provide the docker-ce, an edition of Docker that is free software, updated by the community.","tags":null,"title":"Installing Docker from the OpenPower Lab Repository","type":"post"},{"authors":["Matheus Fernandes"],"categories":null,"content":"In this post, we will show how to get data on power consumption in a POWER9 bare-metal machine and how to plot this data using python.\nTo measure the power consumption, a program called ipmitool will be used, once it give us the access to ipmi sensor data.\nIn order to get a good span of values, a Image Classification script was used, which its code and execution procedure can be found here: https://openpower.ic.unicamp.br/post/ai-profiling-for-power/\nUsing ipmitool to get sensors data In order to get a list of IPMI sensors, use the following command.\nCommand: sudo ipmitool sensor list Output fields meaning with example:\n   Sensor ID Sensor Reading Sensor Reading Unit Status Lower Non-Recoverable Lower Critical Lower Non-Critical Upper Non-Critical Upper Critical Upper Non-Recoverable     CPU1 Temp 34.000 degrees C ok 5.000 5.000 10.000 88.000 90.000 92.000    To get a single sensor data, use the command:\nipmitool sensor get \u0026lt;Sensor ID\u0026gt;\nExample:\nsudo ipmitool sensor get CPU1\\ Temp\nLocating sensor record... Sensor ID : CPU1 Temp (0xb)  Entity ID : 65.1  Sensor Type (Threshold) : Temperature  Sensor Reading : 36 (+/- 1) degrees C  Status : ok  Lower Non-Recoverable : 5.000  Lower Critical : 5.000  Lower Non-Critical : 10.000  Upper Non-Critical : 88.000  Upper Critical : 90.000  Upper Non-Recoverable : 92.000  Positive Hysteresis : 2.000  Negative Hysteresis : 2.000  Assertion Events :  Assertions Enabled : ucr+  Deassertions Enabled : ucr+ The power measurement was collected by using the following python script, which not only calls ipmitool, but also parse the data into a csv file.\nimport sys import subprocess import time import csv  duration = sys.argv[1] file_name = sys.argv[2] sensors = sys.argv[3:] time_begin = time.time()  with open(file_name + \u0026#39;.csv\u0026#39;, \u0026#39;w\u0026#39;) as file_out:  write = csv.writer(file_out)  first_row = [\u0026#39;Sensor_ID\u0026#39;, \u0026#39;Entity_ID\u0026#39;, \u0026#39;Sensor_Type_Threshold_\u0026#39;,  \u0026#39;Sensor_Reading\u0026#39;, \u0026#39;Status\u0026#39;,  \u0026#39;Lower_Non_Recoverable\u0026#39;, \u0026#39;Lower_Critical\u0026#39;,  \u0026#39;Lower_Non_Critical\u0026#39;, \u0026#39;Upper_Non_Critical\u0026#39;, \u0026#39;Upper_Critical\u0026#39;,  \u0026#39;Upper_Non_Recoverable\u0026#39;, \u0026#39;Positive_Hysteresis\u0026#39;,  \u0026#39;Negative_Hysteresis\u0026#39;, \u0026#39;Assertion_Events\u0026#39;,  \u0026#39;Assertions_Enabled\u0026#39;, \u0026#39;Deassertions_Enabled\u0026#39;, \u0026#39;Time_elapsed\u0026#39;]  write.writerow(first_row)  end = time.time() + float(duration)  while end \u0026gt; time.time():  for sens in sensors:  command = [\u0026#39;sudo\u0026#39;, \u0026#39;ipmitool\u0026#39;, \u0026#39;sensor\u0026#39;, \u0026#39;get\u0026#39;, sens]  process = subprocess.run(  command,  stdout=subprocess.PIPE,  universal_newlines=True)  output = process.stdout  output = output.split(\u0026#39;\\n\u0026#39;)[2:-2]  current_row = []  current_row.append(sens)  for i in range(len(output)):  output[i] = output[i].split(\u0026#39;:\u0026#39;)  output[i][0] = output[i][0].replace(\u0026#39; \u0026#39;, \u0026#39;\u0026#39;)  for j in range(1,len(output[i])):  output[i][j] = output[i][j].split()  if len(output[i][j]) \u0026gt; 0:  current_row.append(output[i][j][0])  else:  current_row.append(\u0026#39;\u0026#39;)  current_row.append(\u0026#34;{:.5f}\u0026#34;.format(time.time() - time_begin))  write.writerow(current_row) This was the python command used:\npython3 sensorsIPMI.py 1000 pwr.csv Total\\ Power CPU1\\ Power CPU2\\ Power PCIE\\ CPU1\\ Pwr PCIE\\ CPU2\\ Pwr Plot the data In order to plot the data, a jupyter notebook was used.\nWhich is available here:\n\nThe following graphs were ploted in the notebook: All sensors:\nTotal Power:\nCPU1 Power:\nCPU2 Power:\nPCIE CPU1 Power:\nPCIE CPU2 Power:\nIPMI on OpenPOWER source:\n https://www.ibm.com/developerworks/library/l-openpower-firmware-ipmi/index.html https://www.ibm.com/support/knowledgecenter/9006-22C/p9eih/p9eih_ipmi_syshealth.htm  Although made for a different hardware vendor, a good source for ipmitool commands can be found on the following url: https://docs.oracle.com/cd/E19464-01/820-6850-11/IPMItool.html\n","date":1608163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608163200,"objectID":"c8a6845335cda2ec252dde05eb31ac18","permalink":"https://openpower.ic.unicamp.br/post/power-consumption-on-power/","publishdate":"2020-12-17T00:00:00Z","relpermalink":"/post/power-consumption-on-power/","section":"post","summary":"In this post, we will show how to get data on power consumption in a POWER9 bare-metal machine and how to plot this data using python.\nTo measure the power consumption, a program called ipmitool will be used, once it give us the access to ipmi sensor data.\nIn order to get a good span of values, a Image Classification script was used, which its code and execution procedure can be found here: https://openpower.","tags":null,"title":"Measuring energy power consumption on POWER9 through IPMI sensors","type":"post"},{"authors":["Julio Kiyoshi","Matheus Fernandes"],"categories":null,"content":"This blog post will show how to install tensorflow 2.2 in POWER, how to use profiler and make a comparison between different architectures ( x86, POWER 8 and 9).\nPrerequisites In this part I\u0026rsquo;ll show how to setup your Virtual Machine (VM) and install tensorflow 2.2 in POWER. My PIP version is 20.3 and my version of python is 3.8.\nFirst we need to install some libraries to install tensorflow 2.2.\nInstalling dependecies of scipy:\n sudo apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran Installing h5py:\n sudo apt install python3-h5py Installing keras using pip:\n pip3 install -U --user keras_applications --no-deps  pip3 install -U --user keras_preprocessing --no-deps  Now we are able to install tensorflow 2.2. For this, access the site (https://github.com/tensorflow/tensorflow) to download .whl file (this file is used to install tensorflow using pip comand). First go in Community Supported Builds Section, and click in Artifacts release 2.x of Linux ppc64le CPU Stable Release.\n Figure 1: Tensorflow 2.2 cpu- only installation.\n After clicking we are directed to jenkins, where we click in tensorflow_cpu-2.2.0-cp38-cp38-linux_ppc64le.whl. Note that \u0026ldquo;cp38\u0026rdquo; indicates that the tensorflow should be installed in python 3.8. However, if you are using different versions of python you can download the version corresponding to your python version. But in this tutorial I\u0026rsquo;ll show how to setup using python 3.8.\n Figure 2: Download .whl tensorflow 2.2 build.\n For download in VM, copy the link (tensorflow_cpu-2.2.0-cp38-cp38-linux_ppc64le.whl) and use the command below:\n wget https://powerci.osuosl.org/job/TensorFlow2_PPC64LE_CPU_Release_Build/lastSuccessfulBuild/artifact/tensorflow_pkg/tensorflow_cpu-2.2.0-cp38-cp38-linux_ppc64le.whl Now for installation of the tensorflow using pip command:\n pip3 install tensorflow_cpu-2.2.0-cp38-cp38-linux_ppc64le.whl For more information you can visit https://www.tensorflow.org/install/source.\nNow we need to install tensorboard, tensorboard-plugin-profiler and tensorflow-datasets.\n pip3 install --upgrade tensorboard  pip3 install tensorflow-datasets  pip3 install -U tensorboard_plugin_profile Get access to POWER 8 VM in minicloud Here is a brief tutorial on how to access POWER 8 virtual machine in minicloud, first access https://openpower.ic.unicamp.br/minicloud/ and click in Request Access and answer the google forms to get access. Here is a link that may help you to get access to an instance on minicloud https://github.com/Unicamp-OpenPower/minicloud/wiki. In the next section I\u0026rsquo;ll show to access tensorboard by terminal.\nSSH connection You\u0026rsquo;ll need to connect to VM via ssh using the -L 6006:localhost:6006 flag. To be able to use tensorboard in the terminal, your command should be like this:\nssh ubuntu@minicloud.parqtec.unicamp.br -i ~/.ssh/your-key.pem -p \u0026lt;vm-port\u0026gt; -L 6006:localhost:6006\nFor using tensorboard in the terminal we use this command:\n tensorboard --logdir=\u0026lt;name_of_log_directory\u0026gt; Now we are able to open the link in your favorite browser.\nCompare Tensorboard-Profiler in different architectures In this section, we will be profiling using Tensorboard-Profiler in different architectures and showing the results. First, we will standardize the test file. For this, download the file available in https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras and modify the line:\nmodel.fit(ds_train, epochs=2, validation_data=ds_test, callbacks = [tboard_callback]) to:\nmodel.fit(ds_train, epochs=5, validation_data=ds_test, callbacks = [tboard_callback]) Now we are ready to execute the script and debug performance bottlenecks using Tensorboard-Profiler.\n Sometimes when running Tensorflow we get some errors like: AttributeError: partially initialized module \u0026rsquo;tensorflow\u0026rsquo; has no attribute \u0026lsquo;version\u0026rsquo; (most likely due to a circular import). To fix this error you can use the flag -m.\n  python3 -m \u0026lt;your-python-file\u0026gt; After running the script in different architectures we obtain the following results:\nInput pipeline analyzer:\n Data preprocessing (ms)  Table 1: Data preprocessing in different architectures\n   X86 POWER8 POWER9     390 180 164     Reading data from files in advance (including caching, prefetching, interleaving) (in ms):  Table 2: Reading data from files in advance in different architectures\n   X86 POWER8 POWER9     6.7 ~ 0 ~0    Tensorflow stats:\n Operations which consume more time:  Table 3: Operations which consume more time in x86\n   Type Operation Occurrences total time (us)     Dataset Iterator::Model::MapAndBatch 21 112,868   Dataset Iterator::Model::MapAndBatch::Prefetch::ParallelMap 2.907 106,162   Dataset Iterator::Model::MapAndBatch::Prefetch::ParallelMap::ParallelMap 2.907 104,103   Decode Png decode_image/cond_jpeg/else/_1/cond_png/then/_0/DecodePng 2.910 79,162    Table 4: Operations which consume more time in POWER8\n   Type Operation Occurrences total time (us)     Dataset Iterator::Model::MapAndBatch 21 131,659   Dataset Iterator::Model::MapAndBatch::Prefetch::ParallelMap 2.673 23,513   MatMul gradient_tape/sequential/dense/MatMul 21 16,335   Dataset Iterator::Model::MapAndBatch::Prefetch 2.673 15,375    Table 5: Operations which consume more time in POWER9\n   Type Operation Occurrences total time (us)     Dataset Iterator::Model::MapAndBatch 21 114,562   _FusedMatMul sequential/dense/Relu 21 30,306   Dataset Iterator::Model::MapAndBatch::Prefetch::ParallelMap 2.676 20,957   Dataset Iterator::Model::MapAndBatch::Prefetch 2.675 16,722    Now we can analyze the dada and compare beteween different architectures. First we note that x86 consumes more time for data preprocessing and reading data from files in advance (Tables 1 and 2). In Tensorflow stats we can crack the entire code in operations but I\u0026rsquo;ll show only the top 4 time-consuming operations in tables 3, 4 and 5. However, you can get all operations in tensorboard-profiler in section Tensorflow Stats. From tables 3, 4 and 5 we obtain that the type of operation differs a little, for example in table 3 we have Decode Png in top 4, whereas in power architectures (Tables 4 and 5) we have matmul. But in all 3 architectures Dataset is highly time-consuming.\nAn interesting function in tensorboard-profiler is Recommendation for Next Step. This function highlights some otimizations that could improve your program, for exemple, when I execute my program in POWER 8 we have some recommendations like:\n Your program is HIGHLY input-bound because 68.8% of the total step time sampled is waiting for input. Therefore, you should first focus on reducing the input time 7.3 % of the total step time sampled is spent on All Others time.  Next tools to use for reducing the input time\n input_pipeline_analyzer (especially Section 3 for the breakdown of input operations on the Host) trace_viewer (look at the activities on the timeline of each Host Thread near the bottom of the trace view)  ","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"fb69cac67e2d37821713783b736bdca6","permalink":"https://openpower.ic.unicamp.br/post/profiling-using-tensorboard-profiler/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/post/profiling-using-tensorboard-profiler/","section":"post","summary":"This blog post will show how to install tensorflow 2.2 in POWER, how to use profiler and make a comparison between different architectures ( x86, POWER 8 and 9).\nPrerequisites In this part I\u0026rsquo;ll show how to setup your Virtual Machine (VM) and install tensorflow 2.2 in POWER. My PIP version is 20.3 and my version of python is 3.8.\nFirst we need to install some libraries to install tensorflow 2.","tags":null,"title":"Profiling using Tensorboard-Profiler","type":"post"},{"authors":["Matheus Fernandes","Julio Kiyoshi"],"categories":null,"content":"Although there is plenty information on AI profiling for x86_64 and ARM architectures, there is almost none on POWER.\nWith that motivation in mind, this post aim to share some results on this subject.\nThe program profiled was a python script that had a pre-trained ResNet50 with ImageNet weights, which was obtained from TensorFlow API.\nIt aimed to classify 500 hot-dogs images downloaded from the ImageNet.\nThe profiling was done using Perf for collecting PMU data and ipmitool for energy consumption data.\nRequirements for the pyhton script:\n Bare-metal machine ipmitool python 3.6 TensorFlow 2.1.1  Machine Stats:\n POWER9 Processor CPU(s): 128 On-line CPU(s) list: 0-127 Thread(s) per core: 4 Core(s) per socket: 16 Socket(s): 2 NUMA node(s): 2 Model: 2.2  You can find information on how to install TensorFlow on POWER in this post: https://openpower.ic.unicamp.br/post/building-tensorflow-on-power/\nEleven tests were executed.\nfrom tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.preprocessing import image from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions import numpy as np import sys import time from datetime import datetime  begin = time.time()  #folder = sys.argv[1] length = 731 length = int(sys.argv[1]) if (length \u0026gt; 731):  print(\u0026#34;Using maximum length: 731\u0026#34;)  lenght = 731 again = int(sys.argv[2]) folder_name = \u0026#34;hot_dog\u0026#34;  model = ResNet50(weights=\u0026#39;imagenet\u0026#39;)  images = [] count = 0 for i in range(length):  img_path = folder_name + \u0026#39;/\u0026#39; + str(i) + \u0026#39;.jpg\u0026#39;  img = image.load_img(img_path, target_size=(224, 224))  images.append(image.img_to_array(img))  images[i] = np.expand_dims(images[i], axis=0)  images[i] = preprocess_input(images[i])  for j in range(again):  for i in range(length):  prediction = model.predict(images[i])  #print(decode_predictions(prediction[i], top=1)[0][0][1])  strPrediction = decode_predictions(prediction, top=1)[0][0][1]  if (strPrediction == \u0026#39;hotdog\u0026#39;):  count += 1  else:  #print(str(i) + \u0026#34; -\u0026gt; \u0026#34; + strPrediction)  pass  print(\u0026#34;Begin: \u0026#34; + datetime.utcnow().strftime(\u0026#34;%H:%M:%S\u0026#34;)) print(\u0026#34;End: \u0026#34; + datetime.utcnow().strftime(\u0026#34;%H:%M:%S\u0026#34;)) print(\u0026#34;RIGHTS: {}\u0026#34;.format(count)) print(\u0026#34;WRONGS: {}\u0026#34;.format(again*length - count)) print(\u0026#34;ACC: {}\u0026#34;.format(count/(again*length))) print(\u0026#34;Time Elapsed: {}s\u0026#34;.format(time.time() - begin)) Because the model is pre-trained, it obtained the same classification accuracy for every test.\nRIGHTS: 433 WRONGS: 67 Profiling using perf. Perf is a profiling program included with the Linux kernel. Here it was used to instrument CPU performance counters.\nPMUs used:\nbranches, branch-misses, cache-misses, cache-references, cycles, instructions, idle-cycles-backend, idle-cycles-frontend. The following graphs shows the data fetched from those PMUs.\nGraphs:\nCPU Cycles:\nInstructions:\nCache-references:\nCache-misses:\nBranches:\nBranch-misses:\nTime Elapsed:\nEnergy consumption. Make sure you are running on a bare-metal machine.\nHow to use the ipmitool to get power consumption data: Install ipmitool through:\nsudo apt-get install ipmitool Then run the command:\nsudo ipmitool dcmi power reading Which is going to give you the output:\n  Instantaneous power reading: 262 Watts  Minimum during sampling period: 248 Watts  Maximum during sampling period: 263 Watts  Average power reading over sample period: 257 Watts  IPMI timestamp: Sun Nov 8 19:51:18 2020  Sampling period: 00000005 Seconds.  Power reading state is: activated This command was executed continuosly for 1500 seconds using a python script that would parse the results into a csv file.\nAltough the sampling period was used for reference in order to plot the following graph, it does not represent an accurate time series in the x axis. For a better undertanding of power consumption profiling on POWER with ML algorithms, see the following post: https://openpower.ic.unicamp.br/post/power-consumption-on-power/\nThat said, the data was used to plot the following graph:\nIt is possible to see the average of energy consumption for each test and, at the end, the energy consumption going back to a normal state. It can also be observed that there is an increase close to 100W when a test begins to run.\n","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605484800,"objectID":"b3c59ceed071d10fc211f87137faf746","permalink":"https://openpower.ic.unicamp.br/post/ai-profiling-for-power/","publishdate":"2020-11-16T00:00:00Z","relpermalink":"/post/ai-profiling-for-power/","section":"post","summary":"Although there is plenty information on AI profiling for x86_64 and ARM architectures, there is almost none on POWER.\nWith that motivation in mind, this post aim to share some results on this subject.\nThe program profiled was a python script that had a pre-trained ResNet50 with ImageNet weights, which was obtained from TensorFlow API.\nIt aimed to classify 500 hot-dogs images downloaded from the ImageNet.\nThe profiling was done using Perf for collecting PMU data and ipmitool for energy consumption data.","tags":null,"title":"AI Profiling for POWER","type":"post"},{"authors":["Júnior Santos","Matheus Fernandes"],"categories":null,"content":"This tutorial shows you how to create a cluster for the Power architecture (ppc64le) using Minikube.\nThe tutorial was performed on Ubuntu 20.10 (ppc64le), the packages were downloaded using the package repository from OpenPower Lab @ Unicamp.\nDependencies The following packages are required:\n Minikube Kubectl Docker-ce Conntrack  You can use the commands below to solve the dependencies:\napt-get update apt-get install docker-ce conntrack minikube kubectl  Optionally, Kubeadm and Kubelet can be installed.\n Create a minikube cluster  Start Minikube  sudo minikube start --driver=none  The default drive is Docker, however the minikube does not recognize that Docker is available for ppc64le architecture and has an error.\n To make \u0026rsquo;none\u0026rsquo; the default drive, use the command:\nsudo minikube config set driver none  You may need to run the command:: sudo sysctl fs.protected_regular=0\n Check Status  sudo minikube status The output is similar to:\nminikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Open the Kubernetes dashboard in a browser  sudo minikube dashboard Create a Deployment There are two structures in Kubernetes: Pod and Deployment. Pod can be a group of one or more Containers, while a Deployment checks, manages and restarts the pods. That is, the deployment is recommended when it will be used in a large group of pods.\n Create a Deployment  sudo kubectl create deployment hello-node --image=minicloud/node-server  minicloud/node-server: is a public docker image created for the ppc64le architecture. The files used to build the image are in the GitHub.\n View the Deployment:  sudo kubectl get deployments The output is similar to:\nNAME READY UP-TO-DATE AVAILABLE AGE hello-node 1/1 1 1 6m28s View the Pod:  sudo kubectl get pods The output is similar to:\nNAME READY STATUS RESTARTS AGE hello-node-5dd47b76c8-l5vs2 1/1 Running 0 6m51s Create a Service In order to be able to directly access the Pod, it is necessary to create a service.\n Create a Service  sudo kubectl expose deployment hello-node --type=NodePort --port=8080 View the Service  sudo kubectl get services The output is similar to:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node NodePort 10.102.223.224 \u0026lt;none\u0026gt; 8080:31253/TCP 8s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 14m Open the service in the browser: http://localhost:8080/.\n If it is not possible to access this port, change the 8080, for the 5 digit port that appears in the view. In that case it would be port 31253.\n Clean up Now you can clean up the resources you created in your cluster:\nkubectl delete service hello-node kubectl delete deployment hello-node Optionally, stop the Minikube:\nminikube stop Optionally, delete the Minikube:\nminikube delete Tutorial for others architectures Hello Minikube\n","date":1605052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605052800,"objectID":"0eb36a0cc959b983b8da4913acaba8b8","permalink":"https://openpower.ic.unicamp.br/post/hello-minikube/","publishdate":"2020-11-11T00:00:00Z","relpermalink":"/post/hello-minikube/","section":"post","summary":"This tutorial shows you how to create a cluster for the Power architecture (ppc64le) using Minikube.\nThe tutorial was performed on Ubuntu 20.10 (ppc64le), the packages were downloaded using the package repository from OpenPower Lab @ Unicamp.\nDependencies The following packages are required:\n Minikube Kubectl Docker-ce Conntrack  You can use the commands below to solve the dependencies:\napt-get update apt-get install docker-ce conntrack minikube kubectl  Optionally, Kubeadm and Kubelet can be installed.","tags":null,"title":"Hello Minikube for ppc64le","type":"post"},{"authors":["Júnior Santos"],"categories":null,"content":"Bazel is a free software tool that allows for the automation of building and testing of software. Similar to build tools like Make, Maven, and Gradle, Bazel builds software applications from source code using a set of rules.\nIt is not officially supported by the Power architecture, because of that, we provide the binary and the possibility to use the package through the Advanced Packaging Tool (APT), and Red Hat Package Manager (RPM).\nIn order for installation via APT or YUM, the user must add our repository to his system. To do this, just do the following steps.\nAdd the repository and install Bazel To add the repository to the system, and to always obtain the new versions of software that we provide, follow these steps.\nAdd APT repository Edit the file: /etc/apt/sources.list\nInsert the line at the end of the file:\ndeb https://oplab9.parqtec.unicamp.br/pub/repository/debian/ ./\nDownload our GPG key, and use the command below to add it to the system:\nsudo apt-key add openpower-gpgkey-public.asc\nAfter that, update the system using the command below:\nsudo apt update\nInstall Bazel To install, use the command:\nsudo apt install bazel\nAdd RPM repository Create and edit the file: /etc/yum.repos.d/open-power.repo\nAdd the text to it:\n[Open-Power] name=Unicamp OpenPower Lab - $basearch baseurl=https://oplab9.parqtec.unicamp.br/pub/repository/rpm/ enabled=1 gpgcheck=0 repo_gpgcheck=1 gpgkey=https://oplab9.parqtec.unicamp.br/pub/key/openpower-gpgkey-public.asc After that, performs the following command with super-user capabilities to update the system.\nyum update\n To be a super user, use the command: sudo su\n Install Bazel To install, it use the command:\nyum install bazel\n To install using other means, see Installing Bazel on Power and Other Unsupported Architectures/Systems.\n ","date":1598227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598227200,"objectID":"352ba6b770f50df47340c6607c777c75","permalink":"https://openpower.ic.unicamp.br/post/installing-bazel-from-repository/","publishdate":"2020-08-24T00:00:00Z","relpermalink":"/post/installing-bazel-from-repository/","section":"post","summary":"Bazel is a free software tool that allows for the automation of building and testing of software. Similar to build tools like Make, Maven, and Gradle, Bazel builds software applications from source code using a set of rules.\nIt is not officially supported by the Power architecture, because of that, we provide the binary and the possibility to use the package through the Advanced Packaging Tool (APT), and Red Hat Package Manager (RPM).","tags":null,"title":"Installing Bazel and other packages from the OpenPower Lab Repository","type":"post"},{"authors":["Júnior Santos"],"categories":null,"content":"Introduction The OpenPower@Unicamp laboratory performs CI/CD activity for several open-source programs that are not available for the Power architecture. Besides, we make the binaries generated from these programs available on our FTP.\n To add our repositories to the system, jump to step 5.\n Problem Many of the programs we make available have frequent updates and can generate several versions within a month. Based on this, several users needed to constantly access FTP to keep their programs up to date.\nAs a result, we decided to create two repositories. A repository for distros which use the Advanced Packaging Tool (APT), and another for distros that use the Red Hat Package Manager (RPM).\nSteps Step 1 - Prerequisites To create a repository, we first need the programs to be hosted on a place with a public IP and that can be accessed by the browser from anywhere in the world.\nFor us, this was a simple matter, our FTP has these characteristics.\nStep 2 - Create directory This information must be accessible to the general public through the browser, so it is necessary to establish where this information will be in the system.\nFTP is a specific path of the server system. Currently, the path /pub/ppc64el is used to store programs binaries. Then we create the path /pub/repository/debian/ to store the APT repository information, and /pub/repository/rpm/ to store the RPM repository information.\n The steps to create an FTP or make a server system path accessible by browsers will not be covered in this guide.\n Step 3 - Add programs As we are developing binaries and programs for a specific architecture, we create a folder with the architecture name. For APT, it is ppc64el, and for RPM, ppc64le.\nThe next step is to add all the programs into the folder with architecture name. As each of our programs has several versions, we created a folder with the name of the program, and all of its versions are inside it.\n To create a program in APT format, you can see this tutorial.\n  To create a program in RPM format, you can see this tutorial.\n Step 4 - Create compatible file systems For the file system to recognize the repository, it is necessary to create some files. These files contain information about the program and their path within the directory. Besides that, they can be signed with a GPG key, which serves to prove the authenticity of the programs present in the repository.\n To create a gpg key, you can see this tutorial.\n Generate essential APT files To create the files, run the following commands inside the folder that will be the path to your directory. In our case in /pub/repository/debian/\ndpkg-scanpackages -m . \u0026gt;\u0026gt; Packages gzip -c Packages \u0026gt;\u0026gt; Packages.gz apt-ftparchive release . \u0026gt; Release keyname=name-your-gpgkey rm -fr Release.gpg; gpg --default-key ${keyname} -abs -o Release.gpg Release rm -fr InRelease; gpg --default-key ${keyname} --clearsign -o InRelease Release Generate essential RPM files To create the files, run the following commands inside the folder that will be the path to your directory. In our case in /pub/repository/rpm/\ncreaterepo --database /pub/repository/rpm/ gpg --detach-sign --armor repodata/repomd.xml Step 5 - Add the repository to the system To add the repository to the system, and always stay updated on the new versions of the software we make available, follow the next steps.\nAdd APT repository Edit the file: /etc/apt/sources.list\nInsert the line at the end of the file:\ndeb https://oplab9.parqtec.unicamp.br/pub/repository/debian/ ./\nDownload our GPG key, and use the command below to add it to the system:\napt-key add openpower-gpgkey-public.asc\nLastly, update the system and install any of the program we provide.\nAdd RPM repository Create and edit the file: /etc/yum.repos.d/open-power.repo\nAdd the text to it:\n[Open-Power] name=Unicamp OpenPower Lab - $basearch baseurl=https://oplab9.parqtec.unicamp.br/pub/repository/rpm/ enabled=1 gpgcheck=0 repo_gpgcheck=1 gpgkey=https://oplab9.parqtec.unicamp.br/pub/key/openpower-gpgkey-public.asc Lastly, update the system and install any of the program we provide.\n","date":1597708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597708800,"objectID":"f5a3a3c72834d22fbc3d853146e1ba87","permalink":"https://openpower.ic.unicamp.br/post/how-create-repository/","publishdate":"2020-08-18T00:00:00Z","relpermalink":"/post/how-create-repository/","section":"post","summary":"Introduction The OpenPower@Unicamp laboratory performs CI/CD activity for several open-source programs that are not available for the Power architecture. Besides, we make the binaries generated from these programs available on our FTP.\n To add our repositories to the system, jump to step 5.\n Problem Many of the programs we make available have frequent updates and can generate several versions within a month. Based on this, several users needed to constantly access FTP to keep their programs up to date.","tags":null,"title":"Creating and adding a Linux repository","type":"post"},{"authors":["Vinicius Couto Espindola"],"categories":null,"content":"Introduction Usually, we let the compiler handle all the “C to Assembly” conversion, but there are certain losses in this automated process: poorly utilized special registers, unnecessary branching and memory accesses, among other issues. These details are insignificant when there are few occurrences during a program’s runtime, however, some sections of code can be executed over a billion times, and, if these sections ignore these details, it can amount to a significant performance overhead. The inline assembly tool in C aims to avert some of these problems by delegating part of the assembling job to the programmer. The tool consists of a special function which receives a raw assembly code section written by the programmer, as well as some register constraints and qualifiers. This tool is mostly used in two scenarios: code optimization and OS/hardware services.\nInline Assembly Basics: Syntax: asm [asm-qualifiers] ( AssemblerTemplate [ : OutputOperands [ : InputOperands [ : Clobbers ] ] ] );  Keyword “asm”: The inline assembly function is called as asm(). There are alternative keywords for “asm” since some compiler options might not recognize it.\nQualifiers: These communicate to the compiler certain behaviors. They tend to be used when the compiler must be informed of something that occurs within the asm code for the program to function properly. The volatile qualifier, for instance, forces the compiler to not remove the asm section during the optimization stage, even if the section produces no outputs (which is quite common when interacting with OS/hardware services).\nAssembler Template: This is the actual assembly code and the only mandatory parameter. Here we can pass one or more strings which will be concatenated and printed into the assembly file (file.s). Due to the concatenation, all instructions must be terminated with “\\n” or “;”. The most common termination used is “\\n\\t” as it ensures the instructions are in separate lines and properly formatted with tabs.\nOutput and Input Operands: Both consist of a list with all the C variables which should be passed to the assembly code. The main difference is that input operands are not meant to be “written to” only “read from”, because the compiler considers that these values will not be altered during the assembly execution. Output operands, on the other hand, are expected to be modified, if they’re not, the compiler may exclude the asm call entirely as it can consider an asm call with no return value useless.\nClobbers List: It is a register list which may have it’s registers altered. Every register in the list will be avoided by the compiler when assigning registers for input/output operands. Useful when instructions implicitly change registers (compare instructions for instance). More about Clobbers here.\nOperands Formats: Output: [ [asmSymbolicName] ] constraint (C_variable_name) Input: [ [asmSymbolicName] ] constraint (C_expression)  Symbolic Names: Every input/output has a symbolic name by default which is defined by the operands index: the first element (indexed by 0) is the leftmost output operand, the last, is the rightmost input operand. The N-th operand can then be referenced with %N, where N is it’s index. A custom symbolic name can be defined in the operands list and reference by using %[asmSymbolicName] within the template.\nConstraints: Every input/output operand must have a constraint which, as the name implies, constrains the contents and location of an operand as well as it’s access. Since the idea is to leave the least amount of work possible for the compiler, using them can be helpful. Constraints can be generic or machine specific, there’s a whole list of these.\nFigure 1 - Input and Output Operands naming conventions.\nNote: You can check out some PowerPC asm examples here.\nMax Element With Inline Assembly As for a practical example, let’s create an asm call that finds the maximum value within an integer array.\nThe Code: int asm_max (int vec[], int size) { int max=0; __asm__( \u0026quot;mtctr %[size]\\n\\t\u0026quot; // Load value to SPR Counter Register \u0026quot;subi %[vec], %[vec], 4\\n\\t\u0026quot; // Load first element and update \u0026quot;loop:\\n\\t\u0026quot; \u0026quot;lu 4, 4(%[vec])\\n\\t\u0026quot; // Load Word and Zeros with update \u0026quot;cmpd 4, %[max]\\n\\t\u0026quot; // Compare \u0026quot;blt skip\\n\\t\u0026quot; // skips update if less than \u0026quot;addi %[max], 4, 0\\n\\t\u0026quot; // Updates maximum \u0026quot;skip:\\n\\t\u0026quot; \u0026quot;bdnz loop\\n\\t\u0026quot; // Branch and Decrement CTR if CTR Not Zero : [max]\u0026quot;+r\u0026quot;(max), [vec]\u0026quot;+r\u0026quot;(vec) : [size]\u0026quot;r\u0026quot;(size) : \u0026quot;ctr\u0026quot;, \u0026quot;cr0\u0026quot;, \u0026quot;r4\u0026quot; ); return max; }  The asm_max function is a wrapper which receives two parameters: an integer array (vec), and it’s size. The asm will be responsible for returning the maximum value within vec and assigning it to the return variable max.\nNote: Isolating the asm call within another function can be helpful to prevent registers from being unintentionally overwritten. Ideally, this should be avoided, as the function call creates an overhead affecting overall performance. The correct use of the operands list should suffice.\nSpecial Registsers Overview: Before we start dissecting the code, let’s take a look at two special purpose registers used (CTR and CR) as well as some specific instructions (mtctr, cmpd, blt and bdnz).\nSpecial Purpose Registers: The Condition Register (CR) holds the results of comparisons. It consists of 8 bitfields (blocks of 4 bits) that can be individually accessed through indexes, also, it can be implicitly altered when the cmp instruction is called. The Counter Register (CTR) is mainly used for loops. Besides being an integer counter, it has special instructions to facilitate assertions (CTR==0? or CTR!=0?) and incrementation/decrementation.\nFigure 2 - Condition register bitfields diagram\nMore About Comparisons: The compare instruction is actually a subtraction that identifies if the result is negative, zero or positive. For example, if we have “cmpd A, B”, the operation realized is “A-B”. If the result is negative, the bit 0 of bitfield X is set. if 0, the bit 1of bitfield X is set. If positive, bit 3 is set. So, if we want to branch when ”A\u0026lt;B” we can use blt (checks if bit 0 from bitfield 0 is set). Note that the order of the registers A and B is paramount.\nMnemonics: Many instructions depend on certain unintuitive parameters to behave as we want them to. The cmp instruction (cmp BF, L, RA, RB), for example, needs parameters to define if it will compare 32 (L=0) or 64 (L=1) bits, and in which bitfield the result of this comparison will be kept (BF=[0-7]). The goal of mnemonics is to facilitate the use of assembly instructions (or sometimes just to shorten them) by adding intuitive synonyms to the instruction set. Example: Let’s suppose we want to compare RA and RB, both with doublewords (64 bits), and save the result in the bitfield 0 of the CR register. The standard instruction to do so would be “cmp 0, 1, RA, RB”, where 0 indicates the bitfield and 1 the fact that we are comparing doublewords. Alternatively, we can use the mnemonic (or synonym) “cmpd RA, RB”, which stands for Compare Doubleword. Note that there’s no bitfield defined in the latter, this is because it implicitly uses the bitfield 0.\nNote: Defaulting to the bitfield 0 or 1 is actually quite common among mnemonics, because we usually only work with one comparison at a time. The bitfield 0 is implicit if it’s an integer comparison, if it’s a floating point comparison, bitfield 1 is implicit.\nSumming it up:  cmpd - Compare Doubleword (and implicitly set result to CR’s bitfield 0) mtctr - Move to counter register (equivalent to mtspr 9, Rx) blt - Branch if Less Than in bitfield 0 (equivalent to bc 12, 0, Label) bdnz - Branch and Decrement if Not Zero (equivalent to bc 16, 0, Label) lu - Load Word and Update (equivalent to lwzu)  The ASM Operands List: : [max]\u0026quot;+r\u0026quot;(max), [vec]\u0026quot;+r\u0026quot;(vec) : [size]\u0026quot;r\u0026quot;(size) : \u0026quot;ctr\u0026quot;, \u0026quot;cr0\u0026quot;, \u0026quot;r4\u0026quot;  First, let’s uderstand what\u0026rsquo;s happening here. We have max and vec as outputs* constrained by “+r” which mark them as read and write (+) and maps them to general purpose registers (r). As for input operands, we have the variable size as read only (it’s the default for input operands) and mapped to a general purpose register as well (r). When defining clobbers, we set the two SPRS discussed previously (CTR and CR) as these are altered during the code’s execution. In particular, CR has the index “0” appended to it, indicating that only the bitfield 0 will be used. The R4 register is also added to the list since it is used as an auxiliary variable in the code and should be avoided by the compiler.\n*Note: In this case, vec is technically not an output, as it shouldn’t be altered. But, since it’s a copy made by the wrapper function, we do not need to worry about modifying its contents and will use it as an index for iterating through the array.\nThe ASM Code: 1 \u0026quot;mtctr %[size]\\n\\t\u0026quot; // Load value to SPR Counter Register 2 \u0026quot;subi %[vec], %[vec], 4\\n\\t\u0026quot; // Load first element and update 3 \u0026quot;loop:\\n\\t\u0026quot; 4 \u0026quot;lu 4, 4(%[vec])\\n\\t\u0026quot; // Load Word and Zeros with update 5 \u0026quot;cmpd 4, %[max]\\n\\t\u0026quot; // Compare 6 \u0026quot;blt skip\\n\\t\u0026quot; // skips update if less than 7 \u0026quot;addi %[max], 4, 0\\n\\t\u0026quot; // Updates maximum 8 \u0026quot;skip:\\n\\t\u0026quot; 9 \u0026quot;bdnz loop\\n\\t\u0026quot; // Branch and Decrement CTR if CTR Not Zero  Finally, let’s understand the code. In the very first line, we set the value of the CTR as the size of the vector, which will make it easier to iterate through the vector. The second line is a bit confusing: since the lu instruction first increments the given address and then access it, we decrement the base address of the array in order to make sure it will start from the first element and not the second (kinda like starting at position -1 and always incrementing before reading). Alternatively we could load the first element in the max variable and exclude the second line, but if so, the code would have to be adjusted to work with arrays of a single element, and the CTR might need to be treated differently depending on how the first element is loaded (if we use lu, the CTR value should be size-1 to not exceed the array’s length). The loop label indicates the start of our iteration through the array. The first step in the loop is to update* our index [vec] and load the next element in R4. Here, the value 4 indicates the amount to add to the current address so we can access the next index*. On line 5 we compare the next element to the current maximum**. If the element is smaller than the maximum, we simply skip the update, otherwise the addi instruction is executed adding 0 to the new maximum and updating [max]’s value. The last instruction in the loop is bdnz, which will first decrement the CTR and then check if it hasn’t reached zero. If CTR is not zero, the loop is executed again, otherwise, the asm terminates.\n*Note: it computes in bytes. Since we have an integer array, which is four bytes per element, to access the next position we must add 4 bytes to the current address [vec].\nConclusion The C Inline assembly tool allows us to quickly integrate assembly code with high level code, which allows for certain optimizations based on the processor\u0026rsquo;s architecture as well as access to specific OS/Hardware services. Despite being useful, it’s a delicate tool which alters the assembly code by the programmer\u0026rsquo;s orders, meaning that, if the parameters are not carefully checked, it could affect the program\u0026rsquo;s cohesion by messing with register allocations. If this is the case, the code’s correctness/consistency would likely be broken rendering it useless.\nAnecdotes on Inline ASM Counter Register and Mnemonics: The major advantage the CTR provides is the ease of executing loops with counters in assembly. This ease is provided mainly by mnemonics which can modify, compare and branch all in single instruction (in our case bdzn). You can check other mnemonics on Appendix C (page 790) of the official Power ISA.\nLabels Duplicates: When talking about the asm call, I commented on the fact that the assembly code is literally concatenated and pasted to the assembly file generated by the compiler. Now suppose your code has multiple sections using asm calls and these sections all have the label loop or skip in it. In this case, when the asm is pasted to the file, there will be multiple labels with the same name and the compiler will point to a conflict of addresses. The asm call allows you to define relative branches, but it’s quite limited: it only works if the labels are numerals, and it can only branch to the first label above or below the branching instruction. To use this tool you can add the suffixes b (backward) or f (forward) to the label’s name when branching. More about it here.\nBranching Prediction: Many architectures implement an optimization called branching prediction. As the name implies, it considers that a certain branch instruction will or won’t be executed and optimizes the code by preprocessing some steps of the branch procedure. This can be relevant when a comparison is executed multiple times in some iteration and its outcome is almost always the same. The prediction can be implied in assembly language by appending the ‘+’ (probably will branch) and ‘-’ (unlikely to branch) symbols to the branching instruction. More about it here.\nMulti-Arch Programs: In case you’re not aware, assembly instructions are architecture dependent (meaning they might not be the same for different processors). In case you’re trying to implement come assembly code for a multi-arch software, you’ll need to be able to identify the machine’s architecture the program is being executed on. For this scenario we can use predefined compiler macros, such as __powerpc64__. With these macros we can chose to compile certain blocks of code using C compiler directives (Ex: #IF DEFINED __powerpc64__) or use an if-else statement to choose between asm blocks.\n","date":1586822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586822400,"objectID":"5812e7fe87494a441f7907a4d34c2504","permalink":"https://openpower.ic.unicamp.br/post/inline-asm-intro/","publishdate":"2020-04-14T00:00:00Z","relpermalink":"/post/inline-asm-intro/","section":"post","summary":"Introduction Usually, we let the compiler handle all the “C to Assembly” conversion, but there are certain losses in this automated process: poorly utilized special registers, unnecessary branching and memory accesses, among other issues. These details are insignificant when there are few occurrences during a program’s runtime, however, some sections of code can be executed over a billion times, and, if these sections ignore these details, it can amount to a significant performance overhead.","tags":null,"title":"Introducing Inline Assembly with PowerPC","type":"post"},{"authors":["Luciano Zago","Vinicius Couto Espindola","Marcelo Martins","Júnior Santos"],"categories":null,"content":"Recently, we’ve upgraded Minicloud’s (a Power architecture based server) Openstack environment to it’s latest version (Openstack Train), and this post aims to tackle some of the issues which we’ve faced.\nThe Minicloud Environment:\nOur server holds multiple machines of two Power architectures: Power8 and Power9 servers. As for our Openstack implementation, we use a Power8 to be the controller and the remaining machines are designated as compute nodes.\nPreparations:\nThere are some details to consider before upgrading, these mainly revolve around softwares and firmwares versions, as well as network architecture.\nAs for firmware, make sure all your machines are up to date with the latest patches, otherwise there can be unforeseen errors even if Openstack is correctly installed. An example of such errors is KVM’s safe cache capability which is not supported on older firmware versions.\nBefore starting, format all bare metal machines to make sure you’re getting a clean install (we’ve used the Ubuntu Server 18.04 as the OS for the server).** Software versions were picked considering Openstack dependencies** and the latest release available for power architecture.\nAs for the network, it won’t be addressed in this post, hence, if necessary, Alta3 Research has a handy playlist addressing this matter.\nNote: avoid running an apt upgrade command after the environment is set, as some packages might break or lose it’s configurations, also, disable automatic package upgrades.Firmware Updates:\nIn case of Power machines, all you’ll need to realize an firmware update is located in IBM’s Fix Central. Simply find the requested hardware info (lshw command should do the job) and search for your machine model. After finding your model, inserting it’s serial number and selecting the latest fix, you will find a page with all software and instruction for the update.\nAdjusting Simultaneous Multithreading:\nOne of the main problems you’ll face with Power8 servers is the Simultaneous Multithreading (SMT) functionality. Essentially, SMT allows a better resource usage, but it can also cause errors. In our case, the SMT was completely disabled in P8 machines and set to 4 in P9 machines.\nWhen running Openstack with SMT enabled on Power8, we dealt with VMs being allocated but remaining in a paused state as they were unable launch due to SMT configurations.\nThe following settings can be used to set a service with systemd which will disable SMT on power machines:\n[Unit] Description=ppc64 set SMT off Before=libvirt-bin.service [Service] Type=oneshot RemainAfterExit=true ExecStart=/usr/sbin/ppc64_cpu --smt=off ExecStop=/usr/sbin/ppc64_cpu --smt=on [Install] WantedBy=multi-user.target  Installation Checklist: Here’s a helpful step by step installation checklist for an environment with multiple node:\n   Steps to Execute Controller Compute     Host networking     Network Time Protocol (NTP)     OpenStack packages     SQL database     Message queue     Memcached     Etcd     keystone     glance     placement     nova     neutron     horizon      Troubleshooting:\nIn this section we’ll share some of the errors we had during installation and the solutions we found to each of them. Note that these errors are not specifically of POWER architecture installations.\nMariaDB note: Some SQL commands were failing due to unknown reasons even with the correct dependencies. A solution we found for this issue was bumping our mariaDB version from 10.2 to 10.4.\nApache \u0026amp; Horizon Login: A small change to Horizon from the previous OpenStack release was the dashboard login page URL settings. Simply using /horizon would redirect to the login page in previous versions. This might require some redirection tweaks in the Apache server configuration file.\nVirtual Interface Exception: When attempting to create a VM, the following error was presented by the Nova module: VirtualInterfaceCreateException: Virtual Interface creation failed.\nTo fix this, wefollowed the instructions from a post in which two lines of configurations are added to the nova.conf file: vif_plugging_is_fatal: false and vif_plugging_timeout: 0.\nGood luck upgrading.\n","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"14c46b432b2fa228b1d02f338dfb7976","permalink":"https://openpower.ic.unicamp.br/post/upgrading-to-openstack-train/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/post/upgrading-to-openstack-train/","section":"post","summary":"Recently, we’ve upgraded Minicloud’s (a Power architecture based server) Openstack environment to it’s latest version (Openstack Train), and this post aims to tackle some of the issues which we’ve faced.\nThe Minicloud Environment:\nOur server holds multiple machines of two Power architectures: Power8 and Power9 servers. As for our Openstack implementation, we use a Power8 to be the controller and the remaining machines are designated as compute nodes.\nPreparations:\nThere are some details to consider before upgrading, these mainly revolve around softwares and firmwares versions, as well as network architecture.","tags":null,"title":"Upgrading to OpenStack Train","type":"post"},{"authors":["Vinicius Couto Espindola","Luciano Zago","Marcelo Martins","Júnior Santos"],"categories":null,"content":"As programmers, we’re fairly used to high level coding and optimization, but we rarely work on lower level languages such as assembly. Even so, understanding these languages is essential for several reasons: optimization, portability, etc. Also, the standard learning languages for assembly tend to be either for Intel’s x86 and/or ARMv7 architectures, leaving aside many others.\nIn this post, we’ll be introducing the Power instruction set architecture (to be precise, the PowerPC 64-bit little-endian architecture) and walking through the initial steps for studying and analysing assembly code in Power. More specifically, the code which we’ll compile and analyse is a C program with a single function which returns one or minus one given a probability (which is passed as a function parameter) using C\u0026rsquo;s standard random number generator.\nCompiling for Power Processors   Using a Power Machine\nThe most simple and straightforward method for obtaining an assembler or binary code for Power architecture is using a Power machine. You can access the Minicloud website and request a free Power VM. Once you’ve setup the VM and installed GCC, all you have to do is compile it.\n Setting Up a Power VM at Minicloud    Using GCC Packages\nTo install a GCC version which can cross compile for power machines we can simply use sudo apt install gcc-7-powerpc64le-linux-gnu. The powerpc64le-linux-gnu suffix is what we call target Here we’re specifying that we want to install GCC v7 for powerpc66le architecture which runs linux-gnu OS. Upon installing the cross compiler we can get the assembly code using powerpc64le-linux-gnu-gcc program.c -S.\n  Overview of the C code #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;time.h\u0026gt; int rand_p(double p) {  double r = (double)rand()/(double)RAND_MAX;   if (r \u0026lt; p) return 1; //Returns with probability p  else return -1; //Returns with probability (1-p) } Understanding the C code is quite trivial. We start with a variable ‘p’ passed as a parameter, and then we instantiate a variable ‘r’ with the casted result of the division of rand() by RAND_MAX. To wrap it up, we test if ‘r’ is smaller than ‘p’. If so, the return value is 1, and -1 otherwise. Should also be noted that the code only works because rand() returns a random value between [0,RAND_MAX] with uniform probability. Now let us see if we can establish a similar analysis but with the assembly code.\nOverview of the Assembly Code Since we’re starting with the basics, we’ll be skipping some lines of code which aren\u0026rsquo;t necessary for grasping the general idea of what’s happening within the program. We can split the code into three main blocks: directives, function call handling and the program logic.\n The directives (Red) assist in guiding the assembly process as well as inserting data. Functions calls demands a series of conventions (Blue) to allow the proper integration of the code within multiple environments. These are defined by the Power’s ABI (Application\u0026rsquo;s Binary Interface), which has a dedicated document for it’s description. The program logic (Black) is where the code we’ve written is translated to the assembler code. This is the section which we’ll be analysing here.  Preliminary Notes Before we can dive in, there\u0026rsquo;s a few concepts which must be known beforehand to fully understand the assembly code:\n  Register Types: There are multiple register types within the Power architecture, the following initials will be used:\n GX stands for General purpose register X. FX stands for Floating point register X. LR and CR refers to Linked Register and Condition Register respectively. These are considered Special Registers.    Special Registers: Some registers have designated functions within the architecture, such as:\n CR which contains 8 adressable fields (with 4 bits each) for saving the result of comparison instructions. LR keeps the return address of a function call when the instruction BL (Branch Linked) is used, and can be used to return to the calling point with the instruction BLR (Branch to Linked Register).    Parameters and Return Registers: The Power ABI defines a set of registers (both GX and FX types) which ares used as variables when returning values or passing parameters to functions. The registers G[3,10] and F[1,13] are such registers. Example: if we have f(int w, int x, float y, double z), the registers G3, G4, F1 and F2 will contain w, x, y and z respectively when f is called.\n  Volatile and Nonvolatile Registers: When a function is called, by the ABI\u0026rsquo;s specifications, nonvolatile registers are presumed to remain intact, meaning that their values either won\u0026rsquo;t change or will be restored by any called function. On the other hand, volatile registers must be saved by the caller if necessary, since these can be altered at will by any called function.\n  Table of Contents (TOC): For now, all we need to know is that RAND_MAX is kept here, and to access it we\u0026rsquo;ll need the address of the table plus an offset. The directives below .LCO: are responsible for defining the offset.\n  Observation: These informations can be found within the Power ISA and Power ABI specifications.\nAnalysing the Assembly Code [...] 7 rand_p: 8 .LCF0: 9 0:\t10 addis 2,12,.TOC.-.LCF0@ha 11 addi 2,2,.TOC.-.LCF0@l [...] 18 stfd 1,40(31) 19 bl rand 20 nop 21 mr 9,3 22 mtvsrd 32,9 23 xscvsxddp 12,32 24 addis 9,2,.LC0@toc@ha 25 addi 9,9,.LC0@toc@l 26 lfd 0,0(9) 27 fdiv 0,12,0 28 stfd 0,56(31) 29 lfd 12,56(31) 30 lfd 0,40(31) 31 fcmpu 7,12,0 32 bnl 7,.L6 33 li 9,1 34 b .L4 35 .L6: 36 li 9,-1 37 .L4: 38 mr 3,9 39\taddi 1,31,80 40\tld 0,16(1) 41\tmtlr 0 42\tld 31,-8(1) 43\tblr [...] First, let’s locate where is the parameter ‘p’. Since ‘p’ is a Float and it’s also the single parameter passed, it’s located at FPR1 (as specified by the ABI).\nThe lines 10 and 11 initialize the TOC base pointer at G2 using the ADDIS and ADD instructions. We\u0026rsquo;ll use this value later for obtaining RAND_MAX from memory. Let’s ignore the .localentry directive that follows.\nAt line 19, the compiler calls the rand() function with the BL instruction, since rand() returns an integer, it’s return value will be placed at G3 (as specified by the ABI) and will be converted to a double at lines 22 and 23 which involves more complicated instructions. Also, in these lines, the value in G3 is transferred to F12. Note that FPR1 is saved at line 17, since FPR1 is a volatile register and can be lost during rand()\u0026rsquo;s execution. The NOP instruction does literally nothing, but it does have a purpose in the bigger picture.\nAt the next step, the compiler will load RAND_MAX. Lines 24 and 25 adds an offset to the TOC pointer (G2) and saves the result at G9. Now, G9 withholds the absolute address of RAND_MAX’s value. To load RAND_MAX’s value, we can use LFD (line 26) using G9 as the offset and setting G0 as RAND_MAX. Note that the LFD instruction interprets the value 0, not as the register G0, but as the number 0, as describes the ISA.\nWe have rand()’s return value and RAND_MAX constant, both at floating point registers, therefore, we can finally divide these values to initialize the variable ‘r’. This division is observed at line 27 by the FDIV instruction, where F12 is divided by F0 and the result saved in F0. In other words, F0 now stores the variable ‘r’ of our C program. In line 28 and 29, the value of F0 is stored and then loaded in F12, probably due to poor optimization.\nSince F1 might have change during rand()’s execution, we must restore F1 with its saved value by loading it from the memory address we saved it in line 18. This can be observed at line 30, where the saved value of our parameter ‘p’ is loaded into F0.\nWe finally have r in F12 and p in F0, meaning that these values can be compared.The instruction FCMPU at line 31 is responsible for comparing F12 with F0 and storing their relation at CR\u0026rsquo;s 7th field.\nAt line 32, the if-else structure is built. First, a BNL instruction checks if F12 is NOT smaller than F0 (by checking CR\u0026rsquo;s 7th field) and, if true, jumps to label .L6 loading -1 into G9, otherwise does not branch and loads 1 at G9. Note that the conditional here (‘r’ \u0026gt;= ‘p’) is the negation of the one present in the C code.\nFinally, we have the function’s return value at G9. To properly end the function call, there are few rules established by the ABI which should be followed, but we won\u0026rsquo;t cover all of them here. For now, we’ll focus on two steps: Moving result from G9 to G3 and loading the return address. The first one is relevant because the caller function will consider that our function’s return value is at G3, therefore, G9 is moved to G3 at line 38 by the MR instruction. The second step ensures that we return to the point where our function was called. For this, we’ll restore the value of the LR register at line 40 using the MTLR instruction.\nTo end our function’s execution, BLR is invoked at line 43 and the function call ends.\nConclusion As short and simple a C program is, when analysed by it’s assembly code, can be quite complex. As seen here, what can be described in a paragraph at high level code, can turn to a long text at low level code (not to mention that we ignored a large portion of the code). The increased complexity is mostly due to the several elements which are omitted for the programmers sake when using high level languages, but this comes at a cost. These instructions can be combined in multiple ways, and the optimal way to do so depends on the program, it’s compilation and the host architecture, resulting in countless combinations which makes the automated optimization process extremely complicated. So overall, understanding such low level code and it’s host architecture is relevant for writing efficient programs.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"0ce81df92e82a280a709aee3911476ac","permalink":"https://openpower.ic.unicamp.br/post/assembly_introduction/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/post/assembly_introduction/","section":"post","summary":"As programmers, we’re fairly used to high level coding and optimization, but we rarely work on lower level languages such as assembly. Even so, understanding these languages is essential for several reasons: optimization, portability, etc. Also, the standard learning languages for assembly tend to be either for Intel’s x86 and/or ARMv7 architectures, leaving aside many others.\nIn this post, we’ll be introducing the Power instruction set architecture (to be precise, the PowerPC 64-bit little-endian architecture) and walking through the initial steps for studying and analysing assembly code in Power.","tags":null,"title":"Introducing Power Architecture's Assembly Language","type":"post"},{"authors":null,"categories":null,"content":"This Privacy Policy describes how we collect, use, and share information. Please read it carefully. This Privacy Policy only applies to the information collected in respect of your account and does not apply to the content that you store on our Services. We will not access, disclose or use your content, except as stated in our Terms of Service.\nInformation Collected by Us at Minicloud   Accounts: Information you provide us when you sign up for our Services, like your name, email address, and purpose of use.\n  Cookies and Access Information: We automatically log information when you interact with us, such as IP address.\n  Usage: System-level metrics such as memory and storage usage and network traffic information.\n  How We Use Information We use the collected information to:\n Provide, operate and improve our Services; Design new services; Communicate with you; Find and prevent fraud; and Enforce our terms of service.  We will not use your data for any purposes unrelated to our Services, like selling it to advertisers.\nHow We Share Information We may share information as discussed below: Your Account usage data (cpu, memory and purpose) with the University of Campinas and with the Unicamp OpenPOWER Lab in order to understand how you use our Services so we can improve it; and With third-party partners providing services for us. We may disclose any of your information if necessary to comply with any laws, governmental request or with your consent.\nModifications to This Policy We may change this policy from time to time and will post or link the most current version on our website. If we materially change it, we will notify you.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://openpower.ic.unicamp.br/privacy/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"This Privacy Policy describes how we collect, use, and share information. Please read it carefully. This Privacy Policy only applies to the information collected in respect of your account and does not apply to the content that you store on our Services. We will not access, disclose or use your content, except as stated in our Terms of Service.\nInformation Collected by Us at Minicloud   Accounts: Information you provide us when you sign up for our Services, like your name, email address, and purpose of use.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"Thank you for using Minicloud. These terms and conditions govern your access and use of our Services. Our Privacy Policy explains how we collect and use your information. By requesting access to Minicloud, you are agreeing to these terms and that we can use your information in accordance with our privacy policy. Please read them carefully.\nYour Responsibilities   You are responsible for all activities that occur under your Account and for keeping your Account secure. You are solely responsible for maintaining the confidentiality and security of your Account. You agree to not share your password or private keys with third parties. Any password we provide to you is temporary and should be changed at your first successful login with it.\n  You agree that you will not transmit, store, promote, distribute or otherwise traffic in content that is illegal, harmful, fraudulent, infringing or offensive.\n  You are responsible for taking appropriate action for preserving and protect the data you store in our Services.\n  You agree to comply with any applicable laws and policies from the University of Campinas; not use the Services to engage in, promote or encourage illegal activity; not probe, scan, or test the vulnerability of any system or network; not distribute malicious code; and not distribute mass email. You shall not interfere or attempt to interfere with the proper working of the Services; breach or otherwise circumvent any security or authentication measures we may use to prevent or restrict access to the Services.\n  You agree to not get or pursue any direct financial benefit from your usage of our Services, including but not limited to cryptocurrency mining.\n  Your Content   We will not access, disclose or use your Content except as necessary to maintain or provide our Services, enforce these Terms of Service, comply with any laws, governmental request or with your authorization.\n  You retain ownership of any intellectual property rights that you hold in the content you store in our services.\n  Warranty Disclaimer The services and content are provided \u0026ldquo;as is\u0026rdquo;, and without warranty of merchantability, fitness for a particular purpose, non-infringement or any kind. we do not warrant that your use of the services will be secure, uninterrupted or error-free. the services are not designed or intended for production activities.\nLimitation of Liability We will not be liable for any direct, indirect, incidental, special, consequential, punitive or exemplary damages; or any loss of use, data, business, or profits, even if a party has been advised of the possibility of such damages.\nTermination You can close your Account and stop using our Services at any time by providing us notice to do so. We also reserve the right to terminate your access to our Services for any reason with or without notice. Upon termination, all the rights granted to you under this Agreement immediately terminate.\nModifications to These Terms We may change these terms from time to time to better reflect changes to the law, to University of Campinas policies or to our Services. We will post or link the most current version on our website. If we materially change it, we will notify you. By continuing to use our Services after any modifications to this Agreement, you agree to be bound by the modified terms. If you do not agree to the revised terms, please discontinue your use of our Services.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://openpower.ic.unicamp.br/terms/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/terms/","section":"","summary":"Thank you for using Minicloud. These terms and conditions govern your access and use of our Services. Our Privacy Policy explains how we collect and use your information. By requesting access to Minicloud, you are agreeing to these terms and that we can use your information in accordance with our privacy policy. Please read them carefully.\nYour Responsibilities   You are responsible for all activities that occur under your Account and for keeping your Account secure.","tags":null,"title":"Terms of Service","type":"page"},{"authors":["Rafael Sene"],"categories":null,"content":"","date":1566219900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566219900,"objectID":"18cdc8193a9be5811a87196590961fd6","permalink":"https://openpower.ic.unicamp.br/talk/power-fingertips/","publishdate":"2019-08-19T13:05:00Z","relpermalink":"/talk/power-fingertips/","section":"talk","summary":"A healthy ecosystem is fundamental for any computer architecture to thrive nowadays and developers are the fuel that keeps it moving and evolving. They need access to resources that can empower them to get the most of their applications and amaze their customers. As the architecture that is the core of the #1 and #2 fastest supercomputers in the world, POWER is where they need to be, but how developers can leverage it as part of their daily activities? This presentation will demonstrate how everyone can get FREE access to the POWER architecture and start building, porting and tuning their applications right away. It will show how to build on POWER using GitHub, Travis CI, and GitLab and how to get FREE access to POWER resources.","tags":[],"title":"The POWER of supercomputers at your fingertips","type":"talk"},{"authors":["Luciano Zago"],"categories":null,"content":"","date":1564939800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564939800,"objectID":"8dc7a6f7f45570de1dbd641e04c24b24","permalink":"https://openpower.ic.unicamp.br/talk/minicloud-linuxdevbr2019/","publishdate":"2019-08-04T17:30:00Z","relpermalink":"/talk/minicloud-linuxdevbr2019/","section":"talk","summary":"In this lightning talk you will know the history behind Minicloud, how students built and maintain a public cloud powered by high-performance machines and how this infrastructure is boosting the research and the open source ecosystem around the POWER processor.","tags":[],"title":"Minicloud","type":"talk"},{"authors":["Gustavo Storti Salibi"],"categories":null,"content":"","date":1564939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564939200,"objectID":"9277d828169504c3c3113319dc54bcd9","permalink":"https://openpower.ic.unicamp.br/talk/power-builds-linuxdevbr2019/","publishdate":"2019-08-04T17:20:00Z","relpermalink":"/talk/power-builds-linuxdevbr2019/","section":"talk","summary":"In this lighting talk Gustavo will show his efforts on porting, building and making available open source projects that wasn't available on Power before. He is going to break-down the components of the infrastructure he is using and how the communities are receiving the support for a new architecture.","tags":[],"title":"Power Builds","type":"talk"},{"authors":["Gustavo Storti Salibi"],"categories":null,"content":"spaCy is an open-source software library for advanced Natural Language Processing, written in Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\nIts installation is very straightforward using the pip package manager. However, you will not succeed if you try to make it into a POWER processor. This is due to a problem with the headers of the Numpy library when using the pip. Thus, the easiest way to install spaCy is by using another package manager, Conda.\nConda is an open source, cross-platform, language-agnostic package manager and environment management system. It is released under the Berkeley Software Distribution License by Continuum Analytics.\n Installing Python 3.7 To install spaCy, you will need to have python 3.7. To verify that you have it installed, simply use the command:\npython3.7 --version If you have not installed it, use the package manager of your system to install.\n Start by updating the packages and installing the prerequisites:  sudo apt update sudo apt install software-properties-common  Add the deadsnakes PPA to your sources list:  sudo add-apt-repository ppa:deadsnakes/ppa  Once the repository is enabled, install Python 3.7 with:  sudo apt install python3.7  Installing Conda 1. Download the Anaconda installer for POWER8 and POWER9.\n2. Enter the following on the download directory:\nbash Anaconda3-2019.03-Linux-ppc64le.sh 3. The installer prompts “In order to continue the installation process, please review the license agreement.” Click Enter to view license terms.\n4. Using Enter, scroll to the bottom of the license terms and enter “Yes” to agree to them.\n5. Click Enter to accept the default install location.\n6. Enter \u0026ldquo;yes\u0026rdquo; to initialize Anaconda3 by running conda init.\n7. Close and open your terminal window for the installation to take effect.\nsource ~/.bashrc.  Installing spaCy You only need to use the following command:\nconda install -c conda-forge spacy  ","date":1558224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558224000,"objectID":"8c658d325fa3c54e047ab91b31f84328","permalink":"https://openpower.ic.unicamp.br/post/installing-spacy-power/","publishdate":"2019-05-19T00:00:00Z","relpermalink":"/post/installing-spacy-power/","section":"post","summary":"spaCy is an open-source software library for advanced Natural Language Processing, written in Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.\nIts installation is very straightforward using the pip package manager. However, you will not succeed if you try to make it into a POWER processor.","tags":null,"title":"Installing spaCy on POWER8 or POWER9.","type":"post"},{"authors":["Igor Matheus Andrade Torrente"],"categories":null,"content":"Introduction and Prerequisites In this tutorial I will show how to use Jupyter in your browser to control scikit-learn running inside a VM. First of all you need build and connect to VM, which is showed in this tutorial.\nSSH connection Now you need connect to VM via ssh using -L 8888:localhost:8888 flag, which will bind your computer port 8888 to port 8888 from VM:\nssh ubuntu@minicloud.parqtec.unicamp.br -i ~/.ssh/your-key.pem -p \u0026lt;vm-port\u0026gt; -L 8888:localhost:8888\nInstalation and executing Now let\u0026rsquo;s update O.S., then install jupyter and sklearn:\n sudo apt update  sudo apt upgrade -y  sudo apt sudo apt install jupyter python3-sklearn python3-pandas -y Initiate jupyter notebook with \u0026amp; flag, which will allow jupyter run in backgroud: Results Open link showed by jupyter on your favorite browser: And now you are ready to use scikit-learn using jupyter remotely.\n","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"b47061e46286a8d739b03b1518264b3b","permalink":"https://openpower.ic.unicamp.br/post/how-use-scikit-learn-and-jupyter-remotely/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/post/how-use-scikit-learn-and-jupyter-remotely/","section":"post","summary":"Introduction and Prerequisites In this tutorial I will show how to use Jupyter in your browser to control scikit-learn running inside a VM. First of all you need build and connect to VM, which is showed in this tutorial.\nSSH connection Now you need connect to VM via ssh using -L 8888:localhost:8888 flag, which will bind your computer port 8888 to port 8888 from VM:\nssh ubuntu@minicloud.parqtec.unicamp.br -i ~/.ssh/your-key.pem -p \u0026lt;vm-port\u0026gt; -L 8888:localhost:8888","tags":null,"title":"How use scikit-learn and Jupyter remotely","type":"post"},{"authors":["Gustavo Storti Salibi"],"categories":null,"content":"Bazel is a free software tool that allows for the automation of building and testing of software. Similar to build tools like Make, Maven, and Gradle, Bazel builds software applications from source code using a set of rules.\nIt uses a human-readable, high-level build language. Bazel supports projects in multiple languages and builds outputs for multiple platforms and supports large codebases across multiple repositories, and large numbers of users.\nIn designing Bazel, emphasis has been placed on build speed, correctness, and reproducibility. The tool uses parallelization to speed up parts of the build process. It includes a Bazel Query language that can be used to analyze build dependencies in complex build graphs\nBazel must have Power support in the future, making its installation possible through community-supported methods. However, currently, if you want to install on Power or other architectures or systems that do not have support, you need compiling Bazel from source.\n Building Bazel from scratch (bootstrapping) Here we will see how to do self-compilation. If you are using Ubuntu 14.04 or Ubuntu 16.04 in ppc64le, you can skip right to: Using ready binaries.\n First, install the prerequisites:\nPkg-config\nZip, Unzip\nG++\nZlib1g-dev\nJDK 8 (you must install version 8 of the JDK. Versions other than 8 are not supported)\nPython (versions 2 and 3 are supported, installing one of them is enough)  # add-apt-repository ppa:openjdk-r/ppa # apt-get update # apt-get install pkg-config zip unzip g++ zlib1g-dev openjdk-8-jdk python  Next, download the Bazel binary installer named bazel--dist.zip from the Bazel releases page on GitHub:   wget https://github.com/bazelbuild/bazel/releases/download/\u0026lt;version\u0026gt;/bazel-\u0026lt;version\u0026gt;-dist.zip There is a single architecture-independent distribution archive. There are no architecture-specific or OS-specific distribution archives.\nYou have to use the distribution archive to bootstrap Bazel. You cannot use a source tree cloned from GitHub (the distribution archive contains generated source files that are required for bootstrapping and are not part of the normal Git source tree).\n Unpack the zip file somewhere on disk:   unzip bazel-\u0026lt;version\u0026gt;-dist.zip  Run the compilation script:   bash ./compile.sh This may take several minutes\u0026hellip;\n Using ready binaries If you are using Ubuntu 14.04 or Ubuntu 16.04 in ppc64le, you can use our already compiled versions of the binaries.\nMake sure you have the JDK 8 installed:\n java -version  If you do not have it, you need to install it:  # add-apt-repository ppa:openjdk-r/ppa # apt-get update # apt-get install openjdk-8-jdk We have released the last 10 versions of Bazel already compiled in this link: https://oplab9.parqtec.unicamp.br/pub/ppc64el/bazel/\n Download the desired version:   wget https://oplab9.parqtec.unicamp.br/pub/ppc64el/bazel/ubuntu_\u0026lt;version\u0026gt;/bazel_bin_\u0026lt;version\u0026gt; # mv bazel_bin_\u0026lt;version\u0026gt; bazel # chmod +x bazel  Installing Bazel Finally, the compiled output is placed into output/bazel (or it is in the current directory if you have downloaded the binary). This is a self-contained Bazel binary, without an embedded JDK. You can copy it anywhere or use it in-place. For convenience we recommend copying this binary to a directory that\u0026rsquo;s on your PATH (such as /usr/local/bin on Linux).\n# mv output/bazel /usr/local/bin or\n# mv bazel /usr/local/bin When using Bazel for the first time, it will extract the installation and prepare everything. To do this, simply use the command:\nFrom now on, Bazel is installed and to use it simply use the command:\n bazel \u0026lt;command\u0026gt; \u0026lt;options\u0026gt;  Using Bazel to compile Bazel Once installed, you can use Bazel itself to compile a new version. To do this, simply download the desired version (as seen in Building Bazel from scratch) or even the developing version on GitHub and use the following command in the directory of the downloaded files:\n Bazel build //src:bazel  References  https://docs.bazel.build/   ","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"7ce4b765968acf6ee9db85db5570ffcc","permalink":"https://openpower.ic.unicamp.br/post/installing-bazel-power-other-architectures-systems/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/installing-bazel-power-other-architectures-systems/","section":"post","summary":"Bazel is a free software tool that allows for the automation of building and testing of software. Similar to build tools like Make, Maven, and Gradle, Bazel builds software applications from source code using a set of rules.\nIt uses a human-readable, high-level build language. Bazel supports projects in multiple languages and builds outputs for multiple platforms and supports large codebases across multiple repositories, and large numbers of users.\nIn designing Bazel, emphasis has been placed on build speed, correctness, and reproducibility.","tags":null,"title":"Installing Bazel on Power and Other Unsupported Architectures/Systems","type":"post"},{"authors":["Igor Matheus Andrade Torrente","Julio Kiyoshi"],"categories":null,"content":"This tutorial I will show how create a openstack image (.qcow2) of opensuse from a ISO image using qemu. In this tutorial will be used opensuse Tumbleweed ppc64 le (because it\u0026rsquo;s the most challenging), but similiar process can be done for leap (15 and 42.3) and Tumbleweed ppc64be.\nPreparing environment First we need download opensuse image from repository (Tumbleweed, leap 15 and leap 42.3) and sha256 of respective image.\nExecute sha256:\nsha256sum openSUSE-Tumbleweed-DVD-ppc64le-Current.iso Compare sha256sum output with sha256 downloaded:\n715d9f89d90eb795b6a64ffe856aa5b7f3a64c7195a9ede8abea14a9d4f69e67 Install qemu using:\nsudo apt update sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system -y Now we need create a disk .qcow2 to install our O.S. with this command:\nqemu-img create -f qcow2 openSUSE-Tumbleweed-ppc64le.qcow2 5G  Update 09/2020: this comand above may cause problem, try this command:\n qemu-img create -f qcow2 openSUSE-Tumbleweed-ppc64le.qcow2 6G Execute qemu to run the instaler:\nsudo qemu-system-ppc64le -enable-kvm -m 1024 -cdrom openSUSE-Tumbleweed-DVD-ppc64le-Current.iso -drive file=openSUSE-Tumbleweed-ppc64le.qcow2,media=disk,if=virtio -nographic -smp cores=1,threads=1 -monitor pty -serial stdio -nodefaults -netdev user,id=enp0s1 -device virtio-net-pci,netdev=enp0s1 -boot order=d  Update 09/2020: this comand above may not work, try this command:\n sudo qemu-system-ppc64le -machine cap-htm=off -m 1024 -cdrom openSUSE-Tumbleweed-DVD-ppc64le-Current.iso -drive file=openSUSE-Tumbleweed-ppc64le.qcow2,media=disk,if=virtio -nographic -smp cores=1,threads=1 -monitor pty -serial stdio -nodefaults -netdev user,id=enp0s1 -device virtio-net-pci,netdev=enp0s1 -boot order=d Installing openSUSE Select your language (using tab and arrows):  Figure 1: Language selection screen\n Select te most suitable bundle for your goal:  Figure 2: Bundle selector screen\n Select expert partitioner:  Figure 3-4: Partioner selection screen\n Select the hard drive that you want install opensuse:  Figure 5: Drive selector screen\n Add new partition selecting add button:  Figure 6: Partition screen\n Set partition size to 8 MiB:  Figure 7: Partition size screen (Boot)\n Select raw partition:  Figure 8: Partition role screen (Boot)\n Select file system as Ext4 (or other filesystem of your preference):  Figure 9: File System type (Boot)\n Select partition as PReP Boot Partition and next:  Figure 10: Partition type (Boot)\n The boot partition was create and now we will create O.S. partition, select add and inside Patition size screen select Maximum Size:  Figure 11: Partition size screen (O.S)\n Select Operating System option:  Figure 12: Partition role screen (O.S)\n Select file system as Ext4 again (or other filesystem of your preference):  Figure 13: File System type (O.S)\n Left selected Linux Native:  Figure 14: Partition type (O.S)\n Left Mount device as / and select next:  Figure 15: Mount point\n Partition configuration will look like this:  Figure 16: Final partion configuration\n We will receive warning message but we can ignore it and select yes:  Figure 17: Warning message\n Next again:  Figure 18: Sumary partition screen\n Select your clock and time zone:  Figure 19: Clock and time zone screen\n Put you username and password:  Figure 20: Local user screen\n Accept instalation and install:  Figure 21: Summary screen\n  Figure 22: Instalation screen\n Preparing image Update all packages and install necessary ones (you can also uninstall unnecessary packages):\nsudo zypper update sudo zypper install cloud-init growpart yast2-network yast2-services-manager acpid Remove hard-coded MAC address:\nsudo cat /dev/null \u0026gt; /etc/udev/rules.d/70-persistent-net.rules Enable ssh and cloud-init:\nsudo systemctl enable cloud-init sudo systemctl enable sshd Disable firewall:\nsudo systemctl stop firewalld sudo systemctl disable firewalld Inside /etc/default/grub file, set grub timeout to 0:\nGRUB_TIMEOUT=0  Figure 23: Grub configuration\n Update grub:\nsudo exec grub2-mkconfig -o /boot/grub2/grub.cfg \u0026#34;$@\u0026#34; Only for openSUSE Tumbleweed Le/Be Opensuse Tumbleweed ppc64 Le/Be lacks some parameters on cloud-init.service, this causes instability on boot, which, sometimes, causes network connection errors. This problem was reported and hopefully will be solved when you read this tutorial.\nEdit cloud-init.service file:\nsudo vim /etc/systemd/system/cloud-init.target.wants/cloud-init.service Add lines bellow after After=systemd-networkd-wait-online.service line:\nRequires=wicked.service After=wicked.service After=dbus.service Conflicts=shutdown.target  Figure 24: Configuration of cloud-init.service\n Reload cloud-init service:\nsudo systemctl restart cloud-init sudo systemctl daemon-reload Because Leap 42.3 ppc64Le\u0026rsquo;s configuration fits better for a cloud role, so we will replace cloud.cfg of Tumbleweed by Leap42.3\u0026rsquo;s cloud.cfg:\nsudo vim /etc/cloud/cloud.cfg  Cleaning image Now delete all remaining data:\ncat /dev/null \u0026gt; ~/.bash_history \u0026amp;\u0026amp; history -c \u0026amp;\u0026amp; sudo su cat /dev/null \u0026gt; /var/log/wtmp cat /dev/null \u0026gt; /var/log/btmp cat /dev/null \u0026gt; /var/log/lastlog cat /dev/null \u0026gt; /var/run/utmp cat /dev/null \u0026gt; /var/log/auth.log cat /dev/null \u0026gt; /var/log/kern.log cat /dev/null \u0026gt; ~/.bash_history \u0026amp;\u0026amp; history -c \u0026amp;\u0026amp; sudo poweroff Adding to openstack And finaly add image to openstack:\nglance image-create --file openSUSE-Tumbleweed-ppc64le.qcow2 --container-format bare --disk-format qcow2 --property hw_video_model=vga --name \u0026#34;openSUSE Tumbleweed ppc64le\u0026#34; If all the steps worked, you should see these messages at the next boot.  Figura 25: Boot\n ","date":1541462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541462400,"objectID":"1c83c29c4f8d43cb9362826bb25f4d58","permalink":"https://openpower.ic.unicamp.br/post/opensuse-tutorial/","publishdate":"2018-11-06T00:00:00Z","relpermalink":"/post/opensuse-tutorial/","section":"post","summary":"This tutorial I will show how create a openstack image (.qcow2) of opensuse from a ISO image using qemu. In this tutorial will be used opensuse Tumbleweed ppc64 le (because it\u0026rsquo;s the most challenging), but similiar process can be done for leap (15 and 42.3) and Tumbleweed ppc64be.\nPreparing environment First we need download opensuse image from repository (Tumbleweed, leap 15 and leap 42.3) and sha256 of respective image.\nExecute sha256:","tags":null,"title":"Building a opensuse openstack image","type":"post"},{"authors":["Luciano Zago"],"categories":null,"content":"In this guide, we will show how to setup a public FTP server with directory access control and disk quota per-user. We used Ubuntu Server 16.04, running on ppc64le architecture, but it should work on other architectures as well, because no exclusive software was used, only open source software.\nDisk space You will need an ext4 partition with enough space, that can be mounted on / or on /var/www. If you need help, look at this tutorial.\nAfter that, create the directories that will be used in the web and ftp servers:\nsudo mkdir /var/www/html sudo mkdir /var/www/html/pub Set the permissions to these directories:\nsudo chown nobody:nogroup /var/www/html sudo chmod a-w /var/www/html HTTP Server (apache) We intend that our files can be accessed through a web browser. In that case, we will need a HTTP Server, like Apache.\nInstallation Install the package apache2, with the following commands:\nsudo apt-get update sudo apt-get install apache2 Restart the service to make sure that the web server works:\nsudo systemctl restart apache2 Content You can create a welcome page in HTML with links to /pub folder, to show the files though the browser. Your page index.html need to be in the directory /var/www/html.\nFor reference, you can look at our web page in this link.\nSSL Certificate (certbot) Certbot is a client that deploy free SSL certificates from Let\u0026rsquo;s Encrypt to any web server. If you already have a SSL certificate, you can skip this part.\nInstallation Run these commands to install the package certbot:\nsudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install python-certbot-apache Configuration We need to configure the web server to work with the certificate. Run this command to use the Certbot certificate with the Apache web server:\nsudo certbot --apache The certificate expires in 90 days, so you need to renew this certificate periodically. To schedule the execution of certobot renew command, we will use cronjob, a time-base job scheduler. To use the scheduler, run this command:\nsudo crontab -e And add the following line in the end of the file:\n0 0 * * * sudo certbot renew Save the file. After that, the renew command is scheduled to run everyday.\nFirewall (ufw) The UFW is an easy frontend interface for iptables. We need to configure the firewall to work with the other installed software.\nInstallation Install the package ufw to manage the firewall, with the following commands:\nsudo apt-get update sudo apt-get install ufw Configuration Forwarding the ports:\nsudo ufw allow 20/tcp sudo ufw allow 21/tcp sudo ufw allow 990/tcp sudo ufw allow 60000:60500/tcp sudo ufw allow ssh sudo ufw allow \u0026#39;Apache Full\u0026#39; sudo ufw status Restart to conclude the steps:\nsudo ufw disable sudo ufw enable FTP Server (vsftpd) We will use the vsftpd software to run the FTP server, the default in the Ubuntu, CentOS, Fedora, NimbleX, Slackware and RHEL Linux distributions.\nInstallation Install the package vsftpd with the following command:\nsudo apt-get install vsftpd Configuration Backup your original file:\nsudo cp /etc/vsftpd.conf /etc/vsftpd.orig Edit the configuration file with the following command:\nsudo nano /etc/vsftpd.conf Example config file:  In the previous config, we allowed read permission for anonymous.\nTo create the userlist that have permission to access the FTP server, and allow the anonymous user, use the following commands:\nsudo touch /etc/vsftpd.userlist sudo echo \u0026#34;anonymous\u0026#34; \u0026gt;\u0026gt; /etc/vsftpd.userlist Disabling shell for ftp users With these commands, we will create a new shell with no functionalities, to restrict the access of the FTP users:\nsudo touch /bin/ftponly sudo echo -e \u0026#39;#!/bin/sh\\necho \u0026#34;This account is limited to FTP access only.\u0026#34;\u0026#39; \u0026gt;\u0026gt; /bin/ftponly sudo chmod a+x /bin/ftponly sudo echo \u0026#34;/bin/ftponly\u0026#34; \u0026gt;\u0026gt; /etc/shells Restart the FTP server service:\nsudo systemctl restart vsftpd Disk Quota We will use a disk quota to limit the disk space used by the FTP users.\nInstallation Install the package quota with the following command:\nsudo apt install quota Configuration Edit the fstab file and add usrquota option in the partition you chose earlier:\nsudo nano /etc/fstab Remount partition and enable the quota:\nsudo mount -o remount /var/www sudo quotacheck -cum /var/www sudo quotaon /var/www Defining a default quota Create a new user to copy the quota settings for the new users:\nsudo adduser ftpuser Insert a password.\nAfter that, you will need to edit the quota of ftpuser with this command:\nsudo edquota ftpuser Put the values of soft and hard quota in these columns.\nExample: 10GB: 10000000 and 10485760 in block quota session.\nLet 0 if you don\u0026rsquo;t want to have a limit.\nSet the default quota user as ftpuser to copy a quota for the new users:\nsudo sed -i -e \u0026#39;s/.*QUOTAUSER=\u0026#34;\u0026#34;.*/QUOTAUSER=\u0026#34;ftpuser\u0026#34;/\u0026#39; /etc/adduser.conf Commands There are a few commands useful for controlling the quota:\n quota user shows the user quota. repquota -a shows the general quota report. edquota user to edit user quota.  Access List (acl) We will use Access List Control, or ACL, to have a better control of file permissions. With ACL we can set different file permissions, in different directories, to each FTP user.\nInstallation Install the package acl with the following command:\nsudo apt install acl Configuration Edit the fstab file and add acl option in the /var/www partition:\nsudo nano /etc/fstab Remount the partition to apply the changes:\nsudo mount -o remount /var/www Commands The commands used to enable write permission to $USER in $DIRECTORY were:\nsetfacl -d -R -m u:$USER:rwX $DIRECTORY setfacl -R -m u:$USER:rwX $DIRECTORY Adding new users We created the following script to manage the creation of new users:  chmod +x create_user.sh Add new users by running the script this way:\nsudo ./create_user.sh \u0026#39;user\u0026#39; \u0026#39;pass\u0026#39; \u0026#39;directory\u0026#39; Directory instructions:\n for the root of FTP directory, use . . for other directories, don\u0026rsquo;t write the initial and final slashes (ex: ppc64el/debian for /www/html/pub/ppc64el/debian/).  Should any problem with file permissions ocurr, use the fix_acl.sh script, that will remake the permissions based on acl.list file.\n Add execute permission to the script:\nchmod +x fix_acl.sh Run the script with sudo, this way:\nsudo ./fix_acl.sh References  https://www.digitalocean.com/community/tutorials/how-to-install-the-apache-web-server-on-ubuntu-16-04 https://www.digitalocean.com/community/tutorials/how-to-setup-a-firewall-with-ufw-on-an-ubuntu-and-debian-cloud-server https://www.digitalocean.com/community/tutorials/how-to-set-up-vsftpd-for-a-user-s-directory-on-ubuntu-16-04  ","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541376000,"objectID":"39eba745a5cfdf797c026425b3a9dfdd","permalink":"https://openpower.ic.unicamp.br/post/ftp-server-setup-with-acl-and-quota/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/post/ftp-server-setup-with-acl-and-quota/","section":"post","summary":"In this guide, we will show how to setup a public FTP server with directory access control and disk quota per-user. We used Ubuntu Server 16.04, running on ppc64le architecture, but it should work on other architectures as well, because no exclusive software was used, only open source software.\nDisk space You will need an ext4 partition with enough space, that can be mounted on / or on /var/www. If you need help, look at this tutorial.","tags":null,"title":"Setting up a FTP Server with Access List and Disk Quota","type":"post"},{"authors":["Nathalia Harumi Kuromiya","Júnior Santos","Matheus Fernandes"],"categories":null,"content":"TensorFlow is a widespread software library for numerical computation using data flow graphs. It is very common on machine learning and deep neural networks projects. Therefore, today we are going to see how to install it on POWER with CPU only configuration.\nUpdate 03/2021 If you only want to install TensorFlow on POWER (and not build it), there is an easier way which is taught in the following tutorial: https://openpower.ic.unicamp.br/post/installing-tensorflow-on-power\nOUTDATED :  OUTDATED 09/2020 The following tutorial is an outdated way of building TensorFlow on Power. If you still want to build TensorFlow from source by following this tutorial, proceed with caution.\n Before installing TensorFlow, there are a couple of details we have to pay attention to:\n Due to Bazel, one of TF dependencies, the operating system must be Ubuntu 14.04 or Ubuntu 16.04. We are going to use Python 2.7, since TF doesn\u0026rsquo;t seem to be supported by Python 3.5 on POWER.  Tensorflow Dependencies You can use the commands below to solve most of the dependencies:\napt-get update apt-get install python-numpy python-dev python-pip python-wheel Bazel installation Bazel is one of the TF dependencies, but its installation is less intuitive than the others due to its community not officially supporting POWER architecture. That said, we must compile it from the Source. First of all, we need to install its own dependencies by the following commands:\napt-get update apt-get install unzip build-essential python openjdk-8-jdk protobuf-compiler zip g++ zlib1g-dev It is also important to add enviroment variables on .bashrc for JDK.\nvi .bashrc \texport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-ppc64el \texport JRE_HOME=${JAVA_HOME}/jre \texport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib \texport PATH=${JAVA_HOME}/bin:$PATH For compiling Bazel, we are going to download and unpack its distribution archive (the zip file from the release page https://github.com/bazelbuild/bazel/releases. The .sh is not compatible with ppc64le) and compile it.\nmkdir bazel cd bazel wget -c https://github.com/bazelbuild/bazel/releases/download/0.11.1/bazel-0.11.1-dist.zip unzip bazel-0.11.1-dist.zip ./compile.sh  if you want to download other version of bazel, this link must be switched by the one you are intenting to use.\n  Update 09/2020: It is also possible to perform the installation by following this tutorial.\n As we can see, this tutorial was tested with bazel 0.11.1, but feel free to try other version and see if it works properly.\nAlso, if you are having any trouble about lack of resources, you can take a look on \u0026lsquo;Build issues and Support Websites\u0026rsquo; to see if there\u0026rsquo;s any link that could help you. Anticipating: if you don\u0026rsquo;t have memory enough and your Bazel can\u0026rsquo;t complete the compile step, you might have a problem with the garbage collector of JAVA (and there\u0026rsquo;s a link which explains how to deal with it).\nInstalling Tensorflow Since we are going to use the current version of TF, we need to clone it from the official GitHub and execute the configuration script.\ngit clone https://github.com/tensorflow/tensorflow cd ~/tensorflow ./configure On this step, we have to specify the pathname of all relevant TF dependencies and other build configuration options. On most of them we can use the answers suggested on each question. Here, I will show how it was done for this tutorial. (Yours might be a little different, depending on the pathnames)\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7 Found possible Python library paths:  /usr/local/lib/python2.7/dist-packages  /usr/lib/python2.7/dist-packages Please input the desired Python library path to use. Default is [/usr/lib/python2.7/dist-packages]: /usr/lib/python2.7/dist-packages  Using python library path: /usr/local/lib/python2.7/dist-packages  #Y/N Answers given: All of them as suggested in each question.  Please specify optimization flags to use during compilation when bazel option \u0026#34;--config=opt\u0026#34; is specified [Default is -march=native]: -mcpu=native Configuration finished To build and install TF, we use:\nbazel build --copt=\u0026#34;-mcpu=native\u0026#34; --jobs 1 --local_resources 2048,0.5,1.0 //tensorflow/tools/pip_package:build_pip_package bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg #creates the pip package pip install /tmp/tensorflow_pkg/tensorflow-1.5.0rc0-cp27-cp27mu-linux_ppc64le.whl #installs the pip package.  This name depends on your operating system, Python version and CPU only vs. GPU support. Therefore, check it out its name before this step.\n By this moment, your TF must be working. Remember not to import it into its own directory: you have to chance directory before executing Python.\nBuild Issues and Support Websites: While testing this tutorial, I could separate some useful issues reports and links to help some of the troubles you might have on the way.\n https://github.com/tensorflow/tensorflow/issues/14540 It solves a protobuf problem I had. It seems pretty common on PPC TF installation. https://github.com/tensorflow/tensorflow/issues/349 This one is about local resources. If you are running out of memory (your build fails on C++ compilation rules), you have to specify your resources on the command line when you build TF. On the tutorial, it is already done. https://www.tensorflow.org/install/install_sources An official tutorial about how to install TF from Sources https://docs.bazel.build/versions/master/install-compile-source.html An official tutorial about how to install Bazel from Sources. https://www.ibm.com/developerworks/community/blogs/fe313521-2e95-46f2-817d-44a4f27eba32/entry/Building_TensorFlow_on_OpenPOWER_Linux_Systems?lang=en IBM source about Tensorflow installation. Provides interesting information about bazel installation on PPC and how to install TF with GPU support. It also points to an IBM Bazel modified to PPC (which we are not using in this tutorial, but you can take a look on it). https://github.com/tensorflow/tensorflow/issues/7979#issuecomment-283559640 An issue about enviroment variables: on the configuration step, if it does not recognize some of the TF variables, this might help you to solve the problem. https://github.com/bazelbuild/bazel/issues/1308 An issue about Bazel: \u0026ldquo;The system is out of resources\u0026rdquo;. You might need to add a command line on compile file to change the garbage collector size. On the issue on git, it\u0026rsquo;s suggested to change it to 384, but, at least on one of the computers I tried to compile, I needed to change it to 512 (in other words, change -J-Xmx384m on the solution line to -J-Xmx512m). It\u0026rsquo;s important to see that ideally we should not have to change the source code, but it solves the problem. Another option is to increase the memory of your system if it\u0026rsquo;s possible (recommended).  ","date":1516060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516060800,"objectID":"ce4dfda09ff2e011d52e3e372ac026b9","permalink":"https://openpower.ic.unicamp.br/post/building-tensorflow-on-power/","publishdate":"2018-01-16T00:00:00Z","relpermalink":"/post/building-tensorflow-on-power/","section":"post","summary":"TensorFlow is a widespread software library for numerical computation using data flow graphs. It is very common on machine learning and deep neural networks projects. Therefore, today we are going to see how to install it on POWER with CPU only configuration.\nUpdate 03/2021 If you only want to install TensorFlow on POWER (and not build it), there is an easier way which is taught in the following tutorial: https://openpower.ic.unicamp.br/post/installing-tensorflow-on-power","tags":null,"title":"Building Tensorflow on POWER - CPU only","type":"post"},{"authors":["Guilherme Tiaki Sato"],"categories":null,"content":"Deploying HTTPS is essential for security, and OpenStack Ansible does it by default. However, if no certificates are provided, it will generate self-signed ones, which although are more secure than no SSL at all, it will trigger a warning when accessing the dashboard in the browser. Luckily, the Let’s Encrypt project provides signed SSL certificates for free.\nLet’s Encrypt requires your server to be validated before issuing the certificate. This means it will create a temporary file on your server and then try to access it from their servers, to verify that you control the domain you\u0026rsquo;re trying to get a certificate to.\nIt can launch a temporary web server to do so, however, this will require to stop your usual web server (e.g. Apache) and lead to a few seconds of downtime. Alternatively, you can provide your web root path. Let’s Encrypt will create the files there, and they will be served directly by your usual web server. This approach does not lead to downtime, but presents some additional challenges when using it with OpenStack Ansible:\n OpenStack Ansible does not have a web root path out of the box to be used by Let’s Encrypt. SSL certificates must be provided to HAProxy, which runs on metal, while the Apache server to be used by Let’s Encrypt runs inside a container.  Installing OpenStack Ansible Install OpenStack as usual, without providing any certificates. Self-signed ones will be therefore generated, and we will replace them later.\nEnable web root We will not actually create a web root. Since Let’s Encrypt only requires writing on your-domain.com/.well-known directory, we will create an alias to the .well-known path.\nAttach to the horizon container. Replace the container name accordingly with your setup. If you don’t know the name, run lxc-ls | grep horizon to get the container name.\nlxc-attach -n infra1_horizon_container-XXXXXXXX Add the following line to /etc/apache2/sites-enabled/openstack-dashboard.conf, inside the \u0026lt;VirtualHost *:80\u0026gt; tag\nAlias /.well-known /var/www/html/.well-known\r Restart the apache2 service:\nservice apache2 restart Now, we can use /var/www/html as our web root, at least from the Let’s Encrypt Certbot point of view.\nGetting the certificates Now install the Let’s Encrypt Certbot. The intention is to only get the certificates files, not configure them in Apache. Use the following commands to do so:\napt-get update apt-get install software-properties-common add-apt-repository ppa:certbot/certbot apt-get update apt-get install certbot certbot certonly When asked to choose an authentication method, choose 2\nHow would you like to authenticate with the ACME CA? ------------------------------------------------------------------------------- 1: Spin up a temporary webserver (standalone) 2: Place files in webroot directory (webroot) ------------------------------------------------------------------------------- Select the appropriate number [1-2] then [enter] (press \u0026#39;c\u0026#39; to cancel): 2 When asked for the webroot, input /var/www/html\nSelect the webroot for your-domain.com: ------------------------------------------------------------------------------- 1: Enter a new webroot ------------------------------------------------------------------------------- Press 1 [enter] to confirm the selection (press \u0026#39;c\u0026#39; to cancel): 1 Input the webroot for unicamp.br: (Enter \u0026#39;c\u0026#39; to cancel): /var/www/html After this, the certificate files will be placed on /etc/letsencrypt/live/your-domain.com\nAllow the container to copy files to the host Generate an SSH key inside the container:\nssh-keygen -t rsa Print the public key and copy it to the clipboard:\ncat /root/.ssh/id_rsa.pub Now append the container\u0026rsquo;s public key to the authorized_keys file in the host:\necho [PASTE THE COPIED KEY HERE] \u0026gt;\u0026gt; /root/.ssh/authorized_keys This will allow the container to copy the certificates to the host using scp.\nApplying the certificates in HAProxy To use them in HAProxy, we must concatenate some files. Replace your-domain.com accordingly.\ncat /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.key cat /etc/letsencrypt/live/your-domain.com/cert.pem /etc/letsencrypt/live/your-domain.com/chain.pem /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.pem Set the permissions properly:\nchmod 640 /etc/letsencrypt/live/your-domain.com/haproxy.key chmod 644 /etc/letsencrypt/live/your-domain.com/haproxy.pem Still inside the horizon container, copy the files we just generated to the host. Replace your-domain.com and HOST_IP_ADDRESS accordingly.\nscp /etc/letsencrypt/live/your-domain.com/haproxy.* HOST_IP_ADDRESS:/etc/ssl/private Now exit the container and apply the new certificate:\nservice haproxy reload Renewing the certificates automatically As Let’s Encrypt certificates are only valid for 90 days, it is highly advisable to schedule automatic renewing. We can do this using crontab inside the horizon container.\nAttach to the horizon container. Replace the container name accordingly with your setup. If you don’t know the name, run lxc-ls | grep horizon to get the container name.\nlxc-attach -n infra1_horizon_container-XXXXXXXX Open the crontab editor:\ncrontab -e Place this line at the end of the file, replacing your-domain.com and HOST_IP_ADDRESS accordingly.\n26 3 * * 5 certbot renew \u0026amp;\u0026amp; cat /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.key \u0026amp;\u0026amp; cat /etc/letsencrypt/live/your-domain.com/cert.pem /etc/letsencrypt/live/your-domain.com/chain.pem /etc/letsencrypt/live/your-domain.com/privkey.pem \u0026gt; /etc/letsencrypt/live/your-domain.com/haproxy.pem \u0026amp;\u0026amp; scp /etc/letsencrypt/live/your-domain.com/haproxy.* HOST_IP_ADDRESS:/etc/ssl/private \u0026amp;\u0026amp; ssh HOST_IP_ADDRESS service haproxy reload\r This will run every week, but it will only actually renew the certificate at most every 60 days, as only certificates that expire in less than 30 days are renewed. Running it more often than every 60 days makes it safer, as even if it fails to run once after the 60 days window, it will still run again before the certificate expire.\n","date":1513468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513468800,"objectID":"c7bb686b2296c01f591a5dcd4aa917ca","permalink":"https://openpower.ic.unicamp.br/post/integrating-openstack-ansible-with-lets-encrypt/","publishdate":"2017-12-17T00:00:00Z","relpermalink":"/post/integrating-openstack-ansible-with-lets-encrypt/","section":"post","summary":"Deploying HTTPS is essential for security, and OpenStack Ansible does it by default. However, if no certificates are provided, it will generate self-signed ones, which although are more secure than no SSL at all, it will trigger a warning when accessing the dashboard in the browser. Luckily, the Let’s Encrypt project provides signed SSL certificates for free.\nLet’s Encrypt requires your server to be validated before issuing the certificate. This means it will create a temporary file on your server and then try to access it from their servers, to verify that you control the domain you\u0026rsquo;re trying to get a certificate to.","tags":null,"title":"Integrating OpenStack Ansible with Let’s Encrypt","type":"post"},{"authors":["Igor Matheus Andrade Torrente"],"categories":null,"content":"Buildbot its tool to automate compilation and tests. This tutorial we will install it on three important distro and run it.\n1 - Installation dependencies:  Installation of necessary packages to correct installation of Buildbot bundle.  Fedora 25: sudo dnf install python-devel python-pip redhat-rpm-config make gcc Fedora 26: sudo dnf install python-pip redhat-rpm-config make gcc Ubuntu and Debian: sudo apt-get install Python-dev build-essential python-pip 2 - Installation without virtualenv: sudo pip install --upgrade pip sudo pip install \u0026#39;buildbot[bundle]\u0026#39; sudo pip install buildbot-grid-view sudo pip install buildbot-worker sudo pip install setuptools-trial 2.5 - Installation with virtualenv (optional):  First we need install virtual environment.  Fedora: sudo dnf install python-virtualenv Ubuntu and Debian: sudo apt install python-virtualenv  Now we need activate the environment  virtualenv --no-site-packages YourSandbox source YourSandbox/bin/activate Instalation: pip install --upgrade pip pip install \u0026#39;buildbot[bundle]\u0026#39; pip install buildbot-grid-view pip install buildbot-worker pip install setuptools-trial 3- Initial Master setup:  Creation of folder where Buildbot archives will stay:  mkdir -p BuildBot cd BuildBot  Creation of Master with name[master]:  buildbot create-master master  The configuration of all functions of Buildbot its done in configuration file inside Master folder, to simplify we will use the sample configuration file provided in default template of Master [master.cfg.sample], but it’s needed be renamed to [master.cfg] to be recognized by Buildbot:  mv master/master.cfg.sample master/master.cfg  Here we will start Master daemon:  buildbot start master 4- Initial Worker setup:  Here we will create a worker (previously slave) with name [worker]:  buildbot-worker create-worker worker localhost example-worker pass The command syntax: * buildbot-worker = Buildbot Worker program. * create-worker = Command to creation of Worker * worker = Name of worker folder * localhost = Master location on the network (This example Master are in the same VM) * example-worker = Name of worker * pass = Authentication password   Start of worker daemon:   buildbot-worker start worker  Access address http://localhost:8010/ on your browser.  ","date":1506038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506038400,"objectID":"b6f6a56cfa4531b7f3f4bc280efe11eb","permalink":"https://openpower.ic.unicamp.br/post/buildbot-tutorial/","publishdate":"2017-09-22T00:00:00Z","relpermalink":"/post/buildbot-tutorial/","section":"post","summary":"Buildbot its tool to automate compilation and tests. This tutorial we will install it on three important distro and run it.\n1 - Installation dependencies:  Installation of necessary packages to correct installation of Buildbot bundle.  Fedora 25: sudo dnf install python-devel python-pip redhat-rpm-config make gcc Fedora 26: sudo dnf install python-pip redhat-rpm-config make gcc Ubuntu and Debian: sudo apt-get install Python-dev build-essential python-pip 2 - Installation without virtualenv: sudo pip install --upgrade pip sudo pip install \u0026#39;buildbot[bundle]\u0026#39; sudo pip install buildbot-grid-view sudo pip install buildbot-worker sudo pip install setuptools-trial 2.","tags":null,"title":"Install and initial configuration Buildbot on Fedora, Ubuntu and Debian","type":"post"},{"authors":null,"categories":null,"content":"Introduction to OpenCL2CUDA We all know that software is replacing many people functions. Said that, many very complicated data processing are now made by computers, that are getting better and better ways to do those tasks. There are many libraries and frameworks that helps programmers and engineers writing code to process some big amount of data. Two of these well known libraries are OpenCL, developed for heterogeneous computing (gpu, processors, fpga), and CUDA, a NVIDIA library created so people can write code to run on their GPUs. These libraries have some similar routines, cause there are many steps you have to do on both of them. Thinking about it, I started writing a OpenCL to CUDA converter.\nImplementation The implementation of this code still really simple, since all I am doing is searching for some OpenCL functions and replacing it for its equivalent on CUDA. Besides, if its not a direct translation, the converter suggests some possible fixes for you code. To find the suggestions on your code search for the #tranlation# word. We are using Python 3. To run this code all you have to do is:\nchmod +x createCUDAkernel.py (just the first time)\r./createCUDAkernel.py --opencl_name=\u0026#34;name of the opencl file\u0026#34; --main_name=\u0026#34;name of the C/C++ file\u0026#34; How to contribute Now, I am searching for CUDA and OpenCL codes that do the same thing, so I can go on this project. Besides, you can fork the project on Github.\nThanks a lot.\n","date":1496793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496793600,"objectID":"0be4d300e6a82bc2771254f3a2c8f189","permalink":"https://openpower.ic.unicamp.br/post/opencl2cuda/","publishdate":"2017-06-07T00:00:00Z","relpermalink":"/post/opencl2cuda/","section":"post","summary":"Introduction to OpenCL2CUDA We all know that software is replacing many people functions. Said that, many very complicated data processing are now made by computers, that are getting better and better ways to do those tasks. There are many libraries and frameworks that helps programmers and engineers writing code to process some big amount of data. Two of these well known libraries are OpenCL, developed for heterogeneous computing (gpu, processors, fpga), and CUDA, a NVIDIA library created so people can write code to run on their GPUs.","tags":null,"title":"Introduction to OpenCL2CUDA","type":"post"},{"authors":null,"categories":null,"content":"Introduction to PowerGraph As computers evolve, people are trying new methods do analyse how good some machine is when compared to another one. The amount of energy that some machine is consuming, seems to be a nice measure, once we are willing to produce faster and cheaper. IPMI(Intelligent Platform Management Interface), on the other side, is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system. One of the measures that IPMI allow us to do is:\ndcmi power reading This command shows the instant power consumption. With this tools, my team decided to create a software that gets informations about the consumption of a machine and exports it on a readable way. Thats how we did it:\nInfrastructure The infrastructure was not so complicated to configure. We have to download some packages and set some configurations (process we are automating with Ansible). All the system is deployed on Minicloud. The packages (all via apt) we are using are:\n ipmitool apache2 python2.7 python-pip git htop tmux  Besides, we are using crontab to be sure our service is still runing, and if it is not, restart it. We are doing this verification every ten minutes with the killer.sh script. To solve all Python dependencies you can run:\npip install -r requirements.txt Apache configurations Here you will install two modules in your apache server and change the virtual host configuration file. Doing this you will be able to control your browser\u0026rsquo;s cache.\nIn your server terminal run:\nsudo a2enmod headers\rsudo a2enmod expires\rsudo service apache2 restart After that, find your virtual host configuration file (/etc/apache2/sites-available/default/000-default.conf if you are using ubuntu OS) and insert the following lines, adjusting the parameters according to your needs:\n\u0026lt;Directory /var/www/html\u0026gt;\rExpiresActive On\rExpiresDefault \u0026#34;access plus 10 minutes\u0026#34;\rExpiresByType text/html \u0026#34;access plus 1 day\u0026#34;\rExpiresByType text/javascript \u0026#34;access plus 1 day\u0026#34;\r# if it is your interest, you can set a specific expiration time for your csv file\r# ExpiresByType text/csv \u0026#34;access plus 30 seconds\u0026#34;\r\u0026lt;FilesMatch \u0026#34;file.csv\u0026#34;\u0026gt;\rHeader set Cache-control \u0026#34;no-cache\u0026#34;\r\u0026lt;/FilesMatch\u0026gt;\r\u0026lt;/Directory\u0026gt; Back-end code PowerGraph was totally developed using Python. There are three main codes:\n powergraph.py graph_csv.py csvcreator.py  Below, I will explain each code and its function.\npowergraph.py This is the code that keeps getting power info about a machine and save it to tinyDB or prompt the result to user. You can run it using the command:\npython2.7 powergraph.py --host=\u0026#34;server address\u0026#34; --port=\u0026#34;server port\u0026#34; --user=\u0026#34;allowed user\u0026#34; --passwd=\u0026#34;password for this user\u0026#34; You can use the optional parameter --store in order to save the infos as json on tinydb. Without this parameter, the script will print on the terminal. Besides, you can use --feedback with store in order to see the measures status. If you want to set the time interval that a new csv file is generated, you can use the flag --csv_interval. The --tail_length is used to set the number of lines the csv file will have.\ncsvcreator.py This is the code that converts the JSON stored on tinyDB for a csv file. This code is really important, cause our front end is expecting a csv with two columns: timestamp and consumption. In order to run this code, you have to store the data of powergraph.py on the database, as we explained before. To run this use:\npython2.7 csvcreator.py --jsonfile=\u0026#34;generated_json_name\u0026#34; There are two optional arguments: --date, to create the csv only with the data from a specific day and --name, with the name you want your csv file.\ngraph_csv.py This is the code that orchestrates the other ones. It is a multithread code that creates one thread always running with the powergraph.py code and another one generating a new thread with csvcreator running from time to time updating the measures. To run this code use:\npython2.7 graph_csv.py --host=\u0026#34;server address\u0026#34; --port=\u0026#34;server port\u0026#34; --user=\u0026#34;allowed user\u0026#34; --passwd=\u0026#34;password for this user\u0026#34; --jsonfile=\u0026#34;path to bd jsonfile\u0026#34; Besides, you can use the following optional arguments:\n interval: interval between each ipmi measure (default=10) nread: number of ipmi measures to be done (default=infinity) csv_interval: interval that a new csv file is made (deafult=300s) tail_length: size of the csv files (default=300)  Front-end code In order to create an interactive website that plot a graph from the csv file, you first need to deal with the following dependencies:\n apache configurations javascript libraries  After that, we have developed the code in javascript/graph.js where you can read and present in real time the data provided by the back-end.\nJavascript libraries Here we have three libraries included in our html file (index.html).\nThe first one is the D3.js library, a worldwide known tool to create dynamic and interactive data visualizations, in other words, this is the engine of the website. Insert in your html body the \u0026lt;script src=\u0026quot;http://d3js.org/d3.v4.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; to get the most recent code from D3.js.\nThe next library, Moment.js, is used to manipulate date objects and in our case, for example, it allows us to show the time adjusted to the user\u0026rsquo;s location. You can download the code from the following address https://momentjs.com/downloads/moment.min.js.\nFinally, the D3-tip library just inserts tooltips in the graph for a better experience of use. This library was donwloaded from https://github.com/Caged/d3-tip/blob/master/index.js. It is also interesting that you take a look our style implementation for the tooltips from the file style/style.css.\nConclusion If you want to see the project working, acces this link. If you want to contribute, acces the github link. Hope you all enjoy it. Thanks a lot!\n","date":1496102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496102400,"objectID":"835cb41e8d3555711d67d426de1c3537","permalink":"https://openpower.ic.unicamp.br/post/powergraph/","publishdate":"2017-05-30T00:00:00Z","relpermalink":"/post/powergraph/","section":"post","summary":"Introduction to PowerGraph As computers evolve, people are trying new methods do analyse how good some machine is when compared to another one. The amount of energy that some machine is consuming, seems to be a nice measure, once we are willing to produce faster and cheaper. IPMI(Intelligent Platform Management Interface), on the other side, is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system.","tags":null,"title":"Introduction to PowerGraph","type":"post"},{"authors":[],"categories":null,"content":"","date":1494525600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494525600,"objectID":"428526ea820c46e3ce272cc4d7ac8e15","permalink":"https://openpower.ic.unicamp.br/talk/power9/","publishdate":"2017-05-11T18:00:00Z","relpermalink":"/talk/power9/","section":"talk","summary":"Leonardo Garcia é Engenheiro de Computação e mestre em Ciência da Computação pela Universidade Estadual de Campinas (Unicamp) e trabalha no Linux Technology Center da IBM há 11 anos. Ele tem liderado o time de desenvolvimento de KVM para Power há 4 anos.","tags":[],"title":"POWER9: a nova geração de processadores POWER (ou processador para a Era Cognitiva)","type":"talk"},{"authors":null,"categories":null,"content":"#Acessing a Docker Container outside Minicloud Docker containers are widely used nowadays for making software development and delivery easier, since it isolates the container from the rest of the system. This is very useful, cause the developer can install any software, depencies to run the project perfectly, delivering the \u0026ldquo;whole package\u0026rdquo; to anyone who wants to run it. Some applications are expected to access or be accessed from outside, like a webserver, Jenkins, and so on. To do it, you have to map a container port with a server port.\nMaping a Container port with a server port Is very simple and useful to do it. All you have to do is to include a parameter on command line when launching a container with docker run, like this example running a Jenkins container:\ndocker run -i -t -p \u0026#34;physical machine port\u0026#34;:\u0026#34;container port\u0026#34; guilhermeslucas/jenkins:2.0 /bin/bash In this example Jenkins container is running through port \u0026ldquo;container port\u0026rdquo; and you can access it by reaching the \u0026ldquo;physical machine port\u0026rdquo; of the server.\nAcessing Docker Container from a local browser Some applications are configured or managed using the browser. In this case, you can run the application on a server, but configure it using a ssh tunnel on your local machine. This is very simple too, just add a parameter on the ssh command line, mapping it correctly, like the example.\nssh user@host -L local-port:host:remote-port it can be used like:\nssh guilherme@123.456.78.910 -L 8080:localhost:8080 In this example, the 8080 remote port will be forward to localhost:8080 and you can access it via browser.\nSo, you\u0026rsquo;ll have to map a container port with a server port and forward this server port on your localhost, on any port not in use.\nThis should do the work.\nWritten by Guilherme Lucas. You can see some of my work at my Github Page.\n","date":1489449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489449600,"objectID":"870fb59ab447475fd2b48d0d90ddaa51","permalink":"https://openpower.ic.unicamp.br/post/external_docker/","publishdate":"2017-03-14T00:00:00Z","relpermalink":"/post/external_docker/","section":"post","summary":"#Acessing a Docker Container outside Minicloud Docker containers are widely used nowadays for making software development and delivery easier, since it isolates the container from the rest of the system. This is very useful, cause the developer can install any software, depencies to run the project perfectly, delivering the \u0026ldquo;whole package\u0026rdquo; to anyone who wants to run it. Some applications are expected to access or be accessed from outside, like a webserver, Jenkins, and so on.","tags":null,"title":"Acessing a Docker Container outside Minicloud","type":"post"},{"authors":[],"categories":null,"content":"","date":1480615200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480615200,"objectID":"72f180f70935ca4f0c35132a949512f1","permalink":"https://openpower.ic.unicamp.br/talk/hhvm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/hhvm/","section":"talk","summary":"Atualmente, paginas da internet são combinações de conteúdos gerados dinamicamente no servidor e programas que fornecem interatividade do lado cliente, tornando-se cada vez mais complexas. Nesse contexto, linguagens dinâmicas para a web como o PHP, são amplamente utilizadas. Entretanto, a medida que a complexidade cresce, ao mesmo tempo, cresce a necessidade de obter-se um desempenho cada vez maior e algumas dessas linguagens não são capazes de tirar o melhor proveito na execução de código nativo pois foram desenvolvidas em uma época onde a maior parte do contéudo para web era estático. Para obter um melhor desempenho tecnicas mais avançadas como JIT (Just In Time compilation) têm sido utilizadas. Nessa palestra iremos explorar os detalhes internos da HipHop Virtual Machine (HHVM) uma maquina virtual de processo baseada em JIT que serve para executar código escrito em PHP e Hack e que permite melhorar o desempenho na execução de programas escritos nessas linguagens. **About the speaker:** Rogerio Alves é mestre em Ciência da Computação com mais de 5 anos de experiência com desenvolvimento de aplicações de baixo nível, além de experiência com linguagens para web. Atualmente, trabalha no Linux Technology Center da IBM com compiladores JIT portando o HHVM para a arquitetura POWER.","tags":[],"title":"HHVM - Uma breve introdução a compiladores JIT para linguagens dinâmicas","type":"talk"},{"authors":null,"categories":null,"content":"How to use the SDAccel Service SDAccel is a service that allow the user to load C/C++ aplications and optmize it using FPGA acceleration. To use this service, first go to the link below:\nhttps://ny1.ptopenlab.com/sdaccel/auth/login/?next=/sdaccel/project/#/projects/b767365a-8402-41a5-97b4-d148c359b114/detail?_k=p64eew In this page, you can enter your username and password to log in the SDAccel service. If you do not have on account, just create one and get back to that link.\nYou will be redirect to a page with the following tabs:\n Overview : this page contains some explanation about how the SDAccel is built and the advantages of using a service like that. Document : tutorials about SDAccel and some C/C++ code to run. Project : manage your projects and upload some code.  To create a new project, go to Project -\u0026gt; New Project.\nJust put a any name and description and press Submit.\nClick on the project name (that should be blue) and you wil be redirect to a files page.\nGo to the compile tab and wait a little till the loading is finished and, the console password on the password place and hit Enter. A desktop environment will appear on the screen.\nNow, on the virtualized desktop, go to Applications -\u0026gt; System Tools -\u0026gt; Terminal. Now type the following commands:\ncd\rmkdir test\rcd test\rgit clone https://github.com/Guilhermeslucas/SDAccel_Examples.git Now, close the terminal and click on the SDAccel Icon on the desktop, and hit ok on the first window and close the welcome tab.\nOn the Project Explorer -\u0026gt; New -\u0026gt; Xilinx SDAccel Project\nNow, enter any string to be the Project Name and change the locarion for the folder you placed the vadd project(just the src folder, from de SDACell_Examples. I will name the project as tutorial_code. The next step, is to click with the right button of the button on the project folder and hit build project. It will ask you to create a solution. Go ahed and create one. In order to do that click on add(change the name if you want) -\u0026gt; ok -\u0026gt; \u0026raquo; -\u0026gt; ok Now, try to build the project again as we said above (it should take some seconds).\nNow, open a terminal and go to the src folder for the project we are using. In that folder, should appear o .tcl file. Type:\nsdaccel \u0026#34;some_name\u0026#34;.tcl It will run and the results will appear on the folder.\nNote: if you prefer a video tutorial on YouTube, Bruno made a really good one.\nHope it was helpful.\nPost written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1475712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475712000,"objectID":"621cb3a7a095e4f7fa737d364b71d232","permalink":"https://openpower.ic.unicamp.br/post/sdaccel/","publishdate":"2016-10-06T00:00:00Z","relpermalink":"/post/sdaccel/","section":"post","summary":"How to use the SDAccel Service SDAccel is a service that allow the user to load C/C++ aplications and optmize it using FPGA acceleration. To use this service, first go to the link below:\nhttps://ny1.ptopenlab.com/sdaccel/auth/login/?next=/sdaccel/project/#/projects/b767365a-8402-41a5-97b4-d148c359b114/detail?_k=p64eew In this page, you can enter your username and password to log in the SDAccel service. If you do not have on account, just create one and get back to that link.\nYou will be redirect to a page with the following tabs:","tags":null,"title":"A Brief tutorial about how to use the SDAccel service","type":"post"},{"authors":[],"categories":null,"content":"","date":1475172000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475172000,"objectID":"b5976a390a62a81876c234555154da73","permalink":"https://openpower.ic.unicamp.br/talk/newcomers-oss-community/","publishdate":"2016-09-29T18:00:00Z","relpermalink":"/talk/newcomers-oss-community/","section":"talk","summary":"Open Source Software is an important economic driving force. Companies are aware of the benefits and are adopting OSS as a strategy, opening their source code. However, fostering an OSS developer community is challenging. Newcomers to OSS projects face many technical and social barriers and commonly drop out before making their first contribution. In this keynote, I will talk about how companies are opening their code, the barriers newcomers face to join OSS projects, and FLOSSCoach, a tool we developed to support newcomers first steps. **About the speaker:** Marco Aurélio Gerosa is an Associate Professor in the Computer Science Department at the University of São Paulo (USP), Brazil. His research lies in the intersection between Software Engineering and Social Computing, focusing on the fields of empirical software engineering, mining software repositories, software evolution, and social dimensions of software development. He has published more than 150 peer-reviewed papers. He served as Program Chair at IEEE ICGSE 2016 and PC member in several conferences, such as ACM CSCW, SANER, MSR, etc. In addition to his research, he also coordinates award-winning open source projects.","tags":[],"title":"Leveraging the Crowd: Supporting Newcomers to Build an OSS Community","type":"talk"},{"authors":[],"categories":null,"content":"","date":1474567200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474567200,"objectID":"d34b53bed728098b758e3380377b4ae7","permalink":"https://openpower.ic.unicamp.br/talk/git/","publishdate":"2016-09-22T18:00:00Z","relpermalink":"/talk/git/","section":"talk","summary":"Se você já perdeu horas e horas de trabalho por não ter salvo de forma apropriado o que havia feito, é melhor assistir a palestra desta quinta feita (22 Setembro no IC3, sala 353). Nela Juvenal A. Silva Jr., engenheiro de software com mais de 25 anos de experiência e que atualmente trabalha com GNU Toolchain no Linux Technology Center da IBM, vai mostrar os beneficios de se usar um software de controle de versão para atividades que vão além de versionar código fonte. Depois desta palestra, dizer que seu animal de estimição comeu seu trabalho não será mais aceito :-) **About the speaker:** Juvenal A. Silva Jr. é engenheiro de software com mais de 25 anos de experiência e atualmente trabalha com GNU Toolchain no Linux Technology Center da IBM.","tags":[],"title":"GIT","type":"talk"},{"authors":null,"categories":null,"content":"Setting Up SuperVessel and SSh connection to the Machines on Debian Based SuperVessel is a cloud based plataform created by IBM Research - China It allows you to set up containers on the cloud to run experiments, test aplications, do some data analysis and so on. One great feature of SuperVessel is to allow FPGA and GPU acelleration for C/C++ aplications.\nCreating an acount on SuperVessel First, go to the page below:\nhttps://ptopenlab.com/cloudlabconsole/?cm_mc_uid=35942743919214714482383\u0026amp;cm_mc_sid_50200000=1471628149#/ When you reach this adress, you will see that the first item of the Service Zone is for SuperVessel Cloud. Click on Apply VM -\u0026gt; Direct Acces. After doing that, you can login, or create an account. You can login as Community user . Do one of these steps and you have an SuperVessel account. This wil allow you to use all the other services on the page, like Acceleration and Big Data Services.\nSetting up a Machine After creating an account, you\u0026rsquo;re good to launch a machine. On the left side of the page, click on Instances. On the up menu of the page, there is a Current Region box. Make sure it is assigned to Beijing so we can access the machines via SSH easily. Now, click on Launch Instance. This should redirect you to a page with the specs of the machine on it. On the first drop-down menu, select Launch Docker Image and choose the specs that best fit your needs and then click on the button on the bottom of the forms to launch the instance and wait. It will appear a box with the instance\u0026rsquo;s information. If you want to access the machine from the browser, go to More Actions -\u0026gt; Console. The informations to log in appear on the top of the terminal.\nConnecting VPN First, install the VPNC:\nsudo apt-get install vpnc Then, create a SuperVessel conf file with the following command:\nsudo vim /etc/vpnc/supervessel.conf Note: you can use any editor, just don\u0026rsquo;t forget to run with sudo or as root user. The content of this file has to be:\nIPSec gateway 36.110.51.131\rIPSec ID Gemini IPSec secret G3m!ni1bmVpn Xauth username PoXXXX (change PoXXXX to your own VPN account) Xauth password secret_password (change secret_passsword to your own VPN password) The Xauth username and password have to be changed to your personal informattion, that you can find at the person symbol ate the upper left of the SuperVessel page, that you used to create an instance. Click on VPN Conf and put the Beijing fields on the supervessel.conf file. Now, run the vpnc:\nsudo vpnc-connect supervessel.conf A message like that should appear:\nVPNC started in background (pid: 12434)... Running SSH Now, on the Instances page, find the External IP(VPN) field. Now run:\nssh opuser@ExternalIP Note: change ExternalIP with the number you just saw. You can use ssh -X to xforward and ssh -C to compress connection.\nAfter you logout, don\u0026rsquo;t forget to run :\nsudo vpnc-disconnect That should do the work!\nPost written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1471564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471564800,"objectID":"8f43d7a44051c779f46acfcf0e5e8055","permalink":"https://openpower.ic.unicamp.br/post/supervessel/","publishdate":"2016-08-19T00:00:00Z","relpermalink":"/post/supervessel/","section":"post","summary":"Setting Up SuperVessel and SSh connection to the Machines on Debian Based SuperVessel is a cloud based plataform created by IBM Research - China It allows you to set up containers on the cloud to run experiments, test aplications, do some data analysis and so on. One great feature of SuperVessel is to allow FPGA and GPU acelleration for C/C++ aplications.\nCreating an acount on SuperVessel First, go to the page below:","tags":null,"title":"Setting Up SuperVessel and SSH connection to Machines on Debian Based","type":"post"},{"authors":["Guilherme Tiaki Sato"],"categories":null,"content":"Continuous integration allows code to be tested automatically every time it’s changed, detecting errors as early as possible. In this tutorial a CI using a GitHub repository will be approached.\nStep 1: Installing and setting up Jenkins and Git To install Jenkins, execute the following commands:\nwget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add -\rsudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list'\rsudo apt-get update\rsudo apt-get install jenkins\r To install git, simply execute:\nsudo apt-get install git\r Access Jenkins through http://localhost:8080 and follow the instructions for the initial setup. Choose Install suggested plugins when asked.\nStep 2: Creating a job In Jenkins dashboard, click on New Item, give your project a name and select Freestyle project.\nYou may choose Discard old builds in order to avoid using too much storage in the long term.\nCheck GitHub project and enter the GitHub URL of the project. Use the format https://github.com/YOUR-USERNAME/YOUR-REPOSITORY\nIn source code management section, choose Git and enter the repository URL the same way as above.\nStep 2.1: Choosing the build trigger Under Build Triggers it is possible to choose to build periodically or when a change is pushed into GitHub. Although building only when GitHub changes is more efficient, it is required to your Jenkins server to be accessible through the internet, and the you must own the repository. Building periodically may waste resources, but it is simpler to configure.\nStep 2.1.1: Build Periodically Check Build Periodically and define the period using the proper syntax found when clicking the ?.\nComplete the job creating by adding a build step (e.g. a shell script to compile and run a test) and jump to step 4\nThe test input and expected output should be in the repository.\nStep 2.1.2: Build when a change is pushed into GitHub Check build when a change is pushed into GitHub\nComplete the job creating by adding a build step (e.g. a shell script to compile and run a test) and follow to step 3\nThe test input and expected output should be in the repository.\nStep 3: Configuring GitHub plugin - Skip if building periodically Go to Manage Jenkins → Configure System → GitHub section → Advanced → Manage additional GitHub Actions → Convert login and password to token\nA new sub-section will appear right above.\nSelect From login and password, fill your login and password from GitHub and press Create token credentials\nAbove this sub-section, click Add GitHub server. Keep the API URL unchanged.\nUnder Credentials dropdown menu, select the token just created and test your connection.\nStep 4: Testing it If using periodical build, click the Build now icon to test. If the test fails, check the console output to find the issue (e.g. missing compiler).\nIf using GitHub trigger, change a file in the repository. The build should start automatically in a few seconds.\nStep 5: Adding slave machines - Optional As your projects grow, you may run out of resources in your machine. A possible solution is to add one or more slave machines, which will be responsible for building your projects, while the current machine will become the master and manage everything (the master will still be able to run jobs if desired).\nThe slave machine doesn\u0026rsquo;t need Jenkins installed on it. There are many ways to connect the slave with the master, here, SSH will be used.\nInstall Java and Git in the slave using:\nsudo apt-get install default-jre\rsudo apt-get install git\r Create a directory to be used by Jenkins, in this case will be the same path used by default in the master machine: /var/lib/jenkins\nsudo mkdir /var/lib/jenkins\r Change the ownership of the directory to the same user used to login using SSH\nchown ubuntu:ubuntu /var/lib/jenkins\r Back to the master machine:\nGo to Manage Jenkins → Manage Nodes → New Node\nName your node and select Permanent Agent\nThe recommended # of executors is the number of cores in the slave machine\nThe Remote root directory is the path to the directory created.\nIf necessary to divide the slave machines into different groups, label them (e.g. the OS running in the machine, the CPU architecture)\nThe Launch method used here will be SSH, but other methods are also fine.\nSimply enter your host and create a credential using your username and password, or username and private key.\nPress Save\nStep 5.1: Restricting machines where projects can be run If your slaves have different environments, your should restrict the machines where each project will run.\nUnder the project settings, check Restrict where this project can be run and type the machine name, use a label, or even use a more complex rule using logical operators (click the ? for more information)\nTo prevent the master machine to run projects, go to Manage Jenkins → Manage Nodes → master → Configure → # of executors and set to 0.\n","date":1465948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465948800,"objectID":"6f29ac7afa870784ac93f27093588259","permalink":"https://openpower.ic.unicamp.br/post/building-a-continuous-integration-platform-using-jenkins-and-github/","publishdate":"2016-06-15T00:00:00Z","relpermalink":"/post/building-a-continuous-integration-platform-using-jenkins-and-github/","section":"post","summary":"Continuous integration allows code to be tested automatically every time it’s changed, detecting errors as early as possible. In this tutorial a CI using a GitHub repository will be approached.\nStep 1: Installing and setting up Jenkins and Git To install Jenkins, execute the following commands:\nwget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add -\rsudo sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list'\rsudo apt-get update\rsudo apt-get install jenkins\r To install git, simply execute:","tags":null,"title":"Building a continuous integration platform using Jenkins and GitHub","type":"post"},{"authors":[],"categories":null,"content":"","date":1465408800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465408800,"objectID":"6d1b1bee3048596e0fd9ec7e37cda8ec","permalink":"https://openpower.ic.unicamp.br/talk/porting-to-power/","publishdate":"2016-06-08T18:00:00Z","relpermalink":"/talk/porting-to-power/","section":"talk","summary":"Manter sistemas legados em operação em novas arquiteturas representa um desafio tecnológico. Algumas empresas chegam a investir milhares de dólares na tentativa de migrar seus sistemas para novas arquiteturas. Contudo, em virtude da complexidade de algumas aplicações, o resultado final da migração nem sempre sai conforme o esperado. Variações sensíveis entre as arquiteturas, como diferenças no conjunto de instruções, da ABI ou na forma como os bytes são organizados em memória, podem ocasionar bugs difíceis de serem detectados durante a fase de teste. Em alguns casos, a migração pode ter um resultado muito aquém do esperado, reduzindo drasticamente o desempenho da aplicação. Nessa palestra, abordaremos alguns problemas clássicos de migração e como devemos proceder para resolvê-los, de forma a reduzir o impacto gerado no desempenho e na precisão dos resultados após a migração. Além disso, apresentaremos de forma didática alguns cuidados que devemos ter quando buscamos desenvolver programas portáveis. **About the speaker:** Alisson Linhares é mestre em Ciência da Computação pela Unicamp e tem mais de 7 anos de experiência com desenvolvimento de aplicações de baixo nível. Atualmente, trabalha no Linux Technology Center da IBM desenvolvendo o IBM SDK (kit de desenvolvimento oficial para Linux nos processadores Power).","tags":[],"title":"Migrando aplicações para a arquitetura Power","type":"talk"},{"authors":null,"categories":null,"content":"Downloading Packages You can use a script to download the necessary packages in the link above:\nftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/at_downloader/?cm_mc_uid=92476109699314629396752\u0026amp;cm_mc_sid_50200000=1464625581 Download the script and change its permission with the command above:\nchmod +x \u0026lt;script name\u0026gt; And then run the script with:\n./\u0026lt;script name\u0026gt; Please download the packages for Ubuntu 14.10. Then, the folder will be full of .deb files. The next step is to install some of these packages using dpkg.\nInstaling Packages and Toolchain Now, you need to install some of these. This is simple, but you have to do it following the order above. The command for each of those is\ndpkg -i \u0026lt;package name\u0026gt; The correct order is (the X is the version of the software):\nadvance-toolchain-atX.X-runtime-X.X-X\radvance-toolchain-atX.X-devel-X.X-X\radvance-toolchain-atX.X-perf-X.X-X\radvance-toolchain-atX.X-mcore-libs-X.X-X You can ignore the errors and move on.\nInstalling IBM SDK After all the dependencies issues are solved, download fdpr_wrap, fdpr-pro, pthread-mon, ibm-sdk-lop and ibm-sdk-lop-remote-dependencies on this link:\nhttps://www-304.ibm.com/webapp/set2/sas/f/lopdiags/sdkdownload.html#4 Install each of these packages with dpkg again, running:\nsudo dpkg -i \u0026lt;package name\u0026gt; respecting the sequence above.\nNote: to run the ibm-sdk-loop on the Power Machines using ssh, you will have to connect to the server using\nssh -XC -c blowfish-cbc,arcfour \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt; -p \u0026lt;port_number\u0026gt; The blowfish-cbc,arcfour will make your connection even better. This command is necessary to xforward the image of the sdk running. Note2: if you get the \u0026ldquo;no matching cipher found\u0026rdquo; error, here you go the solution:\nrun the command above:\nssh -Q cipher localhost | paste -d , -s Now its time to change the sshd_config file on the server(super privileges needed). Add on the /etc/ssh/sshd_config a line with:\nCiphers \u0026lt;output of the last command\u0026gt; Reboot the machine to make this alterations running:\nsudo shutdown -r now Post written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1464566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464566400,"objectID":"0a01874d72b3ee4a153ed48923bdf551","permalink":"https://openpower.ic.unicamp.br/post/sdk/","publishdate":"2016-05-30T00:00:00Z","relpermalink":"/post/sdk/","section":"post","summary":"Downloading Packages You can use a script to download the necessary packages in the link above:\nftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/at_downloader/?cm_mc_uid=92476109699314629396752\u0026amp;cm_mc_sid_50200000=1464625581 Download the script and change its permission with the command above:\nchmod +x \u0026lt;script name\u0026gt; And then run the script with:\n./\u0026lt;script name\u0026gt; Please download the packages for Ubuntu 14.10. Then, the folder will be full of .deb files. The next step is to install some of these packages using dpkg.\nInstaling Packages and Toolchain Now, you need to install some of these.","tags":null,"title":"How to Install IBM SDK on Ubuntu 16.04/14.04 Power Machines","type":"post"},{"authors":null,"categories":null,"content":"Optimizing C/C++ Applications with IBM SDK Build Advisor This is a brief text about how to use the IBM SDK Build Advisor to optimize C/C++ aplications on Power Servers. You can get the project and learn how to run on the link below:\nhttps://github.com/Guilhermeslucas/cmp Running seismic applications without optimization flags Here are some results on Power Machine before the optimization process.\nreal 0m46.837s\nreal 0m48.391s\nreal 0m52.570s\nreal 0m49.249s\nreal 0m48.863s\nImporting a C\\C++ Project to IBM SDK Before you begin, make sure there is a Makefile inside your project. Now, inside the SDK:\n Go to File \u0026gt; Import In the import window, expand C\\C++ and click in Existing Code as Makefile Project Now go to Browse next to the Existing code Location. Type a name for your project Locate the code and then click OK. Back to the Import Existing Code window, click the Advanced Toolchain Version corresponding to the one you have installed on the Power Machine. Click Finish  Using the Build Advisor  Right Click on the project, go to Properties Select the build advisor Enable Enable extra advice and then Finish. Right click on the project and build. The suggestions will appear.  Final results After using the flags that the SDK suggested\n-std=c99 -Ofast -fpeel-loops -flto -fopenmp -mcmodel=medium -ftree-vectorize -mcpu=power8 -mtune=power8 -funroll-loops I got the following results:\nreal\t0m3.177s\nreal\t0m2.573s\nreal\t0m3.066s\nreal\t0m2.954s\nreal\t0m2.930s\nAs you can see, Build Advisor is very effective.\nPost written by Guilherme Lucas. You can see some of my work at https://github.com/Guilhermeslucas .\n","date":1464566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464566400,"objectID":"546f36a957ae777bce116bf53d7337a7","permalink":"https://openpower.ic.unicamp.br/post/sdk_opt/","publishdate":"2016-05-30T00:00:00Z","relpermalink":"/post/sdk_opt/","section":"post","summary":"Optimizing C/C++ Applications with IBM SDK Build Advisor This is a brief text about how to use the IBM SDK Build Advisor to optimize C/C++ aplications on Power Servers. You can get the project and learn how to run on the link below:\nhttps://github.com/Guilhermeslucas/cmp Running seismic applications without optimization flags Here are some results on Power Machine before the optimization process.\nreal 0m46.837s\nreal 0m48.391s\nreal 0m52.570s\nreal 0m49.249s\nreal 0m48.","tags":null,"title":"Optimizing C/C++ applications","type":"post"},{"authors":null,"categories":null,"content":"Instalação do SU e do Cmp_toy na máquina pessoal Instalação do SU Seismic Unix é um conjunto de ferramentas extremamente úteis para o processamento de dados sísmicos. Para instalá-lo no Ubuntu 14.04, precisei baixar algumas bibliotecas. Segue quais são e como instalá-las:\n\u0026lt;X11/Xlib.h\u0026gt;\nsudo apt-get install libx11-dev \u0026lt;X11/Intrinsic.h\u0026gt;\n\u0026lt;X11/Intrinsic.h\u0026gt; - sudo apt-get install libxt-dev Também precisamos do CMake para completar a instalação do Seismic Unix. Rode:\nsudo apt-get install cmake Link do repositório do cmp_toy:\ngithub.com/gga-cepetro/cmp Agora para realmente terminar a instalção, basta rodar os seguintes comandos:\ninstall_dir=~/src/cwp\rmkdir -p $install_dir \u0026amp;\u0026amp; cd $install_dir\rexport CWPROOT=$PWD\recho \u0026#34;export CWPROOT=$PWD\u0026#34; \u0026gt;\u0026gt; ~/.bashrc\recho \u0026#39;export PATH=$PATH:$CWPROOT/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc\rwget ftp://ftp.cwp.mines.edu/pub/cwpcodes/cwp_su_all_44R1.tgz\rtar zxf cwp_su_all_44R1.tgz\rcd src\rsed -i \u0026#39;s/^XDRFLAG/#XDRFLAG/\u0026#39; Makefile.config\rmake install ; make xtinstall Agora para testar se o software, basta executar\nsuplane | suximage Uma imagem com três planos deve ser apresentada. Ela parece ser um pouco esquista, a princípio, mas se uma janela com uma imagem foi aberta, então a instalação foi realizada com sucesso.\nInstalação do cmp_toy Agora para instalar o cmp_toy e obter as curvas desejadas, precisamos executar os seguintes comandos, após obter o cmp_toy.tar.gz :\ntar -xzf cmp_toy.tar.gz\rmkdir build\rcd build\rcmake ..\rmake Agora que o software está instalado, vamos testá-lo. É necessário mudar para o diretório pai desse que estamos e rodar o script de teste, que usa uma imagem criada extamente para esse teste:\ncd ..\r./test-cmp.sh A saída desse comando (ao completar 100%) deve seguir o seguinte padrão, com números possivelmente diferentes:\n[100%] Processing CDP 300\rreal\t0m56.573s\ruser\t0m56.589s\rsys\t0m0.008s Agora, para testar a imagem gerada, basta rodar o seguinte comando:\nsuximage \u0026lt; cmp.stack.su O output desse comando deve ser uma imagem com o plot de algumas curvas.\nPost escrito por Guilherme Lucas. Mais um pouco do meu trabalho no meu Github https://github.com/Guilhermeslucas .\n","date":1463702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463702400,"objectID":"d477cdca83f937f59fce331ee2527fb6","permalink":"https://openpower.ic.unicamp.br/post/ic/","publishdate":"2016-05-20T00:00:00Z","relpermalink":"/post/ic/","section":"post","summary":"Instalação do SU e do Cmp_toy na máquina pessoal Instalação do SU Seismic Unix é um conjunto de ferramentas extremamente úteis para o processamento de dados sísmicos. Para instalá-lo no Ubuntu 14.04, precisei baixar algumas bibliotecas. Segue quais são e como instalá-las:\n\u0026lt;X11/Xlib.h\u0026gt;\nsudo apt-get install libx11-dev \u0026lt;X11/Intrinsic.h\u0026gt;\n\u0026lt;X11/Intrinsic.h\u0026gt; - sudo apt-get install libxt-dev Também precisamos do CMake para completar a instalação do Seismic Unix. Rode:\nsudo apt-get install cmake Link do repositório do cmp_toy:","tags":null,"title":"Instalação de Seismic Unix e Cmp_toy em arquitetura Intel","type":"post"},{"authors":null,"categories":null,"content":"Tutorial de instalação do cmp_toy em Máquinas Power Esse é um breve tutorial de como foi para instalar e rodar o código sísmico cmp_toy nas máquinas Power, acessado pelo sistema de Minicloud. Todo o processo foi feito usando uma máquina com Ubuntu 16.04 instalado, mas também deve funcionar em máquinas com sistemas Debian Based.\nResolução de problemas nos pacotes Ao logar na máquina, tive alguns problemas com pacotes quebrados. Para arrumar esse problema usei os comandos comandos do dpkg e do apt-get, respectivamente:\nsudo dpkg --configure -a\rsudo apt-get -f install Primeiros Softwares e Pacotes necessários O primeiro software que precisei, foi o editor de texto Vim, muito poderoso e eficiente, ainda mais quando estamos acessando uma máquina remotamente. Para instalá-lo basta executar:\nsudo apt-get install vim Após copiar o meu .vimrc para a máquina remota, baixei os dois compiladores necessários para rodar o código. Pra esse, preciso do compilador de C e C++, e por preferências pessoais instalei o gcc e g++. Segue os comandos para instalação de cada um deles:\nsudo apt-get install gcc\rsudo apt-get install g++ Link do repositório do cmp_toy:\ngithub.com/gga-cepetro/cmp Durante a instalação é necessário rodar o CMake, que também não esta instalado. Para acertar esse problema, rode:\nsudo apt-get install cmake Vale lembrar que todos os comandos desde o início do desse tutorial devem ser feitos logado na máquina remota.\nTrocando arquivos remotamente e instalando do software Como fazemos acesso às máquinas com o comando ssh -p , ou seja, acessamos por meio de uma porta específica, temos que executar o comando scp de maneira diferente também, para copiar um arquivo local para uma máquina remota. Nesse caso, queremos copiar o arquivo cmp_toy.tar.gz (você deve estar no diretório desse arquivo, ou seja, na sua máquina física e não logado nas máquinas remotas).\nscp -P \u0026lt;numero da porta\u0026gt; cmp_toy.tar.gz seu_usuario@host_remoto:/algum/diretorio/remoto Agora, se conectado na máquina remota, e vá até o diretório onde mandou o arquivo. Voce deve encontrar o cmp_toy.tar.gz. Agora para instalá-lo, rode os seguintes comandos:\ntar -xzf cmp_toy.tar.gz\rcd cmp_toy\rmkdir build\rcd build\rcmake ..\rmake Agora que a instalação do software foi feita com sucesso, precisamos testá-lo. Para isso, basta ir para o diretório pai e rodar o script de teste, que usa uma imagem pronta para teste.\ncd ..\r./test-cmp.sh Após um tempinho, você deve receber uma saída que segue o seguinte padrão (não necessariamente com os mesmos números):\n[100%] Processing CDP 300\rreal\t0m59.968s\ruser\t0m59.908s\rsys\t0m0.056s Basicamente, os passos são esses. Tentei fazer esse tutorial da maneira mais detalhada possível. Se ainda restarem dúvidas, não exitem em entrar em contato.\nPost escrito por Guilherme Lucas. Mais um pouco do meu trabalho no meu Github https://github.com/Guilhermeslucas .\n","date":1463702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463702400,"objectID":"adfc65621c6840d992fe706d1cf4c6d9","permalink":"https://openpower.ic.unicamp.br/post/power/","publishdate":"2016-05-20T00:00:00Z","relpermalink":"/post/power/","section":"post","summary":"Tutorial de instalação do cmp_toy em Máquinas Power Esse é um breve tutorial de como foi para instalar e rodar o código sísmico cmp_toy nas máquinas Power, acessado pelo sistema de Minicloud. Todo o processo foi feito usando uma máquina com Ubuntu 16.04 instalado, mas também deve funcionar em máquinas com sistemas Debian Based.\nResolução de problemas nos pacotes Ao logar na máquina, tive alguns problemas com pacotes quebrados. Para arrumar esse problema usei os comandos comandos do dpkg e do apt-get, respectivamente:","tags":null,"title":"Instalação de Seismic Unix e Cmp_toy em arquitetura Power","type":"post"},{"authors":[],"categories":null,"content":"","date":1442858400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442858400,"objectID":"34de75fdb75d5510e2f33b9faec1cd86","permalink":"https://openpower.ic.unicamp.br/talk/opensource-development/","publishdate":"2015-09-21T18:00:00Z","relpermalink":"/talk/opensource-development/","section":"talk","summary":"O palestrante irá abordar o tema Desenvolvimento OpenSource: Participando das comunidades, enviando patches, o que não fazer, como fazer... Buscando explanar como os aspirantes ao desenvolvimento opensource podem dar os primeiros passos. **About the speaker:** Leônidas S. Barbosa a.ka leosilva a.k.a Kirotawa, trabalho no Linux Technology Center na IBM no time de segurança.Atualmente é maintainer dos drivers nx-crypto e vmx-crypto, drivers cryptográficos suportados em p7+ e p8. Desenvolvedor aventureiro em tempo livre, sempre que possível coda alguma coisa e pública no .git. Sempre quis trabalhar com OpenSource e com Kernel. Aspira um dia enviar um patch para o Kexec ou Efi subsystem no Linux Kernel.","tags":[],"title":"Alice no País do Desenvolvimento OpenSource","type":"talk"},{"authors":null,"categories":null,"content":"Glance is the Openstack Image Service and enables users to discover, register, and retrieve virtual machine images. This service allows users to query virtual machine images information and retrieve the image itself using a REST API and must be installed in the controller node.\n#Prerequisites\nLoad the OpenRC file:\nsource adm-credentiansl.sh Create a database for Glance:\nmysql -u root -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO \u0026#39;glance\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;glance\u0026#39;; GRANT ALL PRIVILEGES ON glance.* TO \u0026#39;glance\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;glance\u0026#39;; Create the keystone entities, service and endpoint:\nopenstack user create --password-prompt glance openstack role add --project service --user glance admin openstack service create --name glance --description \u0026#34;OpenStack Image service\u0026#34; image openstack endpoint create --publicurl http://controller:9292 \\ --internalurl http://controller:9292 --adminurl http://controller:9292 \\ --region RegionOne image #Install and Configure\nInstall the packages:\napt-get install glance python-glanceclient Edit the file /etc/glance/glance-api.cnf as following:\n[DEFAULT] ... notification_driver = noop verbose = True [database] connection = mysql://glance:GLANCE_DBPASS@controller/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] flavor = keystone [glance_store] default_store = file filesystem_store_datadir = /var/lib/glance/images/ Also edit the file /etc/glance/glance-registry.cnf:\n[DEFAULT] ... notification_driver = noop verbose = True [database] connection = mysql://glance:GLANCE_DBPASS@controller/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] flavor = keystone Populate the image service database:\nsu -s /bin/sh -c \u0026#34;glance-manage db_sync\u0026#34; glance Finalize the installation process:\nservice glance-registry restart service glance-api restart rm -f /var/lib/glance/glance.sqlite #Greenlet Fix\nThe Greenlet version that is in the Ubuntu repository has problems with PPC64le architecture, therefore we need to manually download and install a newer version of the library.\nDownload and unpack the package code:\nwget https://github.com/python-greenlet/greenlet/archive/master.zip -D greenlet.zip unzip greenlet.zip -D cd greenlet Install the package:\nexport CFLAGS=-O1; ./setup.py install Restart the glance services:\nservice glance-registry restart service glance-api restart #Upload an image to the Image Server\nConfigure the server to use the API version 2.0 and reload the credentials:\necho \u0026#34;export OS_IMAGE_API_VERSION=2\u0026#34; | tee -a adm-credentials.sh source adm-credentials.sh As an example upload Ubuntu 14.04.02 PPC64le to the server:\nglance image-create --name=\u0026#34;ubuntu1404-ppc64le\u0026#34; --disk-format=qcow2 --container-format=bare \\ --is-public=true \\ -copy-from https://cloud-images.ubuntu.com/releases/14.04/14.04.2/ubuntu-14.04-server-cloudimg-ppc64el-disk1.img Check the list of images:\nglance image-list ","date":1442361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442361600,"objectID":"8fdadf041d1ad60f6fd073089a2356a7","permalink":"https://openpower.ic.unicamp.br/post/openstack_setup_4/","publishdate":"2015-09-16T00:00:00Z","relpermalink":"/post/openstack_setup_4/","section":"post","summary":"Glance is the Openstack Image Service and enables users to discover, register, and retrieve virtual machine images. This service allows users to query virtual machine images information and retrieve the image itself using a REST API and must be installed in the controller node.\n#Prerequisites\nLoad the OpenRC file:\nsource adm-credentiansl.sh Create a database for Glance:\nmysql -u root -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO \u0026#39;glance\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;glance\u0026#39;; GRANT ALL PRIVILEGES ON glance.","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 04 – Glance","type":"post"},{"authors":null,"categories":null,"content":"This service provides a central directory of users mapped to the OpenStack services. It\u0026rsquo;s used to provide an authentication and authorization service for other OpenStack services. In addition to the identity service, we will install two more packages, the Apache HTTP server and the Memcahed, responsible, respectively, for receiving requests and store the authentication tokens.\n#Prerequisites\nAll the keystone data will be stored in a database, acess the mysql and execute the following commands:\n\u0026gt;mysql -u root -p CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO \u0026#39;keystone\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;keystone\u0026#39;; GRANT ALL PRIVILEGES ON keystone.* TO \u0026#39;keystone\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;keystone\u0026#39;; #Install and Configure\nThe version Kilo uses a WSGI server to listen the requests to keystone, we choose to use the Apache Server running a WSGI mod.\nExecute the following commands to disable keystone automatic start and install the packages:\n\u0026gt;echo \u0026#34;manual\u0026#34; \u0026gt; /etc/init/keystone.override apt-get install keystone python-openstackclient apache2 libapache2-mod-wsgi \\ memcached python-memcache Edit the following sections in the file /etc/keystone/keystone.conf:\u0026gt;[DEFAULT] ... admin_token = ADMIN_TOKEN verbose = True [database] connection = mysql://keystone:KEYSTONE_DBPASS@controller/keystone [memcache] servers = localhost:11211 [token] provider = keystone.token.providers.uuid.Provider driver = keystone.token.persistence.backends.memcache.Token [revoke] driver = keystone.contrib.revoke.backends.sql.Revoke Create the file /etc/apache2/sites-available/wsgi-keystone.conf with the following content:\n\u0026gt;Listen 5000 Listen 35357 \u0026lt; VirtualHost *:5000 \u0026gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /var/www/cgi-bin/keystone/main WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On \u0026lt; IfVersion \u0026gt;= 2.4 \u0026gt; ErrorLogFormat \u0026#34;%{cu}t %M\u0026#34; \u0026lt; /IfVersion \u0026gt; LogLevel info ErrorLog /var/log/apache2/keystone-error.log CustomLog /var/log/apache2/keystone-access.log combined \u0026lt; /VirtualHost \u0026gt; \u0026lt; VirtualHost *:35357 \u0026gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /var/www/cgi-bin/keystone/admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On \u0026lt; IfVersion \u0026gt;= 2.4 \u0026gt; ErrorLogFormat \u0026#34;%{cu}t %M\u0026#34; \u0026lt; /IfVersion \u0026gt; LogLevel info ErrorLog /var/log/apache2/keystone-error.log CustomLog /var/log/apache2/keystone-access.log combined \u0026lt; /VirtualHost \u0026gt; Install and configure the WSGI mod for Apache:\n\u0026gt;ln -s /etc/apache2/sites-available/wsgi-keystone.conf /etc/apache2/sites-enabled mkdir -p /var/www/cgi-bin/keystone curl http://git.openstack.org/cgit/openstack/keystone/plain/httpd/keystone.py?h=stable/kilo \\ | tee /var/www/cgi-bin/keystone/main /var/www/cgi-bin/keystone/admin chown -R keystone:keystone /var/www/cgi-bin/keystone chmod 755 /var/www/cgi-bin/keystone/* Restart the Apache service:\n\u0026gt;rm -f /var/lib/keystone/keystone.db service apache2 restart #Create the service endpoint\nTo really have the Keystone service working is needed to create a service endpoint to listen requests:\nExport the following lines to obtain administrator privileges on Keystone:\n\u0026gt;export OS_TOKEN=ADMIN export OS_URL=http://controller:35357/v2.0 Registry the service:\n\u0026gt;openstack service create --name keystone --description \u0026#34;OpenStack Identity\u0026#34; identity openstack endpoint create --publicurl http://controller:5000/v2.0 \\ --internalurl http://controller:5000/v2.0 --adminurl http://controller:35357/v2.0 \\ --region RegionOne identity #Create the environment entities\nEach of the services that compose OpenStack uses the keystone as the authentication point, this process involves a combination of various entities (users, projects, etc.). You can better understand the functioning of Keystone at the documentation page:\nCreate the Admininstration and Demo projetct:\n\u0026gt;openstack project create --description \u0026#34;Admin Project\u0026#34; admin openstack project create --description \u0026#34;Demo Project\u0026#34; demo Create users and roles and after make sure each user is registred in one role:\n\u0026gt;openstack role create admin openstack role create user openstack user create --password-prompt admin openstack user create --password-prompt demo openstack role add --project admin --user admin admin openstack role add --project demo --user demo user Each service that will be installed is represent as an user in Keystone, they will be part of an project that will contain all the services:\n\u0026gt;openstack project create --description \u0026#34;Service Project\u0026#34; service #OpenRC files\nTo simplify the keystone authentication process create a file that contains the credenditals and export to the system.\nCreate the file adm-credentials.sh and add the following:\n\u0026gt;export OS_PROJECT_DOMAIN_ID=default export OS_USER_DOMAIN_ID=default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=SENHA_ADMIN export OS_AUTH_URL=http://controller:35357/v3 Load the file content:\n\u0026gt;source adm-credentials.sh ","date":1441756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441756800,"objectID":"c3d7e0b566723996e24d685f1ffe031a","permalink":"https://openpower.ic.unicamp.br/post/openstack_setup_3/","publishdate":"2015-09-09T00:00:00Z","relpermalink":"/post/openstack_setup_3/","section":"post","summary":"This service provides a central directory of users mapped to the OpenStack services. It\u0026rsquo;s used to provide an authentication and authorization service for other OpenStack services. In addition to the identity service, we will install two more packages, the Apache HTTP server and the Memcahed, responsible, respectively, for receiving requests and store the authentication tokens.\n#Prerequisites\nAll the keystone data will be stored in a database, acess the mysql and execute the following commands:","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 03 – Keystone","type":"post"},{"authors":null,"categories":null,"content":"The environment will consist of two Power8 machines with Ubuntu 15.04 installed, one of the machines will contain the core cloud services (controller node), and the other one the virtualization services (compute node). In this guide we are going to install the newest OpenStack release, the Kilo version. However, before installing the cloud services it\u0026rsquo;s necessary to properly set up the environment, this post will cover all the datails.\n#Passwords\nAs a convention, passwords for each service will be the service name in lowercase, do not forget that in a real environment passwords must be chosen carefully.\n#Network\nIt\u0026rsquo;s possible to install OpenStack with two different network architectures, legacy networking (nova-network) and Neutron. Initially we will use nova-network, our network environment is represented as shown below:\nNote that we have two networks, the 10.0.0.0/24 is the management network (where the nodes will establish comunicate) and the 10.0.2.0/24 is the external network (each created VM will have an IP external acessible).\nEdit the file /etc/hosts on all machines and add the following:\n#controller 10.0.0.10 controller #compute 10.0.0.11 compute01 #Network Time Protocol (NTP)\nIn order to synchronize services installed on different nodes we will install NTP.\nFirst of all install NTP on all machines:\napt-get install ntp ##On controller\nEdit the file /etc/ntp.conf and add or edit the follwing lines:\nserver 0.ubuntu.pool.ntp.org iburst restrict -4 default kod notrap nomodify restrict -6 default kod notrap nomodify Restart the NTP service:\nservice ntp restart ##On compute\nEdit the file /etc/ntp.conf and change the server to:\nserver controller iburst Restart the NTP service:\nservice ntp restart #Openstack and system packages\nWe have to configure the package respository to point to the Openstack Kilo release and verify if the system is up-to-date. Perform the followig step on all nodes.\nInstall Ubuntu keyring and set the Kilo repository:\napt-get install ubuntu-cloud-keyring echo \u0026#34;deb http://ubuntu-cloud.archive.canonical.com/ubuntu\u0026#34; \\ \u0026#34;trusty-updates/kilo main\u0026#34; \u0026gt; /etc/apt/sources.list.d/cloudarchive-kilo.list Upgrade the packages on your system:\napt-get update apt-get dist-upgrade #SQL database\nThe SQL database will be installed only on controlle node, the openstack services mostly use a database to store all the information they need. We choose to install MySQL server.\nInstall packages:\napt-get install mysql-server python-mysqldb Edit the file /etc/mysql/mysql.conf.d/mysqld.conf in the [mysqld] section:\n[mysqld] ... bind-address = 10.0.0.10 default-storage-engine = innodb innodb_file_per_table collation-server = utf8_general_ci init-connect = \u0026#39;SET NAMES utf8\u0026#39; character-set-server = utf8 Restart the Mysql service:\nservice mysql restart Execute the following mysql script to secure the database service:\nmysql_secure_installation ##Message Queue\nOpenstack uses the strategy of a queue message to coordinate the actions related to the services, in other words, a message queue running on the controller node coordinates the comunication between the services in order to have all the services properly. In this guide we will use a message queue server called RabbitMQ.\nInstall the packages:\napt-get install rabbitmq-server Create the openstack user:\nrabbitmqctl add_user openstack RABBIT_PASS As mentioned in password section, replace RABBIT_PASS by rabbit.\nConfigure permissions to openstack user:\nrabbitmqctl set_permissions openstack \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; ##Next steps\nWith the environment properly configured we can setup the OpenStack services, in the next post we will configure the identity service, known as Keystone.\n","date":1440115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440115200,"objectID":"5626bb191c7615178eca24b5cd833a8a","permalink":"https://openpower.ic.unicamp.br/post/openstack_setup_2/","publishdate":"2015-08-21T00:00:00Z","relpermalink":"/post/openstack_setup_2/","section":"post","summary":"The environment will consist of two Power8 machines with Ubuntu 15.04 installed, one of the machines will contain the core cloud services (controller node), and the other one the virtualization services (compute node). In this guide we are going to install the newest OpenStack release, the Kilo version. However, before installing the cloud services it\u0026rsquo;s necessary to properly set up the environment, this post will cover all the datails.\n#Passwords","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 02 – Environment Setup","type":"post"},{"authors":null,"categories":null,"content":"#Introduction\nIn this series we are going to detail all the necessary steps to setup an OpenStack-based cloud with IBM POWER8 machines from the scratch, but first of all, let\u0026rsquo;s take a look at some basic questions like what is OpenStack and why one would want to install and use it?\n#What is OpenStack?\nOpenStack is a free and open-source set of connected components aiming to serve as an cloud computing operating system capable of managing large pools of compute, storage and networking resources, all managed through a administrator dashboard. It\u0026rsquo;s robustness and reliability as one of the most active open-source project today makes it an really good choice for offering cloud computing services (IaaS) on standarized hardware, and due to its simplicity and massive scalability it can be used as an solution for a large amout of users, from a small home environments with few machines to large datacenters with hundreds of machines.\nThe OpenStack project began first in 2010 as an joint project of Rackspace Hosting and NASA, today, the project itself is managed by the OpenStack Foundation and have more than 500 supporters among companies and research centers.\n#Components\nThe main project is implemented in a modular architecture with many components, each one performing its own responsibility in the system. The diagram below can be found at the OpenStack official documentation:\n##Horizon (Dashboard)\nThis component responsibility consists in providing a web-based interface to easily access the OpenStack services.\n##Compute (Nova)\nThe main part of any IaaS system, the Nova project performs the controller role, managing and automating pools of computer resources. It supports many virtualization technologies and is it responsibility to manage the virtual machines on the system.\n##Networking (Neutron)\nManages the networks and IP adresses, providing users total control over network configurations. Standard network models works with separate VLANs for each user to distribute the network access, the Neutron component treats the question differently, managing the IP adressess, allowing a more flexible and maintainable network usage.\n##Object Storage (Swift)\nThe Swift component stores and retrieves unstructured data object through the HTTP based APIs.\n##Block Storage (Cinder)\nProvides persistent storage to running services, its implemented in such a way that makes creating and managing block storage very easy.\n##Identity Service (Keystone)\nThis provides a central directory of users mapped to the OpenStack services. It is used to provide an authentication and authorization service for other OpenStack services.\n##Image Service (Glance)\nThis provides the discovery, registration and delivery services for the disk and server images. It stores and retrieves the virtual machine disk image.\n##Telemetry (Ceilometer)\nIt monitors the usage of the Cloud services and decides the billing accordingly. This component is also used to decide the scalability and obtain the statistics regarding the usage.\n##Orchestration (Heat)\nThis component manages multiple Cloud applications through an OpenStack-native REST API and a CloudFormation-compatible Query API.\n#Why would I use OpenStack?\nIn sight of all the features listed above and all the benefits that OpenStack can offer to its users and administrators we choose it to serve as infrastructure to our POWER8-based cloud, the steps to setup and manage the system will be discussed throughout the next series posts, keep in touch!\n#Sources:\nWikipedia OpenStack article\nOfficial OpenStack documentation\n","date":1440028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440028800,"objectID":"022bf125ca7c73d7d84888222ae5613d","permalink":"https://openpower.ic.unicamp.br/post/openstack_setup_1/","publishdate":"2015-08-20T00:00:00Z","relpermalink":"/post/openstack_setup_1/","section":"post","summary":"#Introduction\nIn this series we are going to detail all the necessary steps to setup an OpenStack-based cloud with IBM POWER8 machines from the scratch, but first of all, let\u0026rsquo;s take a look at some basic questions like what is OpenStack and why one would want to install and use it?\n#What is OpenStack?\nOpenStack is a free and open-source set of connected components aiming to serve as an cloud computing operating system capable of managing large pools of compute, storage and networking resources, all managed through a administrator dashboard.","tags":null,"title":"Setting up an OpenStack-based cloud with Power8 | Part 01 – Introduction","type":"post"},{"authors":null,"categories":null,"content":"There are many reasons why one would want to build its custom router instead of buying one. Control and flexibility are two reasons, and we need both when dealing with large traffic. The purpose of this guide is to give a step-by-step solution starting on how to build a virtual machine. For this, we will assume that PowerKVM is already up and running along with its network configurations.\nSo, for this guide we will need:\n A PowerKVM machine Two network cards  In this case, eth0 will be our internal network interface and eth1 our external network interface. Both of them will be bridged to the virtual machine and this configuration can be made through Kimchi\u0026rsquo;s web interface.\nCreating a Debian Virtual Machine Downloading the right ISO First we\u0026rsquo;ll download Debian\u0026rsquo;s 8.1 DVD Image for PPC64el architecture. It can be found on this link and should be stored in /var/lib/kimchi/isos/ folder.\ncd /var/lib/kimchi/isos wget http://cdimage.debian.org/debian-cd/8.1.0/ppc64el/iso-dvd/debian-8.1.0-ppc64el-DVD-1.iso Then run md5sum to see if the file is corrupted:\nwget http://cdimage.debian.org/debian-cd/8.1.0/ppc64el/iso-dvd/MD5SUMS md5sum -c MD5SUMS The result should be:\ndebian-8.1.0-ppc64el-DVD-1.iso: OK Otherwise, try downloading again.\nBringing to Life Now that we have our ISO, we\u0026rsquo;ll create an qcow2 image using qemu to act as a hard drive. Those images should be stored in /var/lib/libvirt/images/.\nqemu-img create -f qcow2 -o preallocation=metadata storage.qcow2 10G Then, we can start the installation using virt-install:\nvirt-install -r 12228 --os-variant=debianwheezy --network bridge=virbr0,model=virtio --accelerate -n debian --vcpus=maxvcpus=16,sockets=2,cores=2,threads=4 -f ./storage.qcow2 --graphics vnc,listen=0.0.0.0 -c /var/lib/kimchi/isos/debian-8.1.0-ppc64el-DVD-1.iso If you\u0026rsquo;re using a different OS, you can list all available options with:\nvirt-install --os-variant list Instalation will start. In this case, it was done throught Kimchi\u0026rsquo;s web monitor, but can be done using libvirt. Proceed normally. After it\u0026rsquo;s finished, you can start your VM and login with:\nvirsh start debian virsh console debian #Network Configuration\nAs said before, eth0 and eth1 will be bridged to the VM through Kimchi\u0026rsquo;s web interface, where eth0 is our internal network interface and eth1, external network.\n##Setting IPs We\u0026rsquo;ll edit /etc/network/interfaces file and assign static IP\u0026rsquo;s both internal and external. Your external address and gateway should be provided by your ISP.\nnano /etc/network/interfaces auto lo iface lo inet loopback # The primary network interface allow-hotplug eth0 iface eth0 inet static address 10.0.0.1 netmask 255.255.255.0 allow-hotplug eth1 iface eth1 inet static address 0.0.0.0 netmask 255.255.255.0 gateway 0.0.0.0 Edit your /etc/resolv.conf if needed by your ISP:\nnano /etc/resolv.conf nameserver ISP_server; search ISP_address; After restarting your network service, you should have something like this:\nsystemctl restart networking \u0026amp;\u0026amp; ifconfig eth0 Link encap:Ethernet HWaddr 52:54:00:37:bc:11 inet addr:10.0.0.1 Bcast:10.0.0.255 Mask:255.255.255.0 eth1 Link encap:Ethernet HWaddr 52:54:00:7b:74:6f inet addr: 0.0.0.0 Bcast:0.0.0.0 Mask:255.255.255.0 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 See if it\u0026rsquo;s working by pinging internal and external addresses:\nping www.cnn.com ping 10.0.0.5 ##Routing Start by flushing all previous configurations, if they exist.\niptables -F iptables -t nat -F iptables -t mangle -F iptables -X We\u0026rsquo;ll now allow established connections, outgoing connections and setup masquerade as follows:\niptables -A INPUT -i lo -j ACCEPT iptables -A FORWARD -i eth1 -o eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE And now, we\u0026rsquo;ll allow IP Forwarding:\necho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward And your Iptables should look like this:\niptables -L Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED ACCEPT all -- anywhere anywhere Chain OUTPUT (policy ACCEPT) target prot opt source destination Now a client should successfully connect to the internet.\n##Making it Permanent Now we want to apply these iptables configurations everytime we start this machine. This can be done by saving them in a file and restoring on the next boot.\niptables-save \u0026gt;\u0026gt; /etc/iptables.rules On /etc/network/interfaces, add this line underneath “iface lo inet loopback”:\nnano /etc/network/interfaces pre-up iptables-restore \u0026lt; /etc/iptables.rules #That\u0026rsquo;s it\nBy now you should have a basic Linux gateway for your network. Much more advanced configuration can be done that can add enormous flexibility. It\u0026rsquo;s up to you to start exploring and unleash the true power of having a dedicated machine as your router.\n","date":1438992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438992000,"objectID":"4aeb49cef50e37592310b61a0cb920ce","permalink":"https://openpower.ic.unicamp.br/post/debian_gateway/","publishdate":"2015-08-08T00:00:00Z","relpermalink":"/post/debian_gateway/","section":"post","summary":"There are many reasons why one would want to build its custom router instead of buying one. Control and flexibility are two reasons, and we need both when dealing with large traffic. The purpose of this guide is to give a step-by-step solution starting on how to build a virtual machine. For this, we will assume that PowerKVM is already up and running along with its network configurations.\nSo, for this guide we will need:","tags":null,"title":"Setting up a Debian Gateway Virtual Machine on PowerKVM","type":"post"},{"authors":[],"categories":null,"content":"","date":1434132000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434132000,"objectID":"c73862ef29f2e7937614a8add46eac49","permalink":"https://openpower.ic.unicamp.br/talk/digital-signature/","publishdate":"2015-06-12T18:00:00Z","relpermalink":"/talk/digital-signature/","section":"talk","summary":"Assinatura digital é uma analogia a conhecida assinatura escrita. No meio digital podemos utilizar uma assinatura digital para (1) prover garantias que foi realmente o signatário quem assinou uma determinada informação, e também (2) detectar se a informação foi modificada ou não após ter sido assinada (integridade da informação). A tecnologia assinatura digital é especificada pelo NIST através da publicação FIPS 184-6. Nesta palestra, o objetivo é mostrar como se faz para gerar e verificar uma assinatura digital em Linux com base na publicação FIPS 184-6. Para mostrar a sua abrangência em Linux, o uso de assinatura digital será discutido em três níveis de aplicações: email, kernel e firmware. **About the speaker:** Claudio Carvalho é engenheiro de software na IBM e atualmente trabalha com desenvolvimento de segurança em Linux no LTC (Linux Technology Center). Claudio é bacharel em Engenharia da Computação pela PUC-GO e Mestre em Ciência da Computação pela Unicamp, e trabalha com Linux há mais de 10 anos.","tags":[],"title":"Assinatura digital: de user-space a firmware","type":"talk"},{"authors":[],"categories":null,"content":"","date":1431108000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431108000,"objectID":"1566725f6485b7fd74316bb77bd28ab7","permalink":"https://openpower.ic.unicamp.br/talk/openpower-research/","publishdate":"2015-05-08T18:00:00Z","relpermalink":"/talk/openpower-research/","section":"talk","summary":"**About the speaker:** Ricardo Matinata, arquiteto do Linux Technology – IBM","tags":[],"title":"Oportunidades de Pesquisa em OpenPOWER","type":"talk"},{"authors":[],"categories":null,"content":"","date":1428602400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428602400,"objectID":"57f0403264ed78a085afa82c8f84fb6d","permalink":"https://openpower.ic.unicamp.br/talk/power-architecture/","publishdate":"2015-04-09T18:00:00Z","relpermalink":"/talk/power-architecture/","section":"talk","summary":"A arquitetura de processadores POWER da IBM, apesar de já muito bem consolidada no mercado corporativo de servidores de médio e grande porte ainda é relativamente pouco explorada na área acadêmica, principalmente no Brasil. Se você nunca pensou em trabalhar com outras arquiteturas de processadores que não a x86, você se surpreendera com algumas características de escalabilidade de processamento paralelo e acesso a grandes quantidades de memória que o processador POWER oferece sem que seja necessário mudar sua aplicação. Venha conhecer um pouco sobre a arquitetura Power e o processador POWER8, a última geração desta família. Se você já conhece a arquitetura e está interessado nela, venha ver como você pode acessar uma máquina remotamente sem grande esforço. **About the speaker:** Leonardo é engenheiro de software do LTC, e hoje trabalha no projeto PowerKVM.","tags":[],"title":"Arquitetura dos processadores POWER da IBM","type":"talk"},{"authors":[],"categories":null,"content":"","date":1428602400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428602400,"objectID":"505db68d361bc775d342373dc31ec90e","permalink":"https://openpower.ic.unicamp.br/talk/openstack/","publishdate":"2015-04-09T18:00:00Z","relpermalink":"/talk/openstack/","section":"talk","summary":"Criado originalmente por uma parceira entre a Nasa e a RackSpace atualmente o OpenStack é gerenciado por um consorcio de mais de 500 membros, incluindo grandes empresas como IBM, Intel, Cisco, Dell e etc. O OpenStack pode ser definido como um Sistema Operacional projetados para nuvem capaz de controlar uma grande quantidade de recursos, utilizando um um conjunto de projetos de software open source para configurar e operar uma infraestrutura de computação e armazenamento. **About the speaker:** Marcelo Claudio Sousa Araújo é graduado pela Universidade Federal do Tocantins (UFT), atualmente mestrando pela Universidade Estadual de Campinas (UNICAMP) e pesquisando em parceira a IBM através do LTC Unicamp. Atualmente trabalha com virtualização, cloud e Power 8 e também tem interesses na área de computação de alto desempenho, paralela e distribuída.","tags":[],"title":"OpenStack","type":"talk"},{"authors":null,"categories":null,"content":"Campinas Public University (Unicamp) and IBM have estabilished important partnerships for the development of the academic community in the area of ​​information technology. The initiatives involves two projects: a public minicloud within Unicamp with IBM Power Systems® infrastructure and Unicamp\u0026rsquo;s participation in the OpenPOWER Foundation\u0026rsquo;s consortium.\nUnicamp\u0026rsquo;s [Minicloud] (http://openpower.ic.unicamp.br/minicloud) provides free access to IBM Power Systems® virtual machines for academic users and for the Open Source community. The environment is available for developing, testing and migrating applications to the IBM server. Minicloud uses IBM PowerKVM ™, which is an open virtualization solution that supports running a large number of virtual machines on a single server running Linux operating system.\n[OpenPOWER] (https://openpowerfoundation.org/) is an open and collaborative community for the development of the IBM POWER processor. In late 2013, IBM opened the POWER processor chip technology to the market to drive innovation and create an ecosystem on the platform. Unicamp is the first Brazilian institution to participate in the consortium, which today has more than 87 members worldwide, including hardware, software, technical computing, data centers and universities.\nUnicamp\u0026rsquo;s membership of the OpenPower Foundation strengthens the partnership between institutions for development in the Linux environment. In late 2003, Unicamp began partnering with IBM\u0026rsquo;s Linux Technology Center Brazil (LTC), and since then both have been working together on developing solutions and research on the POWER platform, both in the Open Source community and in academia. Today, LTC has a laboratory on the university campus where the work is carried out jointly with professors, undergraduate and master students.\n“Partnerships like this one are very important to Unicamp, as they enable scholarship students to be involved in a work environment and in the execution of projects that go beyond the academic environment, complementing the student\u0026rsquo;s experience so that the university fulfills its main function. to train highly qualified human resources”, emphasizes the Professors Sandro Rigo and Rodolfo Azevedo, from the Unicamp Computing Institute.\n“Unicamp\u0026rsquo;s OpenPower Foundation membership will enable workgroups to exchange experience and knowledge across diverse areas, enabling the development of the ecosystem around Linux on POWER servers,” says Aníbal Strianese, Server Sales Executive at OpenPower Foundation. IBM Brazil.\nAbout Unicamp\nThe University of Campinas (Unicamp) was officially founded on October 5, 1966. Today is considered one of the leading educational and research institutions in the country. It has three campuses located in the cities of Campinas, Piracicaba and Limeira, and also 24 teaching and research institutes. It\u0026rsquo;s academic production corresponds to 15% of the research in Brazil. Unicamp considers scientific research primarily as a component in the quality of education, but also as an economic activity. This leads to natural relationships with the industry, easy dialogue with funding agencies and quick insertion into the production process. For more information about Unicamp, visit http://www.unicamp.br.\nAbout IBM\nFor more information about IBM, visit http://www.ibm.com/ IBM on Twitter: http://twitter.com/ibmbrasil Follow www.timaissimples.com.br, the blog that makes technology simple.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://openpower.ic.unicamp.br/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"Campinas Public University (Unicamp) and IBM have estabilished important partnerships for the development of the academic community in the area of ​​information technology. The initiatives involves two projects: a public minicloud within Unicamp with IBM Power Systems® infrastructure and Unicamp\u0026rsquo;s participation in the OpenPOWER Foundation\u0026rsquo;s consortium.\nUnicamp\u0026rsquo;s [Minicloud] (http://openpower.ic.unicamp.br/minicloud) provides free access to IBM Power Systems® virtual machines for academic users and for the Open Source community. The environment is available for developing, testing and migrating applications to the IBM server.","tags":null,"title":"About","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1b1b7e3245409443fba7345aa2766006","permalink":"https://openpower.ic.unicamp.br/project/ftp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/ftp/","section":"project","summary":"OpenPower Lab Repository","tags":["Storage","Builds"],"title":"FTP","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"329ec71bd31cf02078cdb4cac4974c2e","permalink":"https://openpower.ic.unicamp.br/project/minicloud/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/minicloud/","section":"project","summary":"The FREE OpenPower Cloud by Unicamp","tags":["Cloud"],"title":"Minicloud","type":"project"},{"authors":null,"categories":null,"content":"Our infrastructure is being used to validate several Open Source projects.\nOpen-source projects    Status Project FTP Link (ppc64le binaries)      Bazel https://oplab9.parqtec.unicamp.br/pub/ppc64el/bazel    Conmon https://oplab9.parqtec.unicamp.br/pub/ppc64el/conmon    Containerd https://oplab9.parqtec.unicamp.br/pub/ppc64el/containerd    Containerd-cri https://oplab9.parqtec.unicamp.br/pub/ppc64el/containerd-cri    Crio https://oplab9.parqtec.unicamp.br/pub/ppc64el/crio    Crun https://oplab9.parqtec.unicamp.br/pub/ppc64el/crun    Docker CE https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker    Docker Machine https://oplab9.parqtec.unicamp.br/pub/ppc64el/docker-machine    Glide https://oplab9.parqtec.unicamp.br/pub/ppc64el/glide    Grafana https://oplab9.parqtec.unicamp.br/pub/ppc64el/grafana    Kiali https://oplab9.parqtec.unicamp.br/pub/ppc64el/kiali    Kubeadm https://oplab9.parqtec.unicamp.br/pub/ppc64el/kubeadm    Kubectl https://oplab9.parqtec.unicamp.br/pub/ppc64el/kubectl    Kubelet https://oplab9.parqtec.unicamp.br/pub/ppc64el/kubelet    Matchbox https://oplab9.parqtec.unicamp.br/pub/ppc64el/matchbox    Minikube https://oplab9.parqtec.unicamp.br/pub/ppc64el/minikube    Minio https://oplab9.parqtec.unicamp.br/pub/ppc64el/minio    Minio-MC https://oplab9.parqtec.unicamp.br/pub/ppc64el/minio-mc    Rclone https://oplab9.parqtec.unicamp.br/pub/ppc64el/rclone    Restic https://oplab9.parqtec.unicamp.br/pub/ppc64el/restic    Terraform https://oplab9.parqtec.unicamp.br/pub/ppc64el/terraform    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e35048c918e6dc039a213489c864291c","permalink":"https://openpower.ic.unicamp.br/project/power-builds/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/power-builds/","section":"project","summary":"Open Source projects validations on POWER","tags":["CI","Builds"],"title":"POWER Builds","type":"project"},{"authors":null,"categories":null,"content":"We created a repository with all the open source projects that we build on Power.\nOpen-source packages    Project Package Description of package     Bazel bazel Build and test software of any size, quickly and reliably.   Conmon conmon An OCI container runtime monitor.   Containerd containerd An industry-standard container runtime with an emphasis on simplicity, robustness and portability.   Containerd-cri containerd-cri Containerd Plugin for Kubernetes Container Runtime Interface.   Containerd-cri containerd-cri-cni Containerd Plugin for Kubernetes Container Runtime Interface.   Crio crio Open Container Initiative-based implementation of Kubernetes Container Runtime Interface.   Crun crun A fast and lightweight fully featured OCI runtime and C library for running containers.   Docker CE docker-ce Docker Engine is the industry’s de facto container runtime that runs on various operating systems.   Docker CE docker-ce-cli Docker Engine is the industry’s de facto container runtime that runs on various operating systems.   Docker CE docker-ce-rootless-extras Docker Engine is the industry’s de facto container runtime that runs on various operating systems.   Docker Machine docker-machine Machine management for a container-centric world.   Glide glide Package Management for Go.   Grafana grafana-cli The tool for beautiful monitoring and metric analytics \u0026amp; dashboards for Graphite, InfluxDB \u0026amp; Prometheus \u0026amp; More.   Grafana grafana-server The tool for beautiful monitoring and metric analytics \u0026amp; dashboards for Graphite, InfluxDB \u0026amp; Prometheus \u0026amp; More.   Kiali kiali Service mesh management for Istio.   Kubeadm kubeadm The command to bootstrap the cluster.   Kubectl kubectl The command line util to talk to your cluster.   Kubelet kubelet The component that runs on all of the machines in your cluster and does things like starting pods and containers.   Matchbox poseidon-matchbox (apt) \\ matchbox (rpm) Matchbox is a service that matches bare-metal machines to profiles that PXE boot and provision clusters.   Minikube minikube Minikube is a tool that makes it easy to run Kubernetes locally.   Minio minio High Performance, Kubernetes Native Object Storage.   Minio-MC mc MinIO Client is a replacement for ls, cp, mkdir, diff and rsync commands for filesystems and object storage.   Rclone rclone \u0026ldquo;rsync for cloud storage\u0026rdquo; - Google Drive, Amazon Drive, and more.   Restic restic Fast, secure, efficient backup program.   Terraform terraform Use Infrastructure as Code to provision and manage any cloud, infrastructure, or service.    Add the repository and install some package To add the repository to the system, and to always obtain the new versions of software that we provide, follow these steps.\nAdd APT repository Edit the file: /etc/apt/sources.list\nInsert the line at the end of the file:\ndeb https://oplab9.parqtec.unicamp.br/pub/repository/debian/ ./\nDownload our GPG key, and use the command below to add it to the system:\nsudo apt-key add openpower-gpgkey-public.asc\nAfter that, update the system using the command below:\nsudo apt update\nInstall package To install, use the command:\nsudo apt install package\nAdd RPM repository Create and edit the file: /etc/yum.repos.d/open-power.repo\nAdd the text to it:\n[Open-Power] name=Unicamp OpenPower Lab - $basearch baseurl=https://oplab9.parqtec.unicamp.br/pub/repository/rpm/ enabled=1 gpgcheck=0 repo_gpgcheck=1 gpgkey=https://oplab9.parqtec.unicamp.br/pub/key/openpower-gpgkey-public.asc After that, performs the following command with super-user capabilities to update the system.\nyum update\n To be a super user, use the command: sudo su\n Install Package To install, it use the command:\nyum install package\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a03a4b2b4431ad78a9e3615afc0af4fa","permalink":"https://openpower.ic.unicamp.br/project/power-repository/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/power-repository/","section":"project","summary":"Open Source packages build on POWER","tags":["Repository","Packages","APT","RPM"],"title":"POWER Repository","type":"project"}]